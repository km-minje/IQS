"""
Agent Tool Mixins - ê³µí†µ ê¸°ëŠ¥ì„ ìœ„í•œ Mixin í´ë˜ìŠ¤ë“¤
ìê¸° í‰ê°€, ì—ëŸ¬ ì²˜ë¦¬, ë¡œê¹… ë“± ì¤‘ë³µ ê¸°ëŠ¥ì„ í†µí•©
"""
import json
import re
from typing import Dict, Any, List, Optional, Union
from pathlib import Path
from src.utils.logger import log


class SelfAssessmentMixin:
    """
    ìê¸° í‰ê°€ ê¸°ëŠ¥ì„ ìœ„í•œ Mixin í´ë˜ìŠ¤
    
    ëª¨ë“  ë„êµ¬ì—ì„œ ì¤‘ë³µë˜ë˜ assess_query_suitability()ì™€ assess_step_suitability() ë¡œì§ì„ í†µí•©
    ì„¤ì • ê¸°ë°˜ í‰ê°€ ì‹œìŠ¤í…œìœ¼ë¡œ í•˜ë“œì½”ë”© ì œê±°
    """
    
    # ğŸ”§ ìƒˆë¡œìš´ ê¸°ëŠ¥: í´ë˜ìŠ¤ ë ˆë²¨ í‰ê°€ ê²°ê³¼ ìºì‹±
    _assessment_cache = {}
    
    def __init__(self, *args, **kwargs):
        """
        Mixin ì´ˆê¸°í™” - ë‹¤ì¤‘ ìƒì† í˜¸í™˜ì„± ë³´ì¥
        """
        super().__init__(*args, **kwargs)
        
        # ì„¤ì • ë¡œë“œ (í•„ìš”ì‹œ ì§€ì—° ë¡œë”©)
        self._assessment_config = None
        self._global_assessment_config = None
    
    def _load_assessment_config(self) -> Dict[str, Any]:
        """
        ë„êµ¬ë³„ í‰ê°€ ì„¤ì • ë¡œë“œ
        ê° ë„êµ¬ì—ì„œ overrideí•˜ê±°ë‚˜ tool name ê¸°ë°˜ìœ¼ë¡œ ìë™ ë¡œë“œ
        
        Returns:
            í•´ë‹¹ ë„êµ¬ì˜ í‰ê°€ ì„¤ì •
        """
        if self._assessment_config is not None:
            return self._assessment_config
        
        try:
            # ì „ì²´ ì„¤ì • íŒŒì¼ ë¡œë“œ
            if self._global_assessment_config is None:
                config_path = Path("config/tool_assessment.json")
                if config_path.exists():
                    with open(config_path, 'r', encoding='utf-8') as f:
                        self._global_assessment_config = json.load(f)
                        # ğŸ”§ ìƒˆë¡œìš´ ê¸°ëŠ¥: ì„¤ì • íŒŒì¼ ê²€ì¦
                        self._validate_config(self._global_assessment_config)
                else:
                    log.warning(f"Assessment config file not found: {config_path}")
                    self._global_assessment_config = {}
            
            # ë„êµ¬ëª… ìë™ ê°ì§€ (tool name ê¸°ë°˜)
            tool_name = getattr(self, 'name', None)
            if tool_name and tool_name in self._global_assessment_config:
                self._assessment_config = self._global_assessment_config[tool_name]
                log.debug(f"Loaded assessment config for tool: {tool_name}")
            else:
                # ê¸°ë³¸ ì„¤ì • ì‚¬ìš©
                self._assessment_config = self._get_default_assessment_config()
                log.warning(f"Using default assessment config for tool: {tool_name}")
                
        except Exception as e:
            log.error(f"Failed to load assessment config: {e}")
            self._assessment_config = self._get_default_assessment_config()
        
        return self._assessment_config
    
    def _validate_config(self, config: Dict[str, Any]):
        """
        ğŸ”§ ìƒˆë¡œìš´ ê¸°ëŠ¥: ì„¤ì • íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬
        
        Args:
            config: ê²€ì¦í•  ì„¤ì •
        """
        required_tools = ['aggregator', 'hybrid_search', 'reranker', 'glossary_lookup', 'synthesizer']
        required_fields = ['query_keywords', 'base_score', 'threshold']
        
        validation_errors = []
        
        for tool_name in required_tools:
            if tool_name not in config:
                validation_errors.append(f"Missing tool config: {tool_name}")
                continue
            
            tool_config = config[tool_name]
            for field in required_fields:
                if field not in tool_config:
                    validation_errors.append(f"Missing field '{field}' in {tool_name}")
            
            # ì ìˆ˜ ë²”ìœ„ ê²€ì¦
            base_score = tool_config.get('base_score', 0)
            threshold = tool_config.get('threshold', 0)
            
            if not 0 <= base_score <= 1:
                validation_errors.append(f"Invalid base_score in {tool_name}: {base_score}")
            if not 0 <= threshold <= 1:
                validation_errors.append(f"Invalid threshold in {tool_name}: {threshold}")
        
        if validation_errors:
            log.warning(f"Config validation issues: {validation_errors}")
        else:
            log.debug("Config validation passed")
    
    def _get_default_assessment_config(self) -> Dict[str, Any]:
        """
        ê¸°ë³¸ í‰ê°€ ì„¤ì • (ì„¤ì • íŒŒì¼ì´ ì—†ì„ ë•Œ)
        
        Returns:
            ê¸°ë³¸ í‰ê°€ ì„¤ì •
        """
        return {
            "query_keywords": [],
            "base_score": 0.3,
            "keyword_score": 0.1,
            "threshold": 0.4,
            "suggested_params": {},
            "intent_weights": {}
        }
    
    def assess_query_suitability(self, query: str) -> Dict[str, Any]:
        """
        ğŸ”§ í†µí•©ëœ ì¿¼ë¦¬ ì í•©ì„± í‰ê°€ - ëª¨ë“  ë„êµ¬ì˜ ì¤‘ë³µ ë¡œì§ ëŒ€ì²´
        
        ì„¤ì • ê¸°ë°˜ìœ¼ë¡œ í‚¤ì›Œë“œ ê°ì§€, ì ìˆ˜ ê³„ì‚°, ì„ê³„ê°’ ë¹„êµë¥¼ ìˆ˜í–‰
        
        Args:
            query: í‰ê°€í•  ì¿¼ë¦¬
            
        Returns:
            ì í•©ì„± í‰ê°€ ê²°ê³¼
        """
        # ğŸ”§ ìƒˆë¡œìš´ ê¸°ëŠ¥: ìºì‹± ì‹œìŠ¤í…œ
        tool_name = getattr(self, 'name', 'unknown')
        cache_key = f"{tool_name}:{hash(query)}"
        
        if cache_key in self._assessment_cache:
            log.debug(f"Assessment cache hit for {tool_name}: {query}")
            return self._assessment_cache[cache_key]
        
        config = self._load_assessment_config()
        query_lower = query.lower()
        
        # ê¸°ë³¸ ì ìˆ˜ ì„¤ì •
        relevance_score = config.get("base_score", 0.3)
        reasons = []
        
        # 1. ê¸°ë³¸ í‚¤ì›Œë“œ ê²€ì‚¬
        query_keywords = config.get("query_keywords", [])
        keyword_score = config.get("keyword_score", 0.1)
        
        for keyword in query_keywords:
            if keyword in query_lower:
                relevance_score += keyword_score
                reasons.append(f"'{keyword}' í‚¤ì›Œë“œ ë°œê²¬")
        
        # 2. ì¶”ê°€ í‚¤ì›Œë“œ ê²€ì‚¬ (ë„êµ¬ë³„ íŠ¹ë³„ ì²˜ë¦¬)
        additional_analysis = self._assess_additional_patterns(query_lower, config, reasons)
        relevance_score += additional_analysis
        
        # 3. ì‹ ë¢°ë„ ê³„ì‚° ë° ì í•©ì„± íŒë‹¨
        confidence = min(relevance_score, 1.0)
        threshold = config.get("threshold", 0.4)
        suitable = confidence > threshold
        
        # 4. ì œì•ˆ íŒŒë¼ë¯¸í„° ë™ì  ìƒì„±
        suggested_params = self._generate_suggested_params(confidence, config)
        
        result = {
            'suitable': suitable,
            'confidence': confidence,
            'reason': '; '.join(reasons) if reasons else f'ê¸°ë³¸ í‰ê°€ (ë„êµ¬: {getattr(self, "name", "unknown")})',
            'suggested_params': suggested_params,
            'assessment_method': 'config_based_mixin_v1.1',
            'cache_hit': False
        }
        
        # ğŸ”§ ìƒˆë¡œìš´ ê¸°ëŠ¥: ê²°ê³¼ ìºì‹± (ì„±ëŠ¥ ìµœì í™” ì ìš©)
        self._assessment_cache[cache_key] = result
        
        # ìºì‹œ í¬ê¸° ì œí•œ (100ê°œë¥¼ ì´ˆê³¼í•˜ë©´ ì˜¤ë˜ëœ ê²ƒë¶€í„° ì œê±°)
        if len(self._assessment_cache) > 100:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±° (FIFO) - íš¨ìœ¨ì  êµ¬í˜„
            cache_items = list(self._assessment_cache.items())
            # ì˜¤ë˜ëœ ì ˆë°˜ ì œê±°
            for i in range(len(cache_items) // 2):
                key_to_remove = cache_items[i][0]
                del self._assessment_cache[key_to_remove]
            
            log.debug(f"Cache cleanup: removed {len(cache_items) // 2} old entries")
        
        log.debug(f"Assessment for '{query}': suitable={suitable}, confidence={confidence:.2f}, cached={result.get('cache_hit', False)}")
        return result
    
    def _assess_additional_patterns(self, query_lower: str, config: Dict[str, Any], reasons: List[str]) -> float:
        """
        ì¶”ê°€ íŒ¨í„´ ë¶„ì„ - ë„êµ¬ë³„ íŠ¹ë³„í•œ í‰ê°€ ë¡œì§
        
        Args:
            query_lower: ì†Œë¬¸ì ì¿¼ë¦¬
            config: í‰ê°€ ì„¤ì •
            reasons: í‰ê°€ ì´ìœ  ëª©ë¡ (ë³€ê²½ë¨)
            
        Returns:
            ì¶”ê°€ ì ìˆ˜
        """
        additional_score = 0.0
        
        # ë¶„ì„ í‚¤ì›Œë“œ (aggregator ë“±ì—ì„œ ì‚¬ìš©)
        if "analysis_keywords" in config:
            analysis_score = config.get("analysis_score", 0.1)
            for keyword in config["analysis_keywords"]:
                if keyword in query_lower:
                    additional_score += analysis_score
                    reasons.append(f"'{keyword}' ë¶„ì„ ì˜ë„ ê°ì§€")
        
        # ë¹„êµ í‚¤ì›Œë“œ (reranker ë“±ì—ì„œ ì‚¬ìš©)
        if "comparison_keywords" in config:
            comparison_score = config.get("comparison_score", 0.1)
            for keyword in config["comparison_keywords"]:
                if keyword in query_lower:
                    additional_score += comparison_score
                    reasons.append(f"'{keyword}' ë¹„êµ/í•„í„°ë§ ìš”ì²­")
        
        # íŠ¹ë³„ íŒ¨í„´ ì²˜ë¦¬ (search ë“±ì—ì„œ ì‚¬ìš©)
        if "special_patterns" in config:
            patterns = config["special_patterns"]
            
            # ì½”ë“œ íŒ¨í„´ ê°ì§€
            if "code_pattern" in patterns:
                code_pattern = patterns["code_pattern"]
                if re.search(code_pattern, query_lower, re.IGNORECASE):
                    code_bonus = config.get("code_bonus", 0.2)
                    additional_score += code_bonus
                    reasons.append("ì½”ë“œ íŒ¨í„´ ê°ì§€")
            
            # ğŸ”§ ìƒˆë¡œìš´ ê¸°ëŠ¥: í•œê¸€ íŒ¨í„´ ê°ì§€ (glossary_lookup ìš©)
            if "korean_pattern" in patterns:
                korean_pattern = patterns["korean_pattern"]
                if re.search(korean_pattern, query_lower):
                    korean_bonus = config.get("korean_bonus", 0.3)
                    additional_score += korean_bonus
                    reasons.append("í•œê¸€ í…ìŠ¤íŠ¸ ê°ì§€")
            
            # ğŸ”§ ìƒˆë¡œìš´ ê¸°ëŠ¥: í•œê¸€+ì½”ë“œ ë³µí•© íŒ¨í„´
            if "code_with_korean" in patterns:
                code_korean_pattern = patterns["code_with_korean"]
                if re.search(code_korean_pattern, query_lower):
                    additional_score += 0.4  # ë†’ì€ ì ìˆ˜
                    reasons.append("í•œê¸€+ì½”ë“œ ë³µí•© íŒ¨í„´ ê°ì§€")
            
            # ëŒ€ìš©ëŸ‰ ë°ì´í„° ìš”ì²­ ê°ì§€
            if "large_data_keywords" in patterns:
                large_data_bonus = config.get("large_data_bonus", 0.15)
                for keyword in patterns["large_data_keywords"]:
                    if keyword in query_lower:
                        additional_score += large_data_bonus
                        reasons.append("ëŒ€ìš©ëŸ‰ ë°ì´í„° ìš”ì²­")
                        break
        
        return additional_score
    
    def _generate_suggested_params(self, confidence: float, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì‹ ë¢°ë„ ê¸°ë°˜ ì œì•ˆ íŒŒë¼ë¯¸í„° ë™ì  ìƒì„±
        
        Args:
            confidence: ê³„ì‚°ëœ ì‹ ë¢°ë„
            config: ë„êµ¬ ì„¤ì •
            
        Returns:
            ì œì•ˆ íŒŒë¼ë¯¸í„°
        """
        suggested_params = config.get("suggested_params", {}).copy()
        
        # ì‹ ë¢°ë„ì— ë”°ë¥¸ ë™ì  íŒŒë¼ë¯¸í„° ì¡°ì •
        if confidence > 0.8:
            # ë§¤ìš° ë†’ì€ ì‹ ë¢°ë„ - ìµœëŒ€ ì„±ëŠ¥
            if "limit_massive" in suggested_params:
                suggested_params["limit"] = suggested_params["limit_massive"]
            elif "limit_high" in suggested_params:
                suggested_params["limit"] = suggested_params["limit_high"]
            if "size_high" in suggested_params:
                suggested_params["size"] = suggested_params["size_high"]
            if "top_k_high" in suggested_params:
                suggested_params["top_k"] = suggested_params["top_k_high"]
        elif confidence > 0.6:
            # ë†’ì€ ì‹ ë¢°ë„ - ë” ì„¸ë°€í•œ ë¶„ì„
            if "size_high" in suggested_params:
                suggested_params["size"] = suggested_params["size_high"]
            if "limit_high" in suggested_params:
                suggested_params["limit"] = suggested_params["limit_high"]
            if "top_k_high" in suggested_params:
                suggested_params["top_k"] = suggested_params["top_k_high"]
        else:
            # ê¸°ë³¸ ì‹ ë¢°ë„ - í‘œì¤€ ë¶„ì„
            if "size_low" in suggested_params:
                suggested_params["size"] = suggested_params["size_low"]
            if "limit_low" in suggested_params:
                suggested_params["limit"] = suggested_params["limit_low"]
            if "top_k_low" in suggested_params:
                suggested_params["top_k"] = suggested_params["top_k_low"]
        
        # ğŸ”§ ìƒˆë¡œìš´ ê¸°ëŠ¥: ë„êµ¬ë³„ íŠ¹ë³„ íŒŒë¼ë¯¸í„° ì²˜ë¦¬
        tool_name = getattr(self, 'name', 'unknown')
        if tool_name == 'glossary_lookup':
            # í•œê¸€ ê°ì§€ì‹œ ë™ì˜ì–´ í¬í•¨
            suggested_params['include_synonyms'] = confidence > 0.7
            suggested_params['include_codes'] = True
        elif tool_name == 'synthesizer':
            # ì¢…í•© ë„êµ¬ëŠ” LLM ì‚¬ìš© ì—¬ë¶€ ê²°ì •
            suggested_params['llm_synthesis'] = confidence > 0.8
            suggested_params['include_metadata'] = True
        
        # ì„ì‹œ í‚¤ ì œê±°
        for key in ["size_low", "size_high", "limit_low", "limit_high", "limit_massive", "top_k_low", "top_k_high"]:
            suggested_params.pop(key, None)
        
        return suggested_params
    
    def assess_step_suitability(self, step: Dict[str, Any], context: Dict[str, Any]) -> float:
        """
        ğŸ”§ í†µí•©ëœ ë‹¨ê³„ ì í•©ì„± í‰ê°€ - ëª¨ë“  ë„êµ¬ì˜ ì¤‘ë³µ ë¡œì§ ëŒ€ì²´
        
        Args:
            step: ì‹¤í–‰ ë‹¨ê³„ ì •ë³´
            context: ì»¨í…ìŠ¤íŠ¸ ì •ë³´
            
        Returns:
            ì í•©ì„± ì ìˆ˜ (0.0 ~ 1.0)
        """
        config = self._load_assessment_config()
        step_desc = step.get('description', '').lower()
        intent = context.get('intent', 'unknown')
        
        # ê¸°ë³¸ ì ìˆ˜
        suitability_score = config.get("base_score", 0.3)
        
        # ì˜ë„ ê¸°ë°˜ ì ìˆ˜ ì¡°ì •
        intent_weights = config.get("intent_weights", {})
        if intent in intent_weights:
            suitability_score += intent_weights[intent]
        
        # ë‹¨ê³„ ì„¤ëª… ê¸°ë°˜ í‚¤ì›Œë“œ ë§¤ì¹­
        query_keywords = config.get("query_keywords", [])
        for keyword in query_keywords:
            if keyword in step_desc:
                suitability_score += 0.1
                break
        
        # ì¶”ê°€ í‚¤ì›Œë“œ ë§¤ì¹­
        if "analysis_keywords" in config:
            for keyword in config["analysis_keywords"]:
                if keyword in step_desc:
                    suitability_score += 0.1
                    break
        
        if "comparison_keywords" in config:
            for keyword in config["comparison_keywords"]:
                if keyword in step_desc:
                    suitability_score += 0.1
                    break
        
        final_score = min(suitability_score, 1.0)
        log.debug(f"Step assessment: intent={intent}, score={final_score:.2f}")
        
        return final_score
    
    def get_assessment_info(self) -> Dict[str, Any]:
        """
        í˜„ì¬ ë„êµ¬ì˜ í‰ê°€ ì„¤ì • ì •ë³´ ë°˜í™˜ (ë””ë²„ê¹…ìš©)
        
        Returns:
            í‰ê°€ ì„¤ì • ì •ë³´
        """
        config = self._load_assessment_config()
        return {
            "tool_name": getattr(self, 'name', 'unknown'),
            "assessment_config": config,
            "assessment_method": "SelfAssessmentMixin_v1.1",
            "config_loaded": self._assessment_config is not None,
            "cache_size": len(self._assessment_cache),
            "version": "1.1.0",
            "features": [
                "config_based_assessment", 
                "result_caching", 
                "pattern_detection", 
                "dynamic_parameters",
                "validation_system"
            ]
        }
    
    def clear_cache(self):
        """
        ğŸ”§ ìƒˆë¡œìš´ ê¸°ëŠ¥: í‰ê°€ ê²°ê³¼ ìºì‹œ í´ë¦¬ì–´
        """
        cleared_count = len(self._assessment_cache)
        self._assessment_cache.clear()
        log.info(f"Cleared assessment cache: {cleared_count} entries")
        return cleared_count
    
    @classmethod
    def get_global_cache_stats(cls) -> Dict[str, Any]:
        """
        ğŸ”§ ìƒˆë¡œìš´ ê¸°ëŠ¥: ì „ì²´ ìºì‹œ í†µê³„
        
        Returns:
            ì „ì²´ ìºì‹œ í†µê³„ ì •ë³´
        """
        return {
            "cache_size": len(cls._assessment_cache),
            "cache_limit": 100,
            "cache_efficiency": "FIFO eviction policy"
        }


class ErrorHandlingMixin:
    """
    ì—ëŸ¬ ì²˜ë¦¬ ê¸°ëŠ¥ì„ ìœ„í•œ Mixin í´ë˜ìŠ¤
    ëª¨ë“  ë„êµ¬ì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©í•  ì—ëŸ¬ ì²˜ë¦¬ íŒ¨í„´
    """
    
    def safe_execute(self, execute_func, *args, **kwargs):
        """
        ì•ˆì „í•œ ì‹¤í–‰ ë˜í¼ - ê³µí†µ ì—ëŸ¬ ì²˜ë¦¬ íŒ¨í„´
        
        Args:
            execute_func: ì‹¤í–‰í•  í•¨ìˆ˜
            *args, **kwargs: í•¨ìˆ˜ ì¸ìë“¤
            
        Returns:
            ì‹¤í–‰ ê²°ê³¼ ë˜ëŠ” ì—ëŸ¬ ì •ë³´
        """
        tool_name = getattr(self, 'name', 'unknown_tool')
        
        try:
            log.info(f"Executing {tool_name}...")
            result = execute_func(*args, **kwargs)
            log.info(f"{tool_name} completed successfully")
            return result
            
        except Exception as e:
            log.error(f"{tool_name} execution failed: {e}")
            return {
                "error": str(e),
                "tool": tool_name,
                "error_type": type(e).__name__,
                "execution_method": "safe_execute_mixin"
            }


class LoggingMixin:
    """
    ë¡œê¹… ê¸°ëŠ¥ì„ ìœ„í•œ Mixin í´ë˜ìŠ¤
    ì¼ê´€ëœ ë¡œê¹… í˜•ì‹ ì œê³µ
    """
    
    def log_execution_start(self, operation: str, **details):
        """ì‹¤í–‰ ì‹œì‘ ë¡œê¹…"""
        tool_name = getattr(self, 'name', 'unknown_tool')
        log.info(f"{tool_name}: Starting {operation} - {details}")
    
    def log_execution_end(self, operation: str, success: bool, **details):
        """ì‹¤í–‰ ì™„ë£Œ ë¡œê¹…"""
        tool_name = getattr(self, 'name', 'unknown_tool')
        status = "SUCCESS" if success else "FAILED"
        log.info(f"{tool_name}: {operation} {status} - {details}")