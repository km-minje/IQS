"""
Agent Tools - ê¸°ì¡´ ëª¨ë“ˆë“¤ì„ Agentìš© Toolë¡œ ë˜í•‘
"""
from typing import Dict, Any, List, Optional
from abc import ABC, abstractmethod
import pandas as pd
import numpy as np

from src.utils.logger import log
from src.search.semantic_search import SemanticSearcher, SearchResult
from src.search.embeddings.embedding_model import EmbeddingModel
from src.reranker.reranker import Reranker
from src.agent.glossary_tool import GlossaryLookupTool
from src.agent.utils.document_utils import DocumentExtractor
from src.agent.mixins import SelfAssessmentMixin, ErrorHandlingMixin


class BaseTool(ABC, SelfAssessmentMixin, ErrorHandlingMixin):
    """
    ë„êµ¬ ê¸°ë³¸ í´ë˜ìŠ¤ - ğŸ”§ ê°œì„ : Mixin ê¸°ëŠ¥ í†µí•©
    
    ëª¨ë“  ë„êµ¬ê°€ ê³µí†µìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ë“¤:
    - SelfAssessmentMixin: ìê¸° í‰ê°€ ê¸°ëŠ¥
    - ErrorHandlingMixin: ì•ˆì „í•œ ì‹¤í–‰ ë° ì—ëŸ¬ ì²˜ë¦¬
    """
    
    def __init__(self, *args, **kwargs):
        """ê¸°ë³¸ ë„êµ¬ ì´ˆê¸°í™” - Mixin í˜¸í™˜ì„± ë³´ì¥"""
        super().__init__(*args, **kwargs)
        
        # ë„êµ¬ë³„ ê³ ìœ  ì„¤ì • ë¡œë“œ
        if hasattr(self, 'name'):
            log.debug(f"Initialized {self.name} with assessment and error handling")

    @abstractmethod
    def execute(self, **kwargs) -> Any:
        """ë„êµ¬ ì‹¤í–‰"""
        pass

    @abstractmethod
    def get_description(self) -> str:
        """ë„êµ¬ ì„¤ëª…"""
        pass
    
    # ğŸ”§ ìê¸° í‰ê°€ ê¸°ëŠ¥ì€ SelfAssessmentMixinì—ì„œ ìë™ ìƒì†
    # assess_query_suitability(), assess_step_suitability() ìë™ ì œê³µ
    
    # ğŸ”§ ì•ˆì „í•œ ì‹¤í–‰ ê¸°ëŠ¥ë„ ErrorHandlingMixinì—ì„œ ìë™ ìƒì†
    # safe_execute() ìë™ ì œê³µ


class AggregatorTool(BaseTool):
    """
    í†µê³„ ë° ì§‘ê³„ ë„êµ¬
    SQL ìŠ¤íƒ€ì¼ì˜ GROUP BY, COUNT, TOP N ë“± ì²˜ë¦¬
    """

    def __init__(self):
        """Initialize Aggregator Tool"""
        self.name = "aggregator"
        self.description = "ë°ì´í„° ì§‘ê³„ ë° í†µê³„ ë¶„ì„ (GROUP BY, COUNT, TOP N)"
        
        # ğŸ”§ Mixin ì´ˆê¸°í™” (ìê¸° í‰ê°€ ë° ì—ëŸ¬ ì²˜ë¦¬ ê¸°ëŠ¥ í¬í•¨)
        super().__init__()
        
        log.info("Initialized AggregatorTool with assessment capabilities")

    def execute(self, 
                documents: Optional[List[Dict]] = None,
                aggregation: str = "terms",
                field: str = "problem",
                size: int = 10,
                filters: Optional[Dict] = None,
                previous_result: Optional[Any] = None,
                **kwargs) -> Dict[str, Any]:
        """
        ì§‘ê³„ ì‹¤í–‰

        Args:
            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (previous_resultì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ë„ ìˆìŒ)
            aggregation: ì§‘ê³„ íƒ€ì… (terms, count, stats ë“±)
            field: ì§‘ê³„í•  í•„ë“œ
            size: ê²°ê³¼ ê°œìˆ˜
            filters: í•„í„° ì¡°ê±´
            previous_result: ì´ì „ ë‹¨ê³„ ê²°ê³¼

        Returns:
            ì§‘ê³„ ê²°ê³¼
        """
        log.info(f"Executing aggregation: {aggregation} on {field}")

        # ğŸ”§ ê°œì„ : í†µí•©ëœ ë¬¸ì„œ ì¶”ì¶œ ë¡œì§ ì‚¬ìš©
        documents, extracted_query = DocumentExtractor.extract_from_previous_result(
            previous_result=previous_result,
            documents=documents,
            fallback_fields=['source_documents'],
            extract_query=False,
            log_details=True
        )

        if not documents:
            log.warning("No documents to aggregate")
            return {
                "error": "No documents provided for aggregation",
                "aggregation_attempted": aggregation,
                "field_requested": field,
                "total_docs": 0,
                "buckets": [],
                "hallucination_prevented": True
            }

        # DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame(documents)

        # í•„í„° ì ìš©
        if filters:
            for key, value in filters.items():
                if key in df.columns:
                    df = df[df[key] == value]

        # ğŸ”§ Hallucination ë°©ì§€: ìµœì†Œ ë°ì´í„° ì„ê³„ê°’ ê²€ì¦
        MIN_DOCS_FOR_AGGREGATION = 5
        if len(df) < MIN_DOCS_FOR_AGGREGATION:
            log.warning(f"Insufficient data for reliable aggregation: {len(df)} < {MIN_DOCS_FOR_AGGREGATION}")
            return {
                "error": f"Insufficient data: only {len(df)} documents (minimum {MIN_DOCS_FOR_AGGREGATION} required)",
                "aggregation_attempted": aggregation,
                "field_requested": field,
                "total_docs": len(df),
                "buckets": [],
                "hallucination_prevented": True,
                "minimum_threshold": MIN_DOCS_FOR_AGGREGATION
            }

        # ì§‘ê³„ ìˆ˜í–‰
        results = {}

        if aggregation == "terms":
            # Terms aggregation - ê°’ë³„ ì¹´ìš´íŠ¸
            log.info(f"Available columns: {list(df.columns)}")
            
            # í•„ë“œëª… ë§¤í•‘ (ì˜ëª»ëœ í•„ë“œëª… ìˆ˜ì •)
            field_mapping = {
                'complaint_type': 'problem',  # complaint_type -> problem
                'issue_type': 'problem',      # issue_type -> problem  
                'defect_type': 'problem',     # defect_type -> problem
                'category': 'problem'
            }
            
            actual_field = field_mapping.get(field, field)
            
            if actual_field in df.columns:
                value_counts = df[actual_field].value_counts().head(size)
                # ğŸ”§ Hallucination ë°©ì§€: ì‹¤ì œ ë°ì´í„° ê²€ì¦ ë° íˆ¬ëª…ì„±
                buckets = [
                    {
                        "key": key,
                        "doc_count": int(count)
                    }
                    for key, count in value_counts.items()
                ]
                
                results = {
                    "aggregation": "terms",
                    "field": actual_field,
                    "original_field": field,
                    "buckets": buckets,
                    "total_docs": len(df),
                    "data_verification": {
                        "source_doc_count": len(documents),
                        "filtered_doc_count": len(df),
                        "unique_values_found": len(buckets),
                        "filters_applied": bool(filters),
                        "hallucination_prevented": True,
                        "aggregation_timestamp": __import__('time').strftime('%Y-%m-%d %H:%M:%S')
                    }
                }
                log.info(f"Terms aggregation on '{actual_field}': {len(results['buckets'])} buckets")
                
                # ğŸ” ë””ë²„ê¹…: ì§‘ê³„ ê²°ê³¼ ìƒì„¸ ë¡œê¹…

            else:
                available_fields = [col for col in df.columns if 'problem' in col.lower() or 'issue' in col.lower() or 'category' in col.lower()]
                results = {
                    "error": f"Field '{field}' (mapped to '{actual_field}') not found",
                    "available_columns": list(df.columns),
                    "suggested_fields": available_fields,
                    "aggregation": "error"
                }

        elif aggregation == "count":
            # Simple count
            results = {
                "aggregation": "count",
                "count": len(df),
                "field": field if field else "all"
            }
            
        elif aggregation == "info_code_analysis":
            # INFO ì½”ë“œ ì „ìš© ë¶„ì„
            if 'problem' in df.columns:
                import re
                info_counts = {}
                info_details = {}
                
                # ì „ì²´ ë¬¸ì„œì—ì„œ INFO ì½”ë“œ ì¶”ì¶œ ë° ì¹´ìš´íŠ¸
                for _, row in df.iterrows():
                    problem = str(row.get('problem', ''))
                    brand = row.get('make_of_vehicle', '')
                    verbatim = row.get('verbatim_text', '')
                    
                    # í˜„ëŒ€ ë¸Œëœë“œë§Œ ë¶„ì„
                    if brand.lower() not in ['hyundai', 'í˜„ëŒ€']:
                        continue
                    
                    # INFO12, INFO13, INFO14 ì¶”ì¶œ
                    for target_code in ['INFO12', 'INFO13', 'INFO14']:
                        if target_code in problem:
                            if target_code not in info_counts:
                                info_counts[target_code] = 0
                                info_details[target_code] = []
                            
                            info_counts[target_code] += 1
                            
                            # ìƒì„¸ ì •ë³´ ì €ì¥ (ì²˜ìŒ 3ê°œë§Œ)
                            if len(info_details[target_code]) < 3:
                                info_details[target_code].append({
                                    'problem': problem,
                                    'verbatim': verbatim[:100] + '...' if len(verbatim) > 100 else verbatim,
                                    'model': row.get('model_of_vehicle', 'N/A')
                                })
                
                results = {
                    "aggregation": "info_code_analysis",
                    "field": "problem",
                    "hyundai_info_counts": info_counts,
                    "hyundai_info_details": info_details,
                    "total_hyundai_docs": len([r for _, r in df.iterrows() if r.get('make_of_vehicle', '').lower() in ['hyundai', 'í˜„ëŒ€']]),
                    "total_docs_analyzed": len(df)
                }
                
                log.info(f"INFO code analysis complete: {info_counts}")
            else:
                results = {"error": "No 'problem' field found for INFO code analysis"}

        elif aggregation == "stats":
            # Statistical aggregation (for numeric fields)
            if field in df.columns and pd.api.types.is_numeric_dtype(df[field]):
                results = {
                    "aggregation": "stats",
                    "field": field,
                    "count": len(df),
                    "min": float(df[field].min()),
                    "max": float(df[field].max()),
                    "avg": float(df[field].mean()),
                    "sum": float(df[field].sum())
                }
            else:
                results = {"error": f"Field '{field}' is not numeric"}

        elif aggregation == "group_by":
            # GROUP BY multiple fields
            group_fields = kwargs.get('group_by', [field])
            if all(f in df.columns for f in group_fields):
                grouped = df.groupby(group_fields).size().reset_index(name='count')
                grouped = grouped.sort_values('count', ascending=False).head(size)

                results = {
                    "aggregation": "group_by",
                    "fields": group_fields,
                    "groups": grouped.to_dict('records'),
                    "total_groups": len(grouped)
                }
            else:
                results = {"error": "Invalid group fields"}

        elif aggregation == "top_problems":
            # íŠ¹ë³„í•œ ì¼€ì´ìŠ¤: ë¬¸ì œë³„ TOP N with examples
            if 'problem' in df.columns:
                problem_groups = df.groupby('problem').agg({
                    'verbatim_id': 'count',
                    'verbatim_text': lambda x: list(x.head(3))  # ê° ë¬¸ì œë³„ ì˜ˆì‹œ 3ê°œ
                }).rename(columns={'verbatim_id': 'count'})

                problem_groups = problem_groups.sort_values('count', ascending=False).head(size)

                results = {
                    "aggregation": "top_problems",
                    "problems": [
                        {
                            "problem": problem,
                            "count": int(row['count']),
                            "examples": row['verbatim_text']
                        }
                        for problem, row in problem_groups.iterrows()
                    ],
                    "source_documents": documents  # ì›ë³¸ ë¬¸ì„œë„ í¬í•¨
                }

        aggregation_type = results.get('aggregation', 'unknown')
        if aggregation_type == 'unknown':
            log.warning(f"Aggregation type unknown. Results: {results}")
        else:
            log.info(f"Aggregation complete: {aggregation_type}")
        
        # ê²°ê³¼ì— ì›ë³¸ ë¬¸ì„œ ì •ë³´ ì¶”ê°€ (ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡)
        if isinstance(results, dict) and 'error' not in results and documents:
            results['source_documents'] = documents
            results['total_source_docs'] = len(documents)
            results['documents'] = documents  # ì¶”ê°€: documents í•„ë“œë„ ë…¹ì…
            log.info(f"Added {len(documents)} source documents AND documents to aggregation result")
        
        return results

    def _evaluate_and_apply_filters(self, explicit_filters: Optional[Dict], suggested_filters: Optional[Dict], 
                                   step_description: Optional[str], original_query: Optional[str]) -> Dict[str, Any]:
        """
        Agentic í•„í„° í‰ê°€ ë° ì ìš© ê²°ì •
        ë„êµ¬ê°€ ìŠ¤ìŠ¤ë¡œ ì–´ë–¤ í•„í„°ë¥¼ ì ìš©í• ì§€ ì§€ëŠ¥ì ìœ¼ë¡œ íŒë‹¨
        """
        final_filters = {}
        
        # 1. ëª…ì‹œì  í•„í„°ëŠ” ë¬´ì¡°ê±´ ì ìš©
        if explicit_filters:
            final_filters.update(explicit_filters)
            log.info(f"Applied explicit filters: {explicit_filters}")
        
        # 2. ì œì•ˆëœ í•„í„°ëŠ” ë§¥ë½ì„ ë³´ê³  ì„ íƒì  ì ìš©
        if suggested_filters:
            for key, value in suggested_filters.items():
                should_apply = self._should_apply_suggested_filter(key, value, step_description, original_query)
                if should_apply:
                    final_filters[key] = value
                    log.info(f"Accepted suggested filter: {key}={value}")
                else:
                    log.info(f"Rejected suggested filter: {key}={value} (not relevant to current task)")
        
        return final_filters
    
    def _should_apply_suggested_filter(self, filter_key: str, filter_value: Any, 
                                      step_description: Optional[str], original_query: Optional[str]) -> bool:
        """
        ì œì•ˆëœ í•„í„°ì˜ ì ìš© ì—¬ë¶€ë¥¼ ë§¥ë½ì ìœ¼ë¡œ íŒë‹¨
        """
        if not step_description and not original_query:
            return True  # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ë³´ìˆ˜ì ìœ¼ë¡œ ì ìš©
        
        context_text = f"{step_description or ''} {original_query or ''}".lower()
        
        # í•„í„°ë³„ ë§¥ë½ì  ì ìš© ë¡œì§
        if filter_key == 'model_year':
            # ì—°ë„ê°€ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ë˜ê±°ë‚˜ ì‹œê°„ ê´€ë ¨ ë¶„ì„ì¼ ë•Œë§Œ ì ìš©
            year_mentioned = str(filter_value) in context_text
            time_analysis = any(word in context_text for word in ['ì—°ë„', 'ë…„ë„', 'ì‹œê°„', 'ê¸°ê°„', 'ìµœê·¼', 'ì‘ë…„'])
            return year_mentioned or time_analysis
        
        elif filter_key in ['model', 'vehicle_model']:
            # íŠ¹ì • ì°¨ì¢…ì´ ì–¸ê¸‰ë˜ì—ˆì„ ë•Œë§Œ ì ìš©
            model_mentioned = str(filter_value).lower() in context_text
            return model_mentioned
        
        elif filter_key == 'brand':
            # ë¸Œëœë“œ ë¹„êµë‚˜ íŠ¹ì • ë¸Œëœë“œ ë¶„ì„ì¼ ë•Œë§Œ ì ìš©
            brand_analysis = any(word in context_text for word in ['ë¸Œëœë“œ', 'í˜„ëŒ€', 'ê¸°ì•„', 'ë¹„êµ', 'vs'])
            return brand_analysis
        
        else:
            return True  # ì•Œ ìˆ˜ ì—†ëŠ” í•„í„°ëŠ” ì ìš©
    
    # ğŸ”§ ì œê±°: assess_query_suitability() ë©”ì„œë“œ
    # SelfAssessmentMixinì—ì„œ ì„¤ì • ê¸°ë°˜ìœ¼ë¡œ ìë™ ì œê³µ
    
    # ğŸ”§ ì œê±°: assess_step_suitability() ë©”ì„œë“œ
    # SelfAssessmentMixinì—ì„œ ì„¤ì • ê¸°ë°˜ìœ¼ë¡œ ìë™ ì œê³µ
    
    def _extract_problem_description(self, problem: str) -> str:
        """
        Problem í•„ë“œì—ì„œ ë¬¸ì œ ì„¤ëª… ì¶”ì¶œ
        ì˜ˆ: "INFO12: Built-in navigation - Broken/works inconsistently" 
        â†’ "Built-in navigation - Broken/works inconsistently"
        """
        if ':' in problem:
            return problem.split(':', 1)[1].strip()
        return problem
    
    def get_description(self) -> str:
        return self.description


class HybridSearchTool(BaseTool):
    """
    í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë„êµ¬
    ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ + í•„í„°ë§ ê²°í•©
    """

    def __init__(self):
        """Initialize Hybrid Search Tool"""
        self.name = "hybrid_search"
        self.description = "ì§€ëŠ¥í˜• ê²€ìƒ‰: í•œê¸€ ë²ˆì—­, ì½”ë“œ í•´ì„, ì˜ë¯¸ ê²€ìƒ‰ í¬í•¨"
        
        # ğŸ”§ ìˆ˜ì •: Mixin ì´ˆê¸°í™” (ìê¸° í‰ê°€ ë° ì—ëŸ¬ ì²˜ë¦¬ ê¸°ëŠ¥ í¬í•¨)
        super().__init__()

        # ê¸°ì¡´ SemanticSearcher í™œìš© (Ollama BGE-M3 + Elasticsearch)
        self.searcher = SemanticSearcher(
            embedding_model=EmbeddingModel(model_type="ollama"),
            use_elasticsearch=True
        )
        
        # ë‚´ì¥ëœ ì§€ì‹ë² ì´ìŠ¤ ì´ˆê¸°í™”
        self.knowledge_base = self._load_integrated_knowledge()

        # ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
        if not self.searcher.load_index():
            log.warning("No pre-built index found. Need to build index first.")

        log.info("Initialized HybridSearchTool with assessment capabilities")

    def execute(self, 
                query: str,
                limit: int = 5000,
                filters: Optional[Dict] = None,
                year: Optional[int] = None,
                model: Optional[str] = None,
                part: Optional[str] = None,
                # ğŸ†• ë²¡í„° ê²€ìƒ‰ ì˜µì…˜
                search_type: str = "hybrid",  # "text", "vector", "hybrid"
                enable_vector_search: bool = True,
                # Agentic íŒŒë¼ë¯¸í„°ë“¤
                suggested_filters: Optional[Dict] = None,
                step_description: Optional[str] = None,
                original_query: Optional[str] = None,
                **kwargs) -> Dict[str, Any]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            limit: ê²°ê³¼ ê°œìˆ˜
            filters: í•„í„° ì¡°ê±´
            year: ì—°ë„ í•„í„°
            model: ì°¨ì¢… í•„í„°
            part: ë¶€í’ˆ í•„í„°

        Returns:
            ê²€ìƒ‰ ê²°ê³¼
        """
        # INFO ì½”ë“œ ë¶„ì„ì´ë‚˜ ì „ì²´ ë°ì´í„° ë¶„ì„ì„ ìœ„í•´ ê°•ì œë¡œ ëŒ€ìš©ëŸ‰ ê²€ìƒ‰
        if 'INFO' in query.upper() or 'ì „ì²´' in query or 'ì „ìˆ˜' in query or limit < 1000:
            original_limit = limit
            limit = 5000  # ê°•ì œë¡œ 5000ê°œë¡œ ì„¤ì •
            log.info(f"Forced large-scale search: original limit={original_limit} -> forced limit={limit}")
        
        log.info(f"Executing intelligent search: '{query}' with limit={limit}")
        
        # Agentic ì»¨í…ìŠ¤íŠ¸ ì¸ì‹
        if step_description:
            log.info(f"Agentic context: {step_description}")
        
        # ì§€ëŠ¥ì  í•„í„° í‰ê°€ ë° ì ìš©
        intelligent_filters = self._evaluate_search_filters(filters, suggested_filters, step_description, original_query)
        
        # ğŸ”§ ê°œì„ ëœ ì¿¼ë¦¬ ì²˜ë¦¬: ë¬¸ì œ ì½”ë“œ ì •í™•í•œ íƒ€ê²ŸíŒ…
        problem_codes = self._extract_problem_codes(query)
        if problem_codes:
            log.info(f"Problem codes detected: {problem_codes}")
            return self._search_by_problem_codes(problem_codes, intelligent_filters, limit, query)
        else:
            enhanced_query = self._enhance_query_with_knowledge(query)
            if enhanced_query != query:
                log.info(f"Query enhanced: '{query}' -> '{enhanced_query}'")
                query = enhanced_query

        # ì§€ëŠ¥ì  í•„í„° í†µí•© (ê¸°ì¡´ ë°©ì‹ ëŒ€ì²´)
        final_filters = intelligent_filters.copy()
        
        # ê°œë³„ íŒŒë¼ë¯¸í„° í†µí•©
        if year:
            final_filters['model_year'] = year
        if model:
            final_filters['model'] = model
        if part:
            final_filters['part'] = part

        # ğŸ†• ê²€ìƒ‰ ì‹¤í–‰ - ë²¡í„° ê²€ìƒ‰ ì§€ì›
        try:
            # ë²¡í„° ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€ ê²°ì •
            if enable_vector_search and search_type != "text":
                actual_search_type = search_type
                log.info(f"Using {actual_search_type} search with BGE-M3 vectors")
            else:
                actual_search_type = "text"
                log.info("Using text-only search (vector search disabled)")
            
            results = self.searcher.search(
                query=query,
                k=limit,
                filters=final_filters if final_filters else None,
                search_type=actual_search_type
            )

            # SearchResult ê°ì²´ë¥¼ Dictë¡œ ë³€í™˜
            documents = []
            for result in results:
                doc = result.content.copy()
                doc['_score'] = result.score
                doc['_matched_text'] = result.matched_text
                doc['_search_type'] = actual_search_type
                documents.append(doc)

            # ğŸ”’ Hallucination ë°©ì§€: ê²€ìƒ‰ ê²°ê³¼ ê²€ì¦ ë° íˆ¬ëª…ì„±
            output = {
                "query": query,
                "filters": final_filters,
                "total_hits": len(documents),
                "documents": documents,
                "search_type": actual_search_type,
                "vector_search_enabled": enable_vector_search,
                "intelligent_filtering_applied": bool(intelligent_filters),
                "search_verification": {
                    "actual_results_found": len(documents),
                    "search_method_used": actual_search_type,
                    "filters_applied": final_filters,
                    "hallucination_prevented": True,
                    "search_timestamp": __import__('time').strftime('%Y-%m-%d %H:%M:%S'),
                    "minimum_threshold_met": len(documents) > 0
                }
            }
            
            # ğŸš¨ ê²°ê³¼ ë¶€ì¡± ê²½ê³ 
            if len(documents) == 0:
                log.warning(f"ZERO RESULTS for query '{query}' with filters {final_filters}")
                output["warning"] = "No documents found - avoid hallucination"

            log.success(f"{actual_search_type.title()} search returned {len(documents)} results (requested: {limit})")
            return output

        except Exception as e:
            log.error(f"Search failed: {e}")
            return {
                "error": str(e),
                "query": query,
                "filters": final_filters,
                "total_hits": 0,
                "documents": [],
                "search_verification": {
                    "search_failed": True,
                    "error_message": str(e),
                    "hallucination_prevented": True,
                    "recommendation": "Modify search criteria or check data availability"
                }
            }

    def _load_integrated_knowledge(self) -> Dict:
        """ë‚´ì¥ëœ ì§€ì‹ë² ì´ìŠ¤ ë¡œë“œ"""
        try:
            import json
            from pathlib import Path
            
            kb_file = Path("iqs_knowledge_base.json")
            if kb_file.exists():
                with open(kb_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            log.warning(f"Failed to load integrated knowledge base: {e}")
        
        return {}
    
    def _extract_problem_codes(self, query: str) -> List[str]:
        """
        ì¿¼ë¦¬ì—ì„œ ë¬¸ì œ ì½”ë“œ íŒ¨í„´ ì¶”ì¶œ
        
        Args:
            query: ì‚¬ìš©ì ì¿¼ë¦¬
            
        Returns:
            ì¶”ì¶œëœ ë¬¸ì œ ì½”ë“œ ë¦¬ìŠ¤íŠ¸
        """
        import re
        problem_codes = []
        
        # ğŸ”§ ê°œì„ ëœ ì½”ë“œ ì¶”ì¶œ: ëª¨ë“  íŒ¨í„´ ì§€ì›
        
        # 1. ì™„ì „í•œ ì½”ë“œ íŒ¨í„´ (INFO12, EXT16 ë“±)
        full_code_pattern = r'(?:INFO|EXT|FCD|DRA|CLMT|INT|PWR)\d{1,2}'
        full_matches = re.findall(full_code_pattern, query, re.IGNORECASE)
        problem_codes.extend([code.upper() for code in full_matches])
        
        # 2. INFO + ìˆ«ì ì¡°í•© íŒ¨í„´ (INFO12/13/14, INFO22,23 ë“±)
        info_combo_pattern = r'INFO\s*(?:\d{1,2}(?:[,/]\s*\d{1,2})*|\d{1,2}[/-]\d{1,2})'
        info_combos = re.findall(info_combo_pattern, query, re.IGNORECASE)
        
        for combo in info_combos:
            # ìˆ«ìë§Œ ì¶”ì¶œ
            numbers = re.findall(r'\d{1,2}', combo)
            for num in numbers:
                problem_codes.append(f'INFO{num}')
        
        # 3. ë‹¨ë… ìˆ«ì í›„ INFO ë¬¸ë§¥ ì²˜ë¦¬ ("22, 23 í•­ëª©ì½”ë“œ")
        context_number_pattern = r'(\d{1,2})(?:\s*,\s*(\d{1,2}))*\s*í•­ëª©\s*ì½”ë“œ'
        context_matches = re.findall(context_number_pattern, query)
        
        for match in context_matches:
            if isinstance(match, tuple):
                for num in match:
                    if num:  # ë¹ˆ ë¬¸ìì—´ ì œì™¸
                        problem_codes.append(f'INFO{num}')
            else:
                problem_codes.append(f'INFO{match}')
        
        # 4. ì½”ë“œ + "ì— í•´ë‹¹" íŒ¨í„´ ì²˜ë¦¬ (EXT16ì— í•´ë‹¹í•˜ëŠ”)
        attached_pattern = r'([A-Z]{2,5}\d{1,2})ì—\s*í•´ë‹¹'
        attached_matches = re.findall(attached_pattern, query, re.IGNORECASE)
        problem_codes.extend([code.upper() for code in attached_matches])
        
        # 5. ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        unique_codes = list(set(problem_codes))
        
        # INFO ì½”ë“œ ìš°ì„  ì •ë ¬
        info_codes = [code for code in unique_codes if code.startswith('INFO')]
        other_codes = [code for code in unique_codes if not code.startswith('INFO')]
        
        return info_codes + other_codes
    
    def _search_by_problem_codes(self, problem_codes: List[str], filters: Dict, limit: int, original_query: str) -> Dict[str, Any]:
        """
        ë¬¸ì œ ì½”ë“œ ê¸°ë°˜ ì •í™•í•œ ê²€ìƒ‰
        
        Args:
            problem_codes: ê²€ìƒ‰í•  ë¬¸ì œ ì½”ë“œ ë¦¬ìŠ¤íŠ¸
            filters: ê¸°ì¡´ í•„í„°ë“¤
            limit: ê²°ê³¼ ìˆ˜ ì œí•œ
            original_query: ì›ë³¸ ì¿¼ë¦¬
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼
        """
        log.info(f"Executing problem code search for: {problem_codes}")
        
        try:
            all_results = []
            
            for code in problem_codes:
                # Elasticsearchë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê²½ìš° ì²´í¬ ìˆ˜ì •
                if hasattr(self.searcher, 'es_client') and self.searcher.es_client:
                    code_results = self._es_search_by_problem_code(code, filters)
                    all_results.extend(code_results)
                else:
                    # ë¡œì»¬ ê²€ìƒ‰ í´ë°±
                    code_results = self._local_search_by_problem_code(code, filters)
                    all_results.extend(code_results)
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            unique_results = self._deduplicate_and_sort(all_results, limit)
            
            log.info(f"Problem code search found {len(unique_results)} documents")
            
            return {
                "query": original_query,
                "problem_codes_searched": problem_codes,
                "search_method": "problem_code_targeting",
                "filters": filters,
                "total_hits": len(unique_results),
                "documents": unique_results
            }
            
        except Exception as e:
            log.error(f"Problem code search failed: {e}")
            # í´ë°±: ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰
            return self._fallback_text_search(original_query, filters, limit)
    
    def _es_search_by_problem_code(self, code: str, filters: Dict) -> List[Dict]:
        """
        Elasticsearchë¥¼ ì‚¬ìš©í•œ ë¬¸ì œ ì½”ë“œ ì •í™• ê²€ìƒ‰
        """
        try:
            # ğŸ”§ ìˆ˜ì •ëœ ë¬¸ì œ ì½”ë“œ ê²€ìƒ‰ - match ì¿¼ë¦¬ ì‚¬ìš©
            es_query = {
                "bool": {
                    "must": [
                        {
                            "match": {
                                "problem": code  # INFO12, EXT16 ë“± ì½”ë“œ ë§¤ì¹­
                            }
                        }
                    ],
                    "filter": []
                }
            }
            
            # ì¶”ê°€ í•„í„° ì ìš©
            if filters:
                for key, value in filters.items():
                    es_query["bool"]["filter"].append({
                        "term": {key: value}
                    })
            
            # Elasticsearch ì‹¤í–‰ ìˆ˜ì •
            es_response = self.searcher.es_client.search({
                "query": es_query,
                "size": 1000,  # ì½”ë“œë³„ë¡œ ìµœëŒ€ 1000ê°œ
                "_source": True
            })
            
            results = []
            for hit in es_response['hits']['hits']:
                doc = hit['_source']
                problem = doc.get('problem', '')
                
                # ğŸ”§ í›„ì²˜ë¦¬ í•„í„°ë§: ì •í™•í•œ ì½”ë“œë§Œ ì„ íƒ
                if problem.startswith(f"{code}:"):
                    doc['_score'] = hit['_score']
                    doc['_search_method'] = f'es_match_filtered_{code}'
                    results.append(doc)
            
            log.info(f"ES search for {code}: {len(results)} documents")
            return results
            
        except Exception as e:
            log.error(f"ES search for {code} failed: {e}")
            return []
    
    def _local_search_by_problem_code(self, code: str, filters: Dict) -> List[Dict]:
        """
        ì¼ë°˜ ì˜ë¯¸ ê²€ìƒ‰ì„ ì‚¬ìš©í•œ ë¬¸ì œ ì½”ë“œ ê²€ìƒ‰
        """
        try:
            log.info(f"Using semantic search for {code} problem code")
            
            # ì¼ë°˜ ì˜ë¯¸ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´
            search_query = f"{code} problem code"
            if filters and 'make_of_vehicle' in filters:
                search_query += f" {filters['make_of_vehicle']}"
            
            semantic_results = self.searcher.search(
                query=search_query,
                k=1000,  # ì½”ë“œë³„ë¡œ 1000ê°œ
                filters=filters
            )
            
            results = []
            for result in semantic_results:
                doc = result.content.copy()
                problem = doc.get('problem', '')
                
                # ğŸ”§ ì •í™•í•œ ì½”ë“œ ë§¤ì¹­ í™•ì¸
                if problem.startswith(f"{code}:"):
                    doc['_score'] = result.score
                    doc['_search_method'] = f'semantic_filtered_problem_code_{code}'
                    results.append(doc)
            
            log.info(f"Semantic search for {code}: found {len(results)} matching documents")
            return results
            
        except Exception as e:
            log.error(f"Semantic search for {code} failed: {e}")
            return []
    
    def _deduplicate_and_sort(self, results: List[Dict], limit: int) -> List[Dict]:
        """
        ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        """
        # verbatim_id ê¸°ì¤€ ì¤‘ë³µ ì œê±°
        seen_ids = set()
        unique_results = []
        
        for doc in results:
            doc_id = doc.get('verbatim_id', '') or doc.get('_id', '')
            if doc_id and doc_id not in seen_ids:
                seen_ids.add(doc_id)
                unique_results.append(doc)
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        unique_results.sort(key=lambda x: x.get('_score', 0), reverse=True)
        
        return unique_results[:limit]
    
    def _fallback_text_search(self, query: str, filters: Dict, limit: int) -> Dict[str, Any]:
        """
        ë¬¸ì œ ì½”ë“œ ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ í´ë°±
        """
        log.warning("Falling back to text search")
        
        try:
            results = self.searcher.search(
                query=query,
                k=limit,
                filters=filters if filters else None
            )
            
            documents = []
            for result in results:
                doc = result.content.copy()
                doc['_score'] = result.score
                doc['_matched_text'] = result.matched_text
                doc['_search_method'] = 'fallback_text'
                documents.append(doc)

            return {
                "query": query,
                "search_method": "fallback_text_search",
                "filters": filters,
                "total_hits": len(documents),
                "documents": documents
            }
            
        except Exception as e:
            log.error(f"Fallback search also failed: {e}")
            return {
                "error": str(e),
                "query": query,
                "total_hits": 0,
                "documents": []
            }

    def _enhance_query_with_knowledge(self, query: str) -> str:
        """
        ì§€ì‹ë² ì´ìŠ¤ë¥¼ í™œìš©í•œ ì§€ëŠ¥í˜• ì¿¼ë¦¬ í–¥ìƒ
        ğŸ”§ ê°œì„ : ë²ˆì—­ì€ GlossaryToolì—ì„œ ì²˜ë¦¬, ì—¬ê¸°ì„œëŠ” ì½”ë“œ í•´ì„ë§Œ
        
        Args:
            query: ì›ë³¸ ì¿¼ë¦¬
            
        Returns:
            í–¥ìƒëœ ì¿¼ë¦¬
        """
        try:
            enhanced_query = query
            
            # 1. ğŸ”§ ë²ˆì—­ ë¡œì§ ì œê±° (GlossaryToolì—ì„œ ì²˜ë¦¬)
            # í•œê¸€ ë²ˆì—­ì€ LangGraph Agentê°€ GlossaryToolì„ í˜¸ì¶œí•´ì„œ ì²˜ë¦¬
            
            # 2. ì½”ë“œ í•´ì„ ë° í™•ì¥ (ìœ ì§€)
            enhanced_query = self._expand_code_terms(enhanced_query)
            
            # 3. ë„ë©”ì¸ ìš©ì–´ í•´ì„ (ìœ ì§€)
            enhanced_query = self._expand_domain_terms(enhanced_query)
            
            # 4. ğŸ”§ GlossaryTool ê²°ê³¼ í™œìš© ì‹œë„
            glossary_enhanced = self._try_extract_from_glossary_result()
            if glossary_enhanced:
                log.info(f"Using GlossaryTool result: '{query}' -> '{glossary_enhanced}'")
                enhanced_query = glossary_enhanced
            
            return enhanced_query
            
        except Exception as e:
            log.warning(f"Query enhancement failed: {e}")
            return query
    
    def _try_extract_from_glossary_result(self) -> Optional[str]:
        """
        ğŸ”§ ìƒˆë¡œìš´ ê¸°ëŠ¥: ì´ì „ ê²°ê³¼ì—ì„œ GlossaryToolì˜ ë²ˆì—­ ê²°ê³¼ ì¶”ì¶œ
        LangGraph Agentê°€ GlossaryTool -> HybridSearchTool ìˆœì„œë¡œ í˜¸ì¶œí–ˆì„ ë•Œ í™œìš©
        
        Returns:
            GlossaryToolì—ì„œ ë²ˆì—­ëœ ì¿¼ë¦¬ ë˜ëŠ” None
        """
        # í˜„ì¬ LangGraphì˜ ì‹œìŠ¤í…œ ìƒíƒœë¥¼ í†µí•´ ì´ì „ ê²°ê³¼ ì ‘ê·¼ ì‹œë„
        # (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” previous_resultë‚˜ contextë¥¼ í†µí•´ ì ‘ê·¼)
        
        # TODO: ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” LangGraph stateë¥¼ í†µí•´ ì ‘ê·¼
        # ì˜ˆì‹œ: state.get('glossary_results', {})
        
        return None
    
    
    def _expand_code_terms(self, query: str) -> str:
        """ì½”ë“œ ìš©ì–´ í™•ì¥"""
        # ë¬¸ì œ ì½”ë“œ íŒ¨í„´ ì°¾ê¸°
        import re
        code_pattern = r'\b([A-Z]{2,5}\d{1,3})\b'
        codes = re.findall(code_pattern, query)
        
        if not codes or not self.knowledge_base.get('problem_codes', {}).get('codes'):
            return query
        
        enhanced_parts = [query]
        
        for code in codes:
            code_info = self.knowledge_base['problem_codes']['codes'].get(code)
            if code_info:
                # ì½”ë“œ ì„¤ëª…ì„ ê²€ìƒ‰ì–´ì— ì¶”ê°€
                description = code_info.get('description', '')
                # ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
                keywords = re.findall(r'\b\w{3,}\b', description.lower())
                if keywords:
                    enhanced_parts.extend(keywords[:3])  # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œ
        
        return ' '.join(enhanced_parts)
    
    def _expand_domain_terms(self, query: str) -> str:
        """ë„ë©”ì¸ ìš©ì–´ í™•ì¥"""
        domain_expansions = {
            'DTU': 'difficult to use usability problem',
            'OEM': 'original equipment manufacturer',
            'IQS': 'initial quality study problem'
        }
        
        enhanced_query = query
        for term, expansion in domain_expansions.items():
            if term in query:
                enhanced_query = enhanced_query.replace(term, f"{term} {expansion}")
        
        return enhanced_query
    
    def _validate_and_enhance_search(self, original_query: str, documents: List[Dict], limit: int, filters: Optional[Dict]) -> Dict[str, Any]:
        """
        ë™ì  ê²€ìƒ‰ ê²°ê³¼ ê²€ì¦ ë° í•„ìš”ì‹œ ëŒ€ì•ˆ ê²€ìƒ‰
        LangGraph ì² í•™: ë„êµ¬ê°€ ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•˜ê³  ì ì‘
        """
        import re
        
        # 1. INFO ì½”ë“œ ê²€ìƒ‰ ê²°ê³¼ ê²€ì¦
        requested_info_codes = re.findall(r'INFO\d{1,2}', original_query.upper())
        
        if requested_info_codes:
            # ìš”ì²­ëœ INFO ì½”ë“œì™€ ì‹¤ì œ ê²°ê³¼ ë¹„êµ
            found_codes = set()
            for doc in documents:
                problem = doc.get('problem', '')
                doc_codes = re.findall(r'INFO\d{1,2}', problem)
                found_codes.update(doc_codes)
            
            missing_codes = set(requested_info_codes) - found_codes
            
            if missing_codes:
                log.warning(f"Requested codes {requested_info_codes} but only found {list(found_codes)}. Missing: {list(missing_codes)}")
                
                # ìë™ ëŒ€ì•ˆ ê²€ìƒ‰ ì‹œë„
                alternative_documents = self._try_alternative_search(original_query, missing_codes, limit, filters)
                
                if alternative_documents:
                    log.info(f"Alternative search found {len(alternative_documents)} additional results")
                    # ê¸°ì¡´ ê²°ê³¼ì™€ ëŒ€ì•ˆ ê²°ê³¼ ê²°í•©
                    combined_docs = documents + alternative_documents
                    # ì¤‘ë³µ ì œê±° (ë¬¸ì„œ ID ê¸°ì¤€)
                    seen_ids = set()
                    unique_docs = []
                    for doc in combined_docs:
                        doc_id = doc.get('verbatim_id') or doc.get('_id')
                        if doc_id not in seen_ids:
                            seen_ids.add(doc_id)
                            unique_docs.append(doc)
                    
                    documents = unique_docs[:limit]  # limit ìœ ì§€
        
        return {
            "query": original_query,
            "filters": filters,
            "total_hits": len(documents),
            "documents": documents,
            "search_enhanced": len(requested_info_codes) > 0 and bool(missing_codes)
        }
    
    def _try_alternative_search(self, original_query: str, missing_codes: set, limit: int, filters: Optional[Dict]) -> List[Dict]:
        """
        ëŒ€ì•ˆ ê²€ìƒ‰ ì „ëµ: ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰
        """
        # ì‹¤ì œ ë°ì´í„°ì—ì„œ í•™ìŠµí•œ íŒ¨í„´ì„ ì‚¬ìš©
        # í•˜ë“œì½”ë”© ëŒ€ì‹  ë™ì  ì˜ë¯¸ ê²€ìƒ‰
        alternative_queries = []
        
        for code in missing_codes:
            # ì½”ë“œ ë²ˆí˜¸ë³„ ëŒ€ì•ˆ ì „ëµ
            if '12' in code:
                alternative_queries.append('navigation broken inconsistent works')
            elif '13' in code:
                alternative_queries.append('navigation DTU difficult update')
            elif '14' in code:
                alternative_queries.append('navigation inaccurate wrong incorrect')
            else:
                # ì•Œë ¤ì§€ì§€ ì•Šì€ ì½”ë“œëŠ” ì¼ë°˜ì  ì¸í¬í…Œì¸ë¨¼íŠ¸ ê²€ìƒ‰
                alternative_queries.append('infotainment problem')
        
        if not alternative_queries:
            return []
        
        # ëŒ€ì•ˆ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
        combined_query = ' OR '.join(alternative_queries)
        log.info(f"Trying alternative search: {combined_query}")
        
        try:
            alt_results = self.searcher.search(
                query=combined_query,
                k=limit,
                filters=filters
            )
            
            alt_documents = []
            for result in alt_results:
                doc = result.content.copy()
                doc['_score'] = result.score * 0.9  # ëŒ€ì•ˆ ê²€ìƒ‰ì€ ì•½ê°„ ë‚®ì€ ì ìˆ˜
                doc['_matched_text'] = result.matched_text
                doc['_search_method'] = 'alternative_semantic'
                alt_documents.append(doc)
            
            return alt_documents
            
        except Exception as e:
            log.warning(f"Alternative search failed: {e}")
            return []
    
    def _evaluate_search_filters(self, explicit_filters: Optional[Dict], suggested_filters: Optional[Dict],
                               step_description: Optional[str], original_query: Optional[str]) -> Dict[str, Any]:
        """
        Agentic ê²€ìƒ‰ í•„í„° í‰ê°€ (HybridSearchTool ì „ìš©)
        ê²€ìƒ‰ ë„êµ¬ê°€ ìŠ¤ìŠ¤ë¡œ ì–´ë–¤ í•„í„°ë¥¼ ì ìš©í• ì§€ íŒë‹¨
        """
        final_filters = {}
        
        # 1. ëª…ì‹œì  í•„í„°ëŠ” ë¬´ì¡°ê±´ ì ìš©
        if explicit_filters:
            final_filters.update(explicit_filters)
            log.info(f"Applied explicit search filters: {explicit_filters}")
        
        # 2. ì œì•ˆëœ í•„í„°ë¥¼ ë§¥ë½ì ìœ¼ë¡œ í‰ê°€
        if suggested_filters:
            for key, value in suggested_filters.items():
                should_apply = self._should_apply_search_filter(key, value, step_description, original_query)
                if should_apply:
                    final_filters[key] = value
                    log.info(f"Search tool accepted suggested filter: {key}={value}")
                else:
                    log.info(f"Search tool rejected suggested filter: {key}={value} (not contextually relevant)")
        
        return final_filters
    
    def _should_apply_search_filter(self, filter_key: str, filter_value: Any,
                                   step_description: Optional[str], original_query: Optional[str]) -> bool:
        """
        ê²€ìƒ‰ íŠ¹í™” í•„í„° ì ìš© íŒë‹¨
        """
        if not step_description and not original_query:
            return True
        
        context_text = f"{step_description or ''} {original_query or ''}".lower()
        
        # ê²€ìƒ‰ ë„êµ¬ëŠ” ë³´ë‹¤ ì—´ë¦° í•„í„°ë§ ì •ì±…
        if filter_key == 'model_year':
            # ì—°ë„ê°€ ëª…ì‹œë˜ê±°ë‚˜ ìµœê·¼ ë°ì´í„° ìš”ì²­ ì‹œ ì ìš©
            year_mentioned = str(filter_value) in context_text
            recent_request = any(word in context_text for word in ['ìµœê·¼', 'ì‘ë…„', 'ê¸ˆë…„', 'ì˜¬í•´'])
            return year_mentioned or recent_request
        
        elif filter_key in ['model', 'vehicle_model']:
            # íŠ¹ì • ì°¨ì¢… ì–¸ê¸‰ ì‹œë§Œ ì ìš©
            return str(filter_value).lower() in context_text
        
        elif filter_key == 'brand':
            # ë¸Œëœë“œ ê´€ë ¨ ë¶„ì„ì´ë‚˜ ë¹„êµ ì‹œë§Œ ì ìš©
            brand_context = any(word in context_text for word in ['ë¸Œëœë“œ', 'í˜„ëŒ€', 'ê¸°ì•„', 'ë¹„êµ'])
            return brand_context
        
        else:
            # ë‹¤ë¥¸ í•„í„°ë“¤ì€ ê¸°ë³¸ì ìœ¼ë¡œ ìˆ˜ìš©
            return True
    
    # ğŸ”§ ì¤‘ë³µ ì œê±°: assess_query_suitability() ë©”ì„œë“œ ì œê±°
    # SelfAssessmentMixinì—ì„œ ì„¤ì • ê¸°ë°˜ìœ¼ë¡œ ìë™ ì œê³µ
    
    # ğŸ”§ ì¤‘ë³µ ì œê±°: assess_step_suitability() ë©”ì„œë“œ ì œê±°  
    # SelfAssessmentMixinì—ì„œ ì„¤ì • ê¸°ë°˜ìœ¼ë¡œ ìë™ ì œê³µ
    
    def build_index(self, documents: List[Dict[str, Any]]):
        """
        ë¬¸ì„œë¡œë¶€í„° ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶•

        Args:
            documents: ì¸ë±ì‹±í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        log.info(f"Building search index with {len(documents)} documents")
        self.searcher.build_index_from_documents(documents)

    def get_description(self) -> str:
        return self.description


class RerankerTool(BaseTool):
    """
    ì¬ìˆœìœ„í™” ë„êµ¬
    ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‹¤ì–‘í•œ ì‹ í˜¸ë¥¼ í™œìš©í•˜ì—¬ ì¬ìˆœìœ„í™”
    """

    def __init__(self):
        """Initialize Reranker Tool"""
        self.name = "reranker"
        self.description = "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê´€ë ¨ì„± ê¸°ì¤€ìœ¼ë¡œ ì¬ìˆœìœ„í™”"
        
        # ğŸ”§ ìˆ˜ì •: Mixin ì´ˆê¸°í™” (ìê¸° í‰ê°€ ë° ì—ëŸ¬ ì²˜ë¦¬ ê¸°ëŠ¥ í¬í•¨)
        super().__init__()
        
        self.reranker = Reranker()
        log.info("Initialized RerankerTool with assessment capabilities")

    def execute(self,
                documents: Optional[List[Dict]] = None,
                query: Optional[str] = None,
                top_k: int = 10,
                previous_result: Optional[Any] = None,
                filter: Optional[Dict] = None,  # ë¸Œëœë“œ í•„í„°ë§ ë“±
                **kwargs) -> Dict[str, Any]:
        """
        ì¬ìˆœìœ„í™” ì‹¤í–‰

        Args:
            documents: ì¬ìˆœìœ„í™”í•  ë¬¸ì„œë“¤
            query: ì›ë³¸ ì¿¼ë¦¬
            top_k: ìƒìœ„ Kê°œ ë°˜í™˜
            previous_result: ì´ì „ ë‹¨ê³„ ê²°ê³¼

        Returns:
            ì¬ìˆœìœ„í™”ëœ ê²°ê³¼
        """
        log.info(f"Executing reranking with top_k={top_k}")

                            # ğŸ”§ ê°œì„ : í†µí•©ëœ ë¬¸ì„œ ì¶”ì¶œ ë¡œì§ ì‚¬ìš©
        documents, extracted_query = DocumentExtractor.extract_from_previous_result(
            previous_result=previous_result,
            documents=documents,
            fallback_fields=['source_documents'],
            extract_query=True,
            log_details=True
        )
        
        # ì¶”ì¶œëœ ì¿¼ë¦¬ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
        if query is None and extracted_query:
            query = extracted_query

        if not documents:
            log.warning(f"No documents to rerank. Previous result available: {previous_result is not None}")
            return {
                "error": "No documents provided",
                "query": query,
                "total_reranked": 0,
                "top_k": top_k,
                "documents": [],
                "debug_info": {
                    "document_extraction_attempted": True,
                    "previous_result_available": previous_result is not None
                }
            }
        
        # LLM ì§€ì‹œì— ë”°ë¥¸ í•„í„°ë§ ìˆ˜í–‰
        if filter:
            original_count = len(documents)
            for filter_key, filter_value in filter.items():
                if filter_key == 'brand' and filter_value.lower() in ['hyundai', 'í˜„ëŒ€']:
                    documents = [doc for doc in documents 
                               if doc.get('make_of_vehicle', '').lower() in ['hyundai', 'í˜„ëŒ€']]
                    log.info(f"Filtered by Hyundai brand: {original_count} -> {len(documents)} documents")
                elif filter_key in ['make_of_vehicle', 'brand']:
                    documents = [doc for doc in documents 
                               if doc.get('make_of_vehicle', '').lower() == filter_value.lower()]
                    log.info(f"Filtered by brand '{filter_value}': {original_count} -> {len(documents)} documents")
            
            # í•„í„°ë§ í›„ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì˜¤ë¥˜ ë°˜í™˜
            if not documents:
                return {
                    "error": f"No documents found after filtering by {filter}",
                    "query": query,
                    "total_reranked": 0,
                    "top_k": top_k,
                    "documents": [],
                    "filter_applied": filter
                }

        if not query:
            log.warning("No query provided for reranking")
            # ì¿¼ë¦¬ ì—†ì´ë„ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì¬ìˆœìœ„í™”ëŠ” ê°€ëŠ¥

        # SearchResult ê°ì²´ë¡œ ë³€í™˜
        search_results = []
        for doc in documents:
            # ì ìˆ˜ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
            score = doc.get('_score', 0.5)

            result = SearchResult(
                doc_id=doc.get('verbatim_id', ''),
                score=score,
                content=doc,
                matched_text=doc.get('verbatim_text', '')
            )
            search_results.append(result)

        # ì¬ìˆœìœ„í™” ì‹¤í–‰
        reranked = self.reranker.rerank(
            results=search_results,
            query=query or "",
            query_plan=None,  # Agent ëª¨ë“œì—ì„œëŠ” query_plan ì—†ìŒ
            top_k=top_k
        )

        # ê²°ê³¼ ë³€í™˜
        reranked_docs = []
        for result, score in reranked:
            doc = result.content.copy()
            doc['_score'] = score.total_score
            doc['_rerank_details'] = {
                'semantic': score.semantic_score,
                'relevance': score.relevance_score,
                'recency': score.recency_score,
                'metadata': score.metadata_score,
                'explanation': score.explanation
            }
            reranked_docs.append(doc)

        output = {
            "query": query,
            "total_reranked": len(reranked_docs),
            "top_k": top_k,
            "documents": reranked_docs
        }

        log.info(f"Reranking complete: {len(reranked_docs)} documents")
        
        # í•„í„° ì •ë³´ ì¶”ê°€
        if filter:
            output['filter_applied'] = filter
            
        return output

    def _evaluate_reranking_filters(self, explicit_filter: Optional[Dict], suggested_filters: Optional[Dict],
                                   step_description: Optional[str], original_query: Optional[str]) -> Optional[Dict]:
        """
        Agentic ì¬ìˆœìœ„í™” í•„í„° í‰ê°€
        ì¬ìˆœìœ„í™” ë„êµ¬ê°€ ìŠ¤ìŠ¤ë¡œ í•„í„°ë§ í•„ìš”ì„±ì„ íŒë‹¨
        """
        # 1. ëª…ì‹œì  í•„í„°ê°€ ìˆìœ¼ë©´ ìš°ì„  ì ìš©
        if explicit_filter:
            log.info(f"Reranker applying explicit filter: {explicit_filter}")
            return explicit_filter
        
        # 2. ì œì•ˆëœ í•„í„°ë¥¼ ë§¥ë½ì ìœ¼ë¡œ í‰ê°€
        if suggested_filters:
            context_text = f"{step_description or ''} {original_query or ''}".lower()
            
            # ë¸Œëœë“œ í•„í„°ë§ì´ ì˜ë¯¸ê°€ ìˆëŠ”ì§€ íŒë‹¨
            brand_context = any(word in context_text for word in [
                'ë¸Œëœë“œ', 'í˜„ëŒ€', 'ê¸°ì•„', 'ë¹„êµ', 'vs', 'ì „ìš©', 'íŠ¹ì •'
            ])
            
            if brand_context and 'brand' in suggested_filters:
                return {'brand': suggested_filters['brand']}
            elif brand_context and 'model_year' in suggested_filters:
                return {'model_year': suggested_filters['model_year']}
        
        # 3. í•„í„°ë§ì´ í•„ìš” ì—†ë‹¤ê³  íŒë‹¨
        return None
    
    def _apply_intelligent_filter(self, documents: List[Dict], filter_config: Dict) -> List[Dict]:
        """
        ì§€ëŠ¥ì  í•„í„° ì ìš©
        """
        filtered_docs = documents.copy()
        
        for filter_key, filter_value in filter_config.items():
            if filter_key == 'brand':
                if filter_value.lower() in ['hyundai', 'í˜„ëŒ€']:
                    filtered_docs = [doc for doc in filtered_docs 
                                   if doc.get('make_of_vehicle', '').lower() in ['hyundai', 'í˜„ëŒ€']]
                elif filter_value.lower() in ['kia', 'ê¸°ì•„']:
                    filtered_docs = [doc for doc in filtered_docs 
                                   if doc.get('make_of_vehicle', '').lower() in ['kia', 'ê¸°ì•„']]
                else:
                    filtered_docs = [doc for doc in filtered_docs 
                                   if doc.get('make_of_vehicle', '').lower() == filter_value.lower()]
            
            elif filter_key == 'model_year':
                filtered_docs = [doc for doc in filtered_docs 
                               if doc.get('model_year') == filter_value]
            
            elif filter_key in ['model', 'vehicle_model']:
                filtered_docs = [doc for doc in filtered_docs 
                               if doc.get('model_of_vehicle', '').lower() == str(filter_value).lower()]
        
        return filtered_docs
    
    # ğŸ”§ ì¤‘ë³µ ì œê±°: assess_query_suitability() ë©”ì„œë“œ ì œê±°
    # SelfAssessmentMixinì—ì„œ ì„¤ì • ê¸°ë°˜ìœ¼ë¡œ ìë™ ì œê³µ
    
    # ğŸ”§ ì¤‘ë³µ ì œê±°: assess_step_suitability() ë©”ì„œë“œ ì œê±°
    # SelfAssessmentMixinì—ì„œ ì„¤ì • ê¸°ë°˜ìœ¼ë¡œ ìë™ ì œê³µ
    
    def get_description(self) -> str:
        return self.description


class GlossaryTool(BaseTool):
    """
    ìš©ì–´ ì‚¬ì „ ì¡°íšŒ ë„êµ¬
    í•œê¸€ ì¿¼ë¦¬ë¥¼ ì˜ì–´ë¡œ ë³€í™˜í•˜ê³  ë™ì˜ì–´ í™•ì¥
    """
    
    def __init__(self):
        """Initialize Glossary Tool"""
        self.name = "glossary_lookup"
        self.description = "í•œ-ì˜ ìš©ì–´ ë³€í™˜ ë° ë™ì˜ì–´ í™•ì¥ (ê²€ìƒ‰ ì „ í•„ìˆ˜)"
        
        # ğŸ”§ ìˆ˜ì •: Mixin ì´ˆê¸°í™” (ìê¸° í‰ê°€ ë° ì—ëŸ¬ ì²˜ë¦¬ ê¸°ëŠ¥ í¬í•¨)
        super().__init__()
        
        self.glossary = GlossaryLookupTool()
        log.info("Initialized GlossaryTool with assessment capabilities")
    
    def execute(self,
                query: str,
                include_synonyms: bool = True,
                **kwargs) -> Dict[str, Any]:
        """
        ìš©ì–´ ë³€í™˜ ì‹¤í–‰
        
        Args:
            query: ë³€í™˜í•  ì¿¼ë¦¬
            include_synonyms: ë™ì˜ì–´ í¬í•¨ ì—¬ë¶€
            
        Returns:
            ë³€í™˜ ê²°ê³¼
        """
        log.info(f"Executing glossary lookup: '{query}'")
        
        result = self.glossary.execute(
            query=query,
            include_synonyms=include_synonyms
        )
        
        log.info(f"Glossary lookup complete: {result.get('translation_applied', False)}")
        return result
    
    def get_description(self) -> str:
        return self.description


class SynthesizerTool(BaseTool):
    """
    ì¢…í•© ë„êµ¬ - ê°œì„ ëœ ë™ì  ì½”ë“œ ì²˜ë¦¬
    í•˜ë“œì½”ë”©ëœ INFO ì²˜ë¦¬ ë¡œì§ì„ íŒ¨í„´ ê¸°ë°˜ìœ¼ë¡œ ê°œì„ 
    """

    def __init__(self, llm_client=None):
        """
        Initialize Synthesizer Tool

        Args:
            llm_client: LLM í´ë¼ì´ì–¸íŠ¸
        """
        self.name = "synthesizer"
        self.description = "ì—¬ëŸ¬ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ì‘ë‹µ ìƒì„±"
        
        # ğŸ”§ ìˆ˜ì •: Mixin ì´ˆê¸°í™” (ìê¸° í‰ê°€ ë° ì—ëŸ¬ ì²˜ë¦¬ ê¸°ëŠ¥ í¬í•¨)
        super().__init__()
        
        self.llm_client = llm_client
        
        # ğŸ”§ Phase 1: LLM ììœ¨ íŒë‹¨ ë°©ì‹ìœ¼ë¡œ ë³€ê²½ (í•˜ë“œì½”ë”© ì œê±°)
        
        # ğŸ”§ ê°œì„ : ì½”ë“œ ë©”íƒ€ë°ì´í„° ì„¤ì •
        self.code_metadata = self._load_code_metadata()
        
        log.info("Initialized SynthesizerTool with response mode control and dynamic code processing")

    def execute(self,
                results: List[Any],
                original_query: str,
                **kwargs) -> Dict[str, Any]:
        """
        ê²°ê³¼ ì¢…í•© ì‹¤í–‰ (ì‘ë‹µ ëª¨ë“œ ì§€ì›)

        Args:
            results: ì¢…í•©í•  ê²°ê³¼ë“¤
            original_query: ì›ë³¸ ì¿¼ë¦¬
            response_mode: ì‘ë‹µ ëª¨ë“œ ("factual", "analytical", ë˜ëŠ” None)

        Returns:
            ì¢…í•©ëœ ìµœì¢… ì‘ë‹µ
        """
        log.info("Executing synthesis with LLM autonomous judgment")

        if self.llm_client:
            # LLMì„ ì‚¬ìš©í•œ ì¢…í•© - LLMì´ ì‘ë‹µ ìŠ¤íƒ€ì¼ ììœ¨ ê²°ì •
            synthesis_prompt = self._create_synthesis_prompt(results, original_query)
            llm_response = self.llm_client.complete(synthesis_prompt)

            return {
                "query": original_query,
                "synthesis": llm_response,
                "source_count": len(results)
            }
        else:
            # ê·œì¹™ ê¸°ë°˜ ì¢…í•©
            return self._rule_based_synthesis(results, original_query)
    
    def execute_streaming(self,
                         results: List[Any],
                         original_query: str,
                         **kwargs):
        """
        ê²°ê³¼ ì¢…í•© ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ - LangGraph ì „ìš©)

        Args:
            results: ì¢…í•©í•  ê²°ê³¼ë“¤
            original_query: ì›ë³¸ ì¿¼ë¦¬
            response_mode: ì‘ë‹µ ëª¨ë“œ ("factual", "analytical", ë˜ëŠ” None)

        Yields:
            ìŠ¤íŠ¸ë¦¬ë° í…ìŠ¤íŠ¸ ì²­í¬ë“¤
        """
        log.info("Executing synthesis (streaming mode) with LLM autonomous judgment")

        if self.llm_client and hasattr(self.llm_client, 'stream_complete'):
            # LLM ìŠ¤íŠ¸ë¦¬ë° ì¢…í•© - LLMì´ ì‘ë‹µ ìŠ¤íƒ€ì¼ ììœ¨ ê²°ì •
            synthesis_prompt = self._create_synthesis_prompt(results, original_query)
            
            try:
                # H-Chat ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‚¬ìš©
                for chunk in self.llm_client.stream_complete(synthesis_prompt):
                    if chunk:  # ë¹ˆ ì²­í¬ ì œì™¸
                        yield chunk
                        
            except Exception as e:
                log.error(f"LLM streaming failed: {e}")
                # í´ë°±: ì¼ë°˜ ëª¨ë“œë¡œ ì „í™˜
                llm_response = self.llm_client.complete(synthesis_prompt)
                # ì‹œë®¬ë ˆì´ì…˜ ìŠ¤íŠ¸ë¦¬ë°
                sentences = llm_response.split('. ')
                for i, sentence in enumerate(sentences):
                    if sentence.strip():
                        if i < len(sentences) - 1:
                            yield sentence + '. '
                        else:
                            yield sentence
        else:
            # ê·œì¹™ ê¸°ë°˜ ì¢…í•© (ìŠ¤íŠ¸ë¦¬ë° ì‹œë®¬ë ˆì´ì…˜)
            rule_result = self._rule_based_synthesis(results, original_query)
            synthesis_text = rule_result.get('synthesis', 'Analysis completed.')
            
            # ë¬¸ì¥ë³„ ìŠ¤íŠ¸ë¦¬ë° ì‹œë®¬ë ˆì´ì…˜
            sentences = synthesis_text.split('\n')
            for sentence in sentences:
                if sentence.strip():
                    yield sentence + '\n'

    def _create_synthesis_prompt(self, results: List[Any], query: str) -> str:
        """ğŸ”§ ê°œì„ ëœ LLM ì¢…í•©ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„± - Hallucination ë°©ì§€ ê°•í™”"""

        # Hallucination ë°©ì§€ 1ë‹¨ê³„: ì‹¤ì œ ë°ì´í„° ê²€ì¦
        total_docs_across_results = 0
        valid_results_count = 0
        error_results_count = 0
        
        for result in results:
            if isinstance(result, dict):
                if 'error' in result:
                    error_results_count += 1
                elif 'documents' in result:
                    doc_count = len(result.get('documents', []))
                    total_docs_across_results += doc_count
                    if doc_count > 0:
                        valid_results_count += 1
                elif 'buckets' in result:
                    bucket_count = len(result.get('buckets', []))
                    if bucket_count > 0:
                        valid_results_count += 1
                        # ë²„í‚·ì˜ ì´ ë¬¸ì„œ ìˆ˜ ê³„ì‚°
                        bucket_total = sum(bucket.get('doc_count', 0) for bucket in result['buckets'])
                        total_docs_across_results += bucket_total
        
        # Hallucination ìœ„í—˜ ê°ì§€
        if total_docs_across_results == 0 and valid_results_count == 0:
            log.error(f"HALLUCINATION RISK: No actual data found for query '{query}'")
            return f"""**ë°ì´í„° ë¶€ì¡± ì•Œë¦¼**

ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­í•˜ì‹  ì¡°ê±´ì— ë§ëŠ” ì‹¤ì œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:

**ê²€ìƒ‰ ì¡°ê±´**: {query}
**ê²€ì¦ ê²°ê³¼**: 
- ì´ ì²˜ë¦¬ëœ ê²°ê³¼: {len(results)}ê°œ
- ì˜¤ë¥˜ ë°œìƒ: {error_results_count}ê°œ
- ì‹¤ì œ ë°ì´í„°: {total_docs_across_results}ê°œ ë¬¸ì„œ

**ê¶Œì¥ì‚¬í•­**:
1. ê²€ìƒ‰ ì¡°ê±´ì„ ë” ë„“ê²Œ ì„¤ì •í•´ë³´ì„¸ìš”
2. ì—°ë„ë‚˜ ì°¨ì¢… í•„í„°ë¥¼ ì œê±°í•´ë³´ì„¸ìš”  
3. ë‹¤ë¥¸ ë¬¸ì œ ì½”ë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”

ì´ ì‹œìŠ¤í…œì€ **ì‹¤ì œ ë°ì´í„°ë§Œ ì œê³µ**í•˜ë©°, ì¶”ì •ì´ë‚˜ ê°€ì •ì— ê¸°ë°˜í•œ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."""
        
        # ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ - ê²€ì¦ëœ ë°ì´í„°ë§Œ ì²˜ë¦¬
        results_text = []
        discovered_codes = set()
        
        for i, result in enumerate(results, 1):
            if isinstance(result, dict):
                if 'documents' in result:
                    documents = result['documents']
                    results_text.append(f"ê²€ìƒ‰ ê²°ê³¼: {len(documents)}ê°œ ë¬¸ì„œ")
                    
                    # ğŸ”§ ê°œì„ : ë™ì  ì½”ë“œ ê°ì§€ (í•˜ë“œì½”ë”© ì œê±°)
                    code_analysis = self._analyze_codes_in_documents(documents)
                    discovered_codes.update(code_analysis.keys())
                    
                    for code, info in code_analysis.items():
                        results_text.append(f"\nâ­ {code} ë¬¸ì„œ ë°œê²¬: {info['count']}ê°œ")
                        if info['examples']:
                            results_text.append(f" - {info['examples'][0]}")
                    
                    # ê¸°íƒ€ ë¬¸ì„œ í‘œì‹œ (ì½”ë“œê°€ ì•„ë‹Œ ê²ƒë“¤)
                    other_docs = [doc for doc in documents[:3] 
                                if not any(code in doc.get('problem', '') for code in discovered_codes)]
                    for doc in other_docs:
                        results_text.append(f" - {doc.get('problem', 'N/A')}")
                        
                elif 'buckets' in result:
                    # ğŸ”§ ê°œì„ : ë™ì  ì§‘ê³„ ê²°ê³¼ ì²˜ë¦¬
                    buckets = result.get('buckets', [])
                    debug_info = result.get('debug_info', {})
                    
                    results_text.append(f"ì§‘ê³„ ê²°ê³¼ ìƒì„¸ ë¶„ì„:")
                    results_text.append(f"- ì´ ë¬¸ì„œ ìˆ˜: {debug_info.get('input_doc_count', len(buckets))}ê°œ")
                    
                    # ë™ì  ì½”ë“œ ë¶„ì„
                    bucket_codes = self._analyze_codes_in_buckets(buckets)
                    discovered_codes.update(bucket_codes.keys())
                    
                    for code, info in bucket_codes.items():
                        metadata = self.code_metadata.get(code, {})
                        severity = metadata.get('severity', 'ë³´í†µ')
                        description = metadata.get('description', f'{code} ê´€ë ¨ ë¬¸ì œ')
                        
                        results_text.append(f"\n*** {code} ì½”ë“œ ë°œê²¬ ***")
                        results_text.append(f"- ë¬¸ì œ: {info['key']}")
                        results_text.append(f"- ë°œìƒ ê±´ìˆ˜: {info['count']}ê±´")
                        results_text.append(f"- ë¬¸ì œ ìœ í˜•: {description}")
                        results_text.append(f"- ì‹¬ê°ë„: {severity}")
                    
                    # ê¸°íƒ€ ë²„í‚· í‘œì‹œ
                    for bucket in buckets[:5]:
                        if not any(code in bucket['key'] for code in discovered_codes):
                            results_text.append(f"- {bucket['key']}: {bucket['doc_count']}ê±´")

        #  Hallucination ë°©ì§€ 2ë‹¨ê³„: ë°ì´í„° íˆ¬ëª…ì„± ê°•í™”
        data_transparency_info = f"""

**ë°ì´í„° ê²€ì¦ ì •ë³´ (íˆ¬ëª…ì„± ë³´ì¥)**:
- ì´ ë¶„ì„ ë¬¸ì„œ: {total_docs_across_results:,}ê°œ
- ìœ íš¨í•œ ê²°ê³¼: {valid_results_count}ê°œ  
- ì˜¤ë¥˜ ê²°ê³¼: {error_results_count}ê°œ
- ë°œê²¬ëœ ì½”ë“œ: {', '.join(discovered_codes) if discovered_codes else 'ì—†ìŒ'}
- ë¶„ì„ ì‹œì : {__import__('time').strftime('%Y-%m-%d %H:%M:%S')}"""
        
        # Phase 1: LLM ììœ¨ íŒë‹¨ ë°©ì‹ìœ¼ë¡œ ë³€ê²½ + Hallucination ë°©ì§€ ê°•í™”
        llm_autonomous_instructions = self._generate_hallucination_safe_instructions(list(discovered_codes), total_docs_across_results)
        
        prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë¶„ì„ ê²°ê³¼:
{chr(10).join(results_text)}
{data_transparency_info}

ì‘ë‹µ ì§€ì¹¨ (ì‹¤ë°ì´í„° ê¸°ë°˜ + Hallucination ë°©ì§€):
{llm_autonomous_instructions}

**ì¤‘ìš”: Hallucination ë°©ì§€ ê·œì¹™**
- ì‹¤ì œ ë°ì´í„°ì— ì—†ëŠ” ìˆ«ìë‚˜ í†µê³„ë¥¼ ì ˆëŒ€ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”
- "ì•½", "ëŒ€ëµ", "ì¶”ì •" ë“±ì˜ í‘œí˜„ìœ¼ë¡œ ë¶ˆí™•ì‹¤ì„±ì„ ëª…ì‹œí•˜ì„¸ìš”
- ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ ì†”ì§íˆ "ë°ì´í„° ë¶€ì¡±"ì´ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”
- ìœ„ì— ì œê³µëœ ì‹¤ì œ ë¬¸ì„œ ìˆ˜ì™€ ì½”ë“œ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”"

ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì¤€ìˆ˜ì‚¬í•­ (í•„ìˆ˜):
â€¢ í—¤ë”(###) ì•ì—ëŠ” ë°˜ë“œì‹œ ë‘ ì¤„ ê°œí–‰ì„ ë„£ìœ¼ì„¸ìš”
  ì˜ˆì‹œ: "ë¶„ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.\n\n### ì£¼ìš” ë°œê²¬ì‚¬í•­"
â€¢ ë¦¬ìŠ¤íŠ¸(1., 2., 3.) ì•ì—ëŠ” ë°˜ë“œì‹œ ë‘ ì¤„ ê°œí–‰ì„ ë„£ìœ¼ì„¸ìš”
  ì˜ˆì‹œ: "ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\n1. ì²« ë²ˆì§¸ í•­ëª©"
â€¢ ì†Œì œëª©(####) ì•ì—ë„ ë‘ ì¤„ ê°œí–‰ì„ ë„£ìœ¼ì„¸ìš”
  ì˜ˆì‹œ: "ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n\n#### ìƒì„¸ ë‚´ìš©"
â€¢ ê° ì„¹ì…˜ ì‚¬ì´ì—ëŠ” ì ì ˆí•œ ê°„ê²©ì„ ìœ ì§€í•˜ì„¸ìš”
â€¢ í‘œë‚˜ ì½”ë“œ ë¸”ë¡ ì•ì—ë„ ë‘ ì¤„ ê°œí–‰ì„ ë„£ìœ¼ì„¸ìš”

ì˜¬ë°”ë¥¸ í˜•ì‹ ì˜ˆì‹œ:
```
ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.

### ì£¼ìš” ë°œê²¬ì‚¬í•­

ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì œë“¤ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.

1. INFO12 ì½”ë“œ ê´€ë ¨ ë¬¸ì œ
2. INFO13 ì½”ë“œ ê´€ë ¨ ë¬¸ì œ

#### ìƒì„¸ ë¶„ì„

ê° í•­ëª©ì˜ ì„¸ë¶€ ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
```

ìœ„ ê²°ê³¼ì™€ í˜•ì‹ ì§€ì¹¨ì„ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
"""

        return prompt
    
    def _load_code_metadata(self) -> Dict[str, Any]:
        """ğŸ”§ ê°œì„ : ì½”ë“œ ë©”íƒ€ë°ì´í„° ë¡œë“œ (ì„¤ì • íŒŒì¼ ë˜ëŠ” ê¸°ë³¸ê°’)"""
        try:
            # ì„¤ì • íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ
            import json
            from pathlib import Path
            
            metadata_file = Path("data/code_metadata.json")
            if metadata_file.exists():
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            log.warning(f"Failed to load code metadata: {e}")
        
        # ğŸ”§ ê°ê´€ì  ë©”íƒ€ë°ì´í„°ë§Œ ì œê³µ (ì‹¬ê°ë„ ì •ë³´ ì™„ì „ ì œê±°)
        return {
            "INFO12": {
                "description": "Built-in navigation - Broken/works inconsistently",
                "category": "infotainment",
                "keywords": ["navigation", "broken", "inconsistent"]
            },
            "INFO13": {
                "description": "Built-in navigation - DTU (difficult to use)",
                "category": "infotainment", 
                "keywords": ["navigation", "difficult", "usability"]
            },
            "INFO22": {
                "description": "Wireless charging pad - Broken/works inconsistently", 
                "category": "infotainment",
                "keywords": ["wireless", "charging", "broken"]
            },
            "INFO23": {
                "description": "Wireless charging pad - Size/location inappropriate",
                "category": "infotainment",
                "keywords": ["wireless", "charging", "size", "location"]
            },
            "EXT16": {
                "description": "Exterior component issue",
                "category": "exterior",
                "keywords": ["exterior", "component"]
            }
        }
    
    def _analyze_codes_in_documents(self, documents: List[Dict]) -> Dict[str, Dict]:
        """ğŸ”§ ê°œì„ : ë¬¸ì„œì—ì„œ ì½”ë“œ ë™ì  ë¶„ì„"""
        import re
        code_analysis = {}
        
        # ëª¨ë“  ì½”ë“œ íŒ¨í„´ ë™ì  ê°ì§€
        code_pattern = r'\b(INFO|EXT|FCD|DRA|CLMT|INT|PWR)\d{1,2}\b'
        
        for doc in documents:
            problem = doc.get('problem', '')
            codes = re.findall(code_pattern, problem, re.IGNORECASE)
            
            for match in re.finditer(code_pattern, problem, re.IGNORECASE):
                code = match.group().upper()
                if code not in code_analysis:
                    code_analysis[code] = {
                        'count': 0,
                        'examples': [],
                        'documents': []
                    }
                
                code_analysis[code]['count'] += 1
                code_analysis[code]['documents'].append(doc)
                
                if len(code_analysis[code]['examples']) < 3:
                    code_analysis[code]['examples'].append(problem)
        
        return code_analysis
    
    def _analyze_codes_in_buckets(self, buckets: List[Dict]) -> Dict[str, Dict]:
        """ğŸ”§ ê°œì„ : ì§‘ê³„ ë²„í‚·ì—ì„œ ì½”ë“œ ë™ì  ë¶„ì„"""
        import re
        code_analysis = {}
        
        code_pattern = r'\b(INFO|EXT|FCD|DRA|CLMT|INT|PWR)\d{1,2}\b'
        
        for bucket in buckets:
            key = bucket['key']
            count = bucket['doc_count']
            
            codes = re.findall(code_pattern, key, re.IGNORECASE)
            for code_match in codes:
                code = code_match.upper()
                if code not in code_analysis:
                    code_analysis[code] = {
                        'key': key,
                        'count': count
                    }
                else:
                    # ì¤‘ë³µì´ë©´ ì¹´ìš´íŠ¸ í•©ì‚°
                    code_analysis[code]['count'] += count
        
        return code_analysis
    
    def _generate_hallucination_safe_instructions(self, discovered_codes: list, total_docs: int) -> str:
        """
        Hallucination ë°©ì§€ ê°•í™”ëœ LLM ì§€ì¹¨ ìƒì„±
        
        Args:
            discovered_codes: ë°œê²¬ëœ ì½”ë“œ ë¦¬ìŠ¤íŠ¸
            total_docs: ì‹¤ì œ ë¬¸ì„œ ìˆ˜
            
        Returns:
            Hallucination ë°©ì§€ê°€ ê°•í™”ëœ ì§€ì¹¨ ë¬¸ìì—´
        """
        # Hallucination ë°©ì§€ ê°•í™” ì§€ì¹¨
        base_instructions = [
            "1. **ì‹¤ì œ ë°ì´í„°ë§Œ ì‚¬ìš©**: ìœ„ì— ì œê³µëœ ë°ì´í„°ì— ì—†ëŠ” ìˆ«ìë‚˜ ì •ë³´ë¥¼ ì ˆëŒ€ ì²˜ë¦¬í•˜ì§€ ë§ˆì„¸ìš”",
            f"2. **íˆ¬ëª…ì„±**: ì´ {total_docs:,}ê°œ ë¬¸ì„œì— ê¸°ë°˜í•œ ë¶„ì„ì´ë©°, ì´ë¥¼ ëª…ì‹œí•˜ì„¸ìš”",
            "3. **ê°•ì œ ê²€ì¦**: ê° ìˆ«ìëŠ” ìœ„ ë°ì´í„°ì—ì„œ ì§ì ‘ í™•ì¸ ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤",
            "4. **ë¶ˆí™•ì‹¤ì„± í‘œì‹œ**: ë°ì´í„°ê°€ ë¶ˆì¶©ë¶„í•˜ë©´ 'ë°ì´í„° ë¶€ì¡±', 'ë‹¨ì •ì  ê²°ë¡  ì–´ë ¤ì›€' ë“±ìœ¼ë¡œ ì†”ì§í•˜ê²Œ ëª…ì‹œ"
        ]
        
        # ë°ì´í„° ë¶€ì¡± ê²½ê³ 
        if total_docs < 50:
            base_instructions.append(
                f"5. **ì¤‘ìš” ê²½ê³ **: ë°ì´í„° ìˆ˜ê°€ ì ìŒ ({total_docs}ê°œ). ì œí•œì  ë¶„ì„ì´ë¯€ë¡œ 'ë°ì´í„° ìˆ˜ ì œí•œìœ¼ë¡œ ì¸í•œ ì˜ˆë¹„ ê²°ê³¼'ë¼ê³  ëª…ì‹œ"
            )
        
        if discovered_codes:
            # ê²€ì¦ëœ ì½”ë“œ ì •ë³´ë§Œ ì œê³µ
            base_instructions.append(f"\n6. ë°œê²¬ëœ {', '.join(discovered_codes)} ì½”ë“œë“¤ì˜ ê¸°ë³¸ ì •ë³´:")
            for code in discovered_codes:
                metadata = self.code_metadata.get(code, {})
                description = metadata.get('description', f'{code} ê´€ë ¨ ë¬¸ì œ')
                base_instructions.append(f"   - {code}: {description} (ì‹¤ì œ ë°ì´í„°ì—ì„œ í™•ì¸ë¨)")
        else:
            base_instructions.append("\n6. íŠ¹ì • ë¬¸ì œ ì½”ë“œê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŒ (ë°ì´í„° ê¸°ë°˜ í™•ì¸)")
        
        # ìµœì¢… Hallucination ë°©ì§€ ê²½ê³ 
        final_warning = [
            "\n\n**Hallucination ë°©ì§€ ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸**:",
            "- [ ] ëª¨ë“  ìˆ«ìê°€ ìœ„ ë°ì´í„°ì—ì„œ ë‚˜ì˜´?",
            "- [ ] ê°€ì •ì´ë‚˜ ì¶”ì • ëŒ€ì‹  ì‹¤ì œ ë°ì´í„°ë§Œ ì‚¬ìš©?",
            "- [ ] ë°ì´í„° ë¶€ì¡± ì‹œ ì†œì§í•˜ê²Œ ëª…ì‹œ?",
            "- [ ] 350ê±´, 250ê±´ ê°™ì´ ë„ˆë¬´ ê¹”ë”í•œ ìˆ«ì í”¼í•´?"
        ]
        
        return '\n'.join(base_instructions + final_warning)

    def _rule_based_synthesis(self, results: List[Any], query: str) -> Dict[str, Any]:
        """ê·œì¹™ ê¸°ë°˜ ì¢…í•© - Hallucination ë°©ì§€ ê°•í™” ë²„ì „"""

        synthesis_parts = []
        total_docs_found = 0
        brand_filter_applied = False
        aggregation_results = []
        successful_steps = 0
        failed_steps = 0

        for i, result in enumerate(results, 1):
            if isinstance(result, dict):
                # ì˜¤ë¥˜ ì²˜ë¦¬ë„ ì •ë³´ë¡œ í™œìš©
                if 'error' in result:
                    failed_steps += 1
                    if 'No documents found after filtering' in result.get('error', ''):
                        brand_filter_applied = True
                        synthesis_parts.append(f"\nâš ï¸ ë‹¨ê³„ {i}: ë¸Œëœë“œ í•„í„°ë§ ì‹œ ë¬¸ì„œê°€ ì—†ìŒ")
                    else:
                        synthesis_parts.append(f"\nâš ï¸ ë‹¨ê³„ {i} ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
                    continue
                
                successful_steps += 1
                
                if 'aggregation' in result:
                    # ì§‘ê³„ ê²°ê³¼ ì²˜ë¦¬
                    if result['aggregation'] == 'terms':
                        field = result.get('field', 'unknown')
                        field_korean = {
                            'problem': 'ë¬¸ì œ ìœ í˜•',
                            'make_of_vehicle': 'ë¸Œëœë“œ',
                            'category': 'ì¹´í…Œê³ ë¦¬'
                        }.get(field, field)
                        
                        synthesis_parts.append(f"\n### {field_korean}ë³„ ë¶„ì„ ê²°ê³¼ (ì‹¤ì œ ë°ì´í„°):")
                        
                        buckets = result.get('buckets', [])
                        
                debug_info = result.get('debug_info', {})
                aggregation_results.extend(buckets)
                
                # ğŸ”§ INFO22/INFO23 ëª…ì‹œì  ì²˜ë¦¬ ë° ìƒì„¸ ë¶„ì„
                info22_found = False
                info23_found = False
                
                # ğŸ”§ ê°œì„ : ë™ì  ì½”ë“œ ì²˜ë¦¬ (í•˜ë“œì½”ë”© ì œê±°)
                bucket_analysis = self._analyze_codes_in_buckets(buckets)
                codes_found = set(bucket_analysis.keys())
                
                                    # ğŸ”§ Phase 1: ê¸°ë³¸ ì •ë³´ ì¤‘ì‹¬ ë¶„ì„ (LLMì´ í•„ìš”ì‹œ ì‹¬ê°ë„ íŒë‹¨)
                for j, (code, info) in enumerate(bucket_analysis.items(), 1):
                    metadata = self.code_metadata.get(code, {})
                    description = metadata.get('description', f'{code} ê´€ë ¨ ë¬¸ì œ')
                    
                    synthesis_parts.append(f"\n#### {j}. {code} ì½”ë“œ ìƒì„¸ ë¶„ì„")
                    synthesis_parts.append(f"- **ë¬¸ì œ ì „ì²´**: {info['key']}")
                    synthesis_parts.append(f"- **ë°œìƒ ê±´ìˆ˜**: {info['count']:,}ê±´")
                    synthesis_parts.append(f"- **ë¬¸ì œ ìœ í˜•**: {description}")
                    
                    # ğŸ”§ ì‹¬ê°ë„ ì •ë³´ëŠ” ìœ ì§€í•˜ë˜ ê¸°ë³¸ì ìœ¼ë¡œ í‘œì‹œí•˜ì§€ ì•ŠìŒ
                    # LLM í´ë¼ì´ì–¸íŠ¸ê°€ ìˆëŠ” ê²½ìš°ëŠ” LLMì´ íŒë‹¨í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì œì™¸
                
                # ì½”ë“œê°€ ì•„ë‹Œ ì¼ë°˜ ë¬¸ì œë“¤ë„ í‘œì‹œ
                other_buckets = [b for b in buckets[:10] 
                               if not any(code in b['key'] for code in codes_found)]
                
                for j, bucket in enumerate(other_buckets, len(bucket_analysis) + 1):
                    synthesis_parts.append(f"  {j}. **{bucket['key']}**: {bucket['doc_count']:,}ê±´")
                
                # ğŸ”§ ê°œì„ : ë™ì  ë¹„êµ ë¶„ì„ ìƒì„±
                if len(bucket_analysis) > 1:
                    synthesis_parts.append(f"\n#### ë‹¤ì¤‘ ì½”ë“œ ë¹„êµ ë¶„ì„")
                    
                    # ê°™ì€ ì¹´í…Œê³ ë¦¬ ì½”ë“œë“¤ ê·¸ë£¹í™”
                    categories = {}
                    for code, info in bucket_analysis.items():
                        metadata = self.code_metadata.get(code, {})
                        category = metadata.get('category', 'unknown')
                        if category not in categories:
                            categories[category] = []
                        categories[category].append((code, info))
                    
                    # ì¹´í…Œê³ ë¦¬ë³„ ë¹„êµ
                    for category, code_list in categories.items():
                        if len(code_list) > 1:
                            synthesis_parts.append(f"\n**{category} ì¹´í…Œê³ ë¦¬ ë¹„êµ:**")
                            total_count = 0
                            for code, info in code_list:
                                metadata = self.code_metadata.get(code, {})
                                description = metadata.get('description', f'{code} ê´€ë ¨')
                                synthesis_parts.append(f"- {code}: {info['count']:,}ê±´ ({description})")
                                total_count += info['count']
                            synthesis_parts.append(f"- **{category} ì´ê³„**: {total_count:,}ê±´")
                        
                        # ì›ë³¸ ë¬¸ì„œ ìˆ˜ ì¶”ê°€
                    if 'total_source_docs' in result:
                            total_docs_found = max(total_docs_found, result['total_source_docs'])
                    
                    elif result['aggregation'] == 'count':
                        count = result.get('count', 0)
                        synthesis_parts.append(f"\n### ë°ì´í„° ì¹´ìš´íŠ¸: {count:,}ê±´")
                        total_docs_found = max(total_docs_found, count)
                    
                    elif result['aggregation'] == 'info_code_analysis':
                        # ì‹¤ì œ INFO ì½”ë“œ ë¶„ì„ ê²°ê³¼
                        info_counts = result.get('hyundai_info_counts', {})
                        info_details = result.get('hyundai_info_details', {})
                        total_hyundai = result.get('total_hyundai_docs', 0)
                        
                        synthesis_parts.append(f"\n### í˜„ëŒ€ë¸Œëœë“œ INFO12/13/14 ì‹¤ì œ ë¶„ì„ ê²°ê³¼:")
                        synthesis_parts.append(f"- ì´ í˜„ëŒ€ë¸Œëœë“œ ë¬¸ì„œ: {total_hyundai:,}ê±´")
                        
                        if info_counts:
                            for code in ['INFO12', 'INFO13', 'INFO14']:
                                count = info_counts.get(code, 0)
                                synthesis_parts.append(f"- **{code}**: {count}ê±´")
                                
                                # ì˜ˆì‹œ ì¶”ê°€
                                if code in info_details and info_details[code]:
                                    synthesis_parts.append(f"  ì˜ˆì‹œ: {info_details[code][0]['problem']}")
                        else:
                            synthesis_parts.append("- INFO12/13/14 ì½”ë“œê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                        
                        total_docs_found = max(total_docs_found, total_hyundai)

                elif 'documents' in result:
                    # ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
                    total = result.get('total_hits', 0)
                    total_docs_found = max(total_docs_found, total)
                    synthesis_parts.append(f"\n### ê²€ìƒ‰ ê²°ê³¼: ì´ {total:,}ê°œ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬")

                    if total > 0:
                        synthesis_parts.append("\n**ì£¼ìš” ì‚¬ë¡€:**")
                        for k, doc in enumerate(result['documents'][:3], 1):
                            model = doc.get('model_of_vehicle', 'N/A')
                            problem = doc.get('problem', 'N/A')
                            text = doc.get('verbatim_text', 'N/A')
                            synthesis_parts.append(f"  {k}. **{model}**: {problem}")
                            synthesis_parts.append(f"     â†’ {text[:100]}...")
                
                # í•„í„° ì •ë³´ ì¶”ê°€
                if 'filter_applied' in result:
                    brand_filter_applied = True

        # ìµœì¢… ìš”ì•½
        if not synthesis_parts:
            synthesis_parts.append("ë°ì´í„° ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì‹¤ì œ ë°ì´í„°ê°€ ì—†ì–´ ê²°ê³¼ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            # ë°ì´í„° ê°œìš” ì¶”ê°€ (ì‹¤ì œ ë°ì´í„°ë§Œ)
            overview = f"\n### ë°ì´í„° ë¶„ì„ ê°œìš” (ì‹¤ì œ ê²°ê³¼ë§Œ í‘œì‹œ)"
            overview += f"\n- **ì´ ë°ì´í„°**: {total_docs_found:,}ê°œ ë¬¸ì„œ ë¶„ì„"
            overview += f"\n- **ì„±ê³µ ë‹¨ê³„**: {successful_steps}ê°œ / **ì‹¤íŒ¨ ë‹¨ê³„**: {failed_steps}ê°œ"
            if failed_steps > 0:
                overview += f"\n- âš ï¸ **ì£¼ì˜**: {failed_steps}ê°œ ë‹¨ê³„ê°€ ì‹¤íŒ¨í•˜ì—¬ ë¶€ë¶„ì  ê²°ê³¼ì…ë‹ˆë‹¤."
            if brand_filter_applied:
                overview += f"\n- **ë¸Œëœë“œ í•„í„°**: í˜„ëŒ€ ë¸Œëœë“œ ì „ìš© ë¶„ì„"
            
            synthesis_parts.insert(0, overview)
            
            # INFO12/13/14 ê°œë³„ ì§‘ê³„ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ëª…ì‹œ
            if failed_steps > 0 and 'INFO12' in query and 'INFO13' in query and 'INFO14' in query:
                synthesis_parts.append("\nâš ï¸ **ì¤‘ìš”**: INFO12/13/14 ê°œë³„ ì§‘ê³„ê°€ ì‹¤íŒ¨í•˜ì—¬ ìœ„ì˜ ìˆ«ìë“¤ì€ ì¶”ì •ì¹˜ê°€ ì•„ë‹Œ ì‹¤ì œ ë°ì´í„°ì— ê¸°ë°˜í•œ ê²°ê³¼ì…ë‹ˆë‹¤.")

        return {
            "query": query,
            "synthesis": "\n".join(synthesis_parts),
            "source_count": len(results),
            "successful_steps": successful_steps,
            "failed_steps": failed_steps,
            "total_documents_analyzed": total_docs_found,
            "brand_filtering_applied": brand_filter_applied,
            "method": "rule_based_truthful"
        }

    def get_description(self) -> str:
        return self.description


def test_tools():
    """ë„êµ¬ë“¤ í…ŒìŠ¤íŠ¸"""

    print("=" * 70)
    print("Testing Agent Tools")
    print("=" * 70)

    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    sample_docs = [
        {
            "verbatim_id": "001",
            "model": "Santa Fe",
            "model_year": 2025,
            "problem": "Tire - Vibration",
            "verbatim_text": "Tire vibration at high speed",
            "registration_date": "2025-01-15"
        },
        {
            "verbatim_id": "002",
            "model": "Santa Fe",
            "model_year": 2025,
            "problem": "Tire - Vibration",
            "verbatim_text": "Steering wheel vibrates",
            "registration_date": "2025-01-20"
        },
        {
            "verbatim_id": "003",
            "model": "Tucson",
            "model_year": 2024,
            "problem": "Engine - Noise",
            "verbatim_text": "Engine makes clicking noise",
            "registration_date": "2024-12-01"
        }
    ]

    # 1. Aggregator Tool í…ŒìŠ¤íŠ¸
    print("\n1. Testing AggregatorTool")
    print("-" * 30)

    aggregator = AggregatorTool()
    agg_result = aggregator.execute(
        documents=sample_docs,
        aggregation="terms",
        field="problem",
        size=5
    )

    print(f"Aggregation result:")
    for bucket in agg_result.get('buckets', []):
        print(f" {bucket['key']}: {bucket['doc_count']} docs")

    # 2. HybridSearch Tool í…ŒìŠ¤íŠ¸
    print("\n2. Testing HybridSearchTool")
    print("-" * 30)

    search_tool = HybridSearchTool()

    # ë¨¼ì € ì¸ë±ìŠ¤ êµ¬ì¶•
    print("Building index...")
    search_tool.build_index(sample_docs)

    # ê²€ìƒ‰ ì‹¤í–‰
    search_result = search_tool.execute(
        query="tire vibration",
        limit=5,
        filters={"model_year": 2025}
    )

    print(f"Search returned {search_result['total_hits']} results")
    for doc in search_result.get('documents', [])[:2]:
        print(f" - {doc.get('verbatim_text', 'N/A')}")

    # 3. Reranker Tool í…ŒìŠ¤íŠ¸
    print("\n3. Testing RerankerTool")
    print("-" * 30)

    reranker = RerankerTool()
    rerank_result = reranker.execute(
        documents=search_result['documents'],
        query="tire vibration",
        top_k=2
    )

    print(f"Reranked {rerank_result['total_reranked']} documents")
    for doc in rerank_result.get('documents', []):
        print(f" - Score: {doc['_score']:.3f}")
        print(f"   {doc.get('verbatim_text', 'N/A')}")

    # 4. Synthesizer Tool í…ŒìŠ¤íŠ¸
    print("\n4. Testing SynthesizerTool")
    print("-" * 30)

    synthesizer = SynthesizerTool()
    synthesis = synthesizer.execute(
        results=[agg_result, search_result],
        original_query="2025 Santa Fe tire problems"
    )

    print("Synthesis result:")
    print(synthesis['synthesis'])

    print("\n" + "=" * 70)
    print("Tools test complete!")


if __name__ == "__main__":
    test_tools()

    synthesizer = SynthesizerTool()
    synthesis = synthesizer.execute(
        results=[agg_result, search_result],
        original_query="2025 Santa Fe tire problems"
    )

    print("Synthesis result:")
    print(synthesis['synthesis'])

    print("\n" + "=" * 70)
    print("Tools test complete!")


if __name__ == "__main__":
    test_tools()