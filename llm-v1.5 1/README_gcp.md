# Hyundai RAG

## ğŸ“Œ ì£¼ìš” ê¸°ëŠ¥

- Retrievers
- Consistency Check
- Evaluation

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```bash
project/
â”œâ”€â”€ configs/*.yaml          # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ì„¤ì •
â”œâ”€â”€ datasets/               # ë°ì´í„° ì €ì¥ ê³µê°„ (RECOMMENDED)
â”œâ”€â”€ embeddings/             # embedë¥¼ ìˆ˜í–‰ ê°€ëŠ¥í•œ embedding class ì •ì˜
â”œâ”€â”€ kiwi_custom/            # customized kiwi
â”œâ”€â”€ llms/                   # invokeë¥¼ ìˆ˜í–‰ ê°€ëŠ¥í•œ chatmodel class ì •ì˜
â”œâ”€â”€ results/                # evaluation ìˆ˜í–‰ ê²°ê³¼ json í˜•íƒœë¡œ ì €ì¥
â”œâ”€â”€ retrievers/             # similarity_search_with_scoreë¥¼ ìˆ˜í–‰ ê°€ëŠ¥í•œ retriever class ì •ì˜
â”œâ”€â”€ template/               # llms ëª¨ë¸ ì¡°ì •ì„ ìœ„í•œ template
â”œâ”€â”€ utils 
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ consistency.py      # llm ëª¨ë¸ ì¼ê´€ì„± í‰ê°€ë¥¼ ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ load_retriever.py   # configë¥¼ í†µí•´ retriever ìƒì„±ì„ ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ metrics.py          # retriever, reranker ì„±ëŠ¥ í‰ê°€ë¥¼ ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€preprocess.py       # ê¸°íƒ€
â”œâ”€â”€ configs/*.yaml       # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ì„¤ì •
â”œâ”€â”€ requirements.txt     # í•„ìš”í•œ íŒ¨í‚¤ì§€ ëª©ë¡
â”œâ”€â”€ README.md
â””â”€â”€ ...
```

1. ì„¤ì¹˜

# python 3.10

# create env
```bash
python -m venv test
source test/bin/activate
```

# ì˜ì¡´ì„± ì„¤ì¹˜
```bash
pip install -r requirements.txt
```

# ollama ì„¤ì¹˜
```bash
sudo tar -C /usr -xzf ollama-linux-amd64.tgz
```

# llama cpp ì„¤ì¹˜
git clone https://github.com/ggerganov/llama.cpp.git (pws)
vmìœ¼ë¡œ í•´ë‹¹ ì••ì¶•íŒŒì¼ ì´ë™ í›„ í•´ì œ
í´ë” ë‚´ ë””íœë˜ì‹œ ì„¤ì¹˜ 
```bash
pip install -r requirments.txt 
```

# ollama gguf ìƒì„±
í•˜ê¸° ì½”ë“œë¥¼ ì´ìš©í•˜ì—¬ hf íŒŒì¼ ggufë¡œ ë³€ê²½
```bash
python convert_hf_to_gguf.py ../gemma3-4b --outtype auto --outfile gemma3-4b.gguf
```

í•˜ê¸°ì™€ ê°™ì´ Modelfile ë§Œë“¤ê³  ggufì™€ ê°™ì€ ê²½ë¡œì— ë°°ì¹˜
```bash
FROM ./gemma3-4b.gguf

# stop tokens ì„¤ì •
PARAMETER stop ["<end_of_turn>"]

# temperature ì„¤ì • (1ë¡œ)
PARAMETER temperature 1

# top_k ì„¤ì •
PARAMETER top_k 64

# top_p ì„¤ì •
PARAMETER top_p 0.95

# template
TEMPLATE """
{{- range $i, $_ := .Messages }}
{{- $last := eq (len (slice $.Messages $i)) 1 }}
{{- if or (eq .Role "user") (eq .Role "system") }}<start_of_turn>user
{{ .Content }}<end_of_turn>
{{ if $last }}<start_of_turn>model
{{ end }}
{{- else if eq .Role "assistant" }}<start_of_turn>model
{{ .Content }}{{ if not $last }}<end_of_turn>
{{ end }}
{{- end }}
{{- end }}
"""
```

gguf, Modelfile ê²½ë¡œì—ì„œ í•˜ê¸° ëª…ë ¹ì–´ë“¤ ìˆ˜í–‰ í™•ì¸.
ollama serve
ollama create gemma3-4b -f Modelfile
ollama list
ollama run gemma3-4b


<!-- # vllm ë³„ë„ ì„¤ì¹˜ (í•˜ê¸° ì´ìŠˆ ëŒ€ì‘)
```bash
error: could not create 'build\bdist.win-amd64\wheel\.\vllm\model_executor\layers\fused_moe\configs\E=256,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json': No such file or directory
```
win+r -> regedit -> HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\FileSystem LongPathsEnabled -> value to 1
pip install vllm==0.8.5.post1 -->

2. config ì„¤ì •

# í•˜ë“œì›¨ì–´
```bash
hardware:
  cuda_devices: "0, 1"
```
ê°€ìš©í•  gpu ì„ íƒ

# ë°ì´í„° ê²½ë¡œ
```bash
datasets:
  corpus: "D:\\project\\llm-develop\\datasets\\hyundai\\hyundai_corpus.jsonl"
  queries: "D:\\project\\llm-develop\\datasets\\hyundai\\hyundai_queries.jsonl"
  dev: "D:\\project\\llm-develop\\datasets\\hyundai\\hyundai_dev.jsonl"
```
dataset/ í´ë”ì— ë°ì´í„°ì…‹ì„ ë„£ê±°ë‚˜, ë³„ë„ ê²½ë¡œì— ì¶”ê°€ í›„ ê²½ë¡œ ìˆ˜ì •
corpus  : ê²€ìƒ‰ì„ ì§„í–‰í•  documents
queries : ê²€ìƒ‰ì„ ìœ„í•œ queries
dev     : qid, cid ì—°ê²°ì„± ê´€ë¦¬ 

# ì„ë² ë”© ëª¨ë¸ ì •ì˜
```bash
embedding_models:
  finetune_bge_m3_3:
    name: "finetune BGE-M3"
    path: "D:\\project\\models\\bge-m3_finetune3"
    type: "flag"
```
ì„ë² ë”© ëª¨ë¸ ë¶ˆëŸ¬ì˜¬ ê²½ë¡œì™€ ë¶ˆëŸ¬ì˜¤ëŠ” class í˜•íƒœ ì •ì˜

# LLM ëª¨ë¸ ì •ì˜
```bash
llm_models:

  gemma3-4b:
    name: "gemma3-4b"
    path: "gemma3:4b"
    type: "ollamachatmodel"
    max_tokens: 512

  gemma3-4b:
    name: "gemma3-4b"
    path: "D:\\project\\models\\gemma3-4b"
    type: "customchatmodel" or "vllm"
    max_tokens: 512
```
LLM ëª¨ë¸ ë¶ˆëŸ¬ì˜¬ ê²½ë¡œì™€ ë¶ˆëŸ¬ì˜¤ëŠ” class í˜•íƒœ ë° ìµœëŒ€ ì‘ë‹µ í† í° ìˆ˜ ì •ì˜
ollama ê²½ìš° pathëŠ” servingë˜ê³  ìˆëŠ” NAME
* vllmì€ linux ìš©ë„..

# ê²€ìƒ‰ê¸°, Retrievers ì •ì˜
```bash
  faiss_finetune3:
    name: "faiss_finetune3"
    type: "faiss"
    embedding_model: "finetune_bge_m3_3"
    search_type: "mmr"
    k: 5
    instruction: True # encode_queries
  
  bm25_mecab:
    name: "bm25_mecab"
    type: "bm25"
    tokenizer: "mecab"
    k: 5
    
  # Ensemble retrievers
  ensembles:

    bc_finetune_5_5:
      name: "bc_finetune_5_5"
      retrievers: ["bm25_mecab", "faiss_finetune3"]
      weights: [0.5, 0.5]
      search_type: "mmr"
      k: 5
      c: 60.0
```
ê²€ìƒ‰ê¸° íƒ€ì… ì •ì˜ - bm25 (sparse), faiss (dense)
bm25ì˜ ê²½ìš° tokenizer í•„ìš” - retrievers/BaseRetriever.py ë‚´ tokenizer ì°¸ì¡°
faissì˜ ê²½ìš° embedding_model í•„ìš” - config ë‚´ ì •ì˜ë˜ì–´ ìˆëŠ” ì„ë² ë”© ëª¨ë¸ í˜¸ì¶œ ê°€ëŠ¥
faissì˜ ê²½ìš° instruction ì„¤ì • ê°€ëŠ¥ - ëª¨ë“  ì¿¼ë¦¬ì— ëŒ€í•œ 
faiss-ollamaì˜ ê²½ìš° pathëŠ” servingë˜ê³  ìˆëŠ” ì´ë¦„ ì‚¬ìš©
* faissëŠ” FaissRetrieverWithScores, FaissIndexRetrieverWithScores ì¤‘ Indexë¥¼ ì´ìš©í•œ í›„ì ì‚¬ìš© ì¤‘
ensembleì˜ ê²½ìš° retrievers, weights í•„ìš” - config ë‚´ ì •ì˜ë˜ì–´ ìˆëŠ” retriever í˜¸ì¶œ ê°€ëŠ¥
këŠ” ê²€ìƒ‰ê¸° ê³µí†µ ìš”ì†Œë¡œ, ê²€ìƒ‰ ê°œìˆ˜ ì •ì˜

# Reranker ì •ì˜
```bash
rerankers:
  rerank_bc_finetune_5_5:
    retriever: "bc_finetune_5_5"
    embedding_model: "reranker"
```
retriever, embedding_model í•„ìš” - config ë‚´ ì •ì˜ë˜ì–´ ìˆëŠ” retriever, embedding_model(trained for reranker) í˜¸ì¶œ ê°€ëŠ¥

# Evalaution 
```bash
evaluation:
  k: 5
  metrics:
    - ndcg
    - recall
    - rr
    - precision
    - hit
```
retrievers, rerankersì— ëŒ€í•´ ìœ„ 5ê°€ì§€ í•­ëª© í‰ê°€ ê°€ëŠ¥

# Evalaution 
```bash
consistency:
  consistency_gemma3-4b:
    llm_model: gemma3-4b
    retriever: bc_finetune_5_5
    path_num: 5
    k: 5
    seed: 42
```
llm_model, retriever í•„ìš” - config ë‚´ ì •ì˜ë˜ì–´ ìˆëŠ” llm_model, retriever í˜¸ì¶œ ê°€ëŠ¥


3. ì‹¤í–‰

<!-- # ollama êµ¬ë™
ollama serve  -->

# main ì½”ë“œ í˜¸ì¶œ / ollama ì—°ê²° ì´ìŠˆë¡œ fail
```bash
python main.py --config configs/{file_name}.yaml
```