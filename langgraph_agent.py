"""
LangGraph-based Agent Implementation
StateGraphì™€ Node/Edgeë¥¼ ì‚¬ìš©í•œ LangGraph ì‹œìŠ¤í…œ
"""
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from typing import Dict, Any, List, Optional, Annotated, TypedDict
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.tools import tool
from langgraph.checkpoint.memory import MemorySaver
import json
import time

from src.utils.logger import log
from src.agent.tools import (
    HybridSearchTool, AggregatorTool, RerankerTool, 
    SynthesizerTool, GlossaryTool
)
from src.agent.llm_client import LLMClientFactory

class AgentState(TypedDict):
    """LangGraph Agent State Definition"""
    # Core state
    messages: Annotated[List[BaseMessage], add_messages]
    original_query: str
    current_goal: str
    
    # Execution tracking
    step_count: int
    max_steps: int
    
    # Results accumulation
    search_results: Optional[Dict[str, Any]]
    aggregation_results: Optional[Dict[str, Any]]
    reranked_results: Optional[Dict[str, Any]]
    final_synthesis: Optional[Dict[str, Any]]
    
    # Execution metadata
    execution_start_time: float
    step_execution_times: List[float]
    tools_used: List[str]
    
    # Decision making (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
    next_action: Optional[str]
    confidence_score: float
    requires_search: bool
    requires_aggregation: bool
    requires_reranking: bool
    
    # ğŸ†• Dynamic Planning Support
    execution_plan: Optional[List[Dict[str, Any]]]  # LLMì´ ìƒì„±í•œ ì‹¤í–‰ ê³„íš
    current_step_index: int                         # í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ë‹¨ê³„
    user_intent: Optional[str]                      # LLMì´ íŒŒì•…í•œ ì‚¬ìš©ì ì˜ë„
    complexity_level: Optional[str]                 # ì¿¼ë¦¬ ë³µì¡ë„ (simple/moderate/complex)
    plan_reasoning: Optional[str]                   # ê³„íš ìˆ˜ë¦½ ê·¼ê±°
    
    # Error handling
    errors: List[str]
    retry_count: int

class LangGraphIQSAgent:
    
    def __init__(self, llm_type: str = "h-chat"):
        """
        Initialize LangGraph Agent
        
        Args:
            llm_type: LLM type for the agent
        """
        self.llm_client = LLMClientFactory.create(llm_type)
        
        # Initialize tools
        self.tools = {
            'glossary': GlossaryTool(),
            'search': HybridSearchTool(), 
            'aggregator': AggregatorTool(),
            'reranker': RerankerTool(),
            'synthesizer': SynthesizerTool(llm_client=self.llm_client)
        }
        
        # Create the StateGraph
        self.graph = self._create_graph()
        
        # Memory for conversation
        self.memory = MemorySaver()
        
        # Compile the graph
        self.compiled_graph = self.graph.compile(checkpointer=self.memory)
        
        log.info(f"Initialized LangGraph IQS Agent with {len(self.tools)} tools")
        log.info(f"Available tools: {list(self.tools.keys())}")
    
    def _create_graph(self) -> StateGraph:
        """
        Create the LangGraph StateGraph with Nodes and Edges
        
        Returns:
            Configured StateGraph
        """
        # Create the graph
        workflow = StateGraph(AgentState)
        
        # Add nodes
        workflow.add_node("planner", self._planner_node)
        workflow.add_node("glossary_translator", self._glossary_node)
        workflow.add_node("search_executor", self._search_node)
        workflow.add_node("aggregator_executor", self._aggregator_node)
        workflow.add_node("reranker_executor", self._reranker_node)
        workflow.add_node("synthesizer", self._synthesizer_node)
        workflow.add_node("decision_maker", self._decision_node)
        
        # Set entry point
        workflow.set_entry_point("planner")
        
        # Add conditional edges from planner
        workflow.add_conditional_edges(
            "planner",
            self._route_from_planner,
            {
                "glossary": "glossary_translator",
                "search": "search_executor", 
                "aggregation": "aggregator_executor",
                "end": END
            }
        )
        
        # Add edges from glossary
        workflow.add_edge("glossary_translator", "search_executor")
        
        # Add conditional edges from search
        workflow.add_conditional_edges(
            "search_executor",
            self._route_from_search,
            {
                "aggregation": "aggregator_executor",
                "reranking": "reranker_executor",
                "synthesis": "synthesizer",
                "decision": "decision_maker"
            }
        )
        
        # Add conditional edges from aggregator
        workflow.add_conditional_edges(
            "aggregator_executor", 
            self._route_from_aggregator,
            {
                "reranking": "reranker_executor",
                "synthesis": "synthesizer",
                "decision": "decision_maker"
            }
        )
        
        # Add edges from reranker
        workflow.add_edge("reranker_executor", "decision_maker")
        
        # Add conditional edges from decision maker
        workflow.add_conditional_edges(
            "decision_maker",
            self._route_from_decision,
            {
                "search": "search_executor",
                "aggregation": "aggregator_executor", 
                "reranking": "reranker_executor",
                "synthesis": "synthesizer",
                "end": END
            }
        )
        
        # Add edge from synthesizer to end
        workflow.add_edge("synthesizer", END)
        
        return workflow
    
    # ========== NODE IMPLEMENTATIONS ==========
    
    def _planner_node(self, state: AgentState) -> AgentState:

        log.info("ENHANCED PLANNER NODE: Dynamic LLM-based planning")
        
        original_query = state["original_query"]
        
        # Dynamic planning prompt
        enhanced_planning_prompt = f"""
ë‹¹ì‹ ì€ IQS ì°¨ëŸ‰ í’ˆì§ˆ ë°ì´í„° ë¶„ì„ì„ ìœ„í•œ AI ê³„íš ìˆ˜ë¦½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì‚¬ìš©ì ìš”ì²­**: "{original_query}"

**ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë“¤**:
- **glossary**: í•œ-ì˜ ìš©ì–´ ë²ˆì—­ ë° ë™ì˜ì–´ í™•ì¥
- **search**: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (BGE-M3 ì˜ë¯¸ê²€ìƒ‰ + í‚¤ì›Œë“œ)
- **aggregator**: ë°ì´í„° ì§‘ê³„/í†µê³„ (GROUP BY, COUNT, TOP N)
- **reranker**: ê²°ê³¼ ì¬ì •ë ¬ ë° í•„í„°ë§ 
- **synthesizer**: ìµœì¢… ë¶„ì„ ë° ì‘ë‹µ ìƒì„±

**ì—­í• **: ì´ ìš”ì²­ì„ í•´ê²°í•˜ê¸° ìœ„í•œ ìµœì ì˜ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”.

**ì¤‘ìš” ì§€ì¹¨**:
- ë³µì¡í•œ ìš”ì²­ë„ ë‹¨ê³„ë³„ë¡œ ë¶„í•´í•˜ì„¸ìš”
- ë„êµ¬ë¥¼ ì—¬ëŸ¬ ë²ˆ ì‚¬ìš©í•´ë„ ë©ë‹ˆë‹¤ 
- ì°½ì˜ì ì´ê³  íš¨ìœ¨ì ì¸ í•´ê²°ì±…ì„ ì œì‹œí•˜ì„¸ìš”
- ì‚¬ìš©ì ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ì„¸ìš”

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "user_intent": "ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ê²ƒì€ ë¬´ì—‡ì¸ê°€?",
    "complexity_level": "simple|moderate|complex",
    "execution_plan": [
        {{
            "step": 1,
            "action": "tool_name", 
            "purpose": "ì´ ë‹¨ê³„ì˜ ëª©ì ",
            "parameters": {{}}
        }},
        {{
            "step": 2, 
            "action": "tool_name",
            "purpose": "ë‹¤ìŒ ë‹¨ê³„ ëª©ì ", 
            "parameters": {{}}
        }}
    ],
    "reasoning": "ì „ì²´ì ì¸ ê³„íš ìˆ˜ë¦½ ê·¼ê±°",
    "confidence": 0.0-1.0
}}

**ì˜ˆì‹œ**:
- "INFO12ì™€ INFO13 ë¬¸ì œë¥¼ ê°ê° ì°¾ì•„ì„œ ë¹„êµí•´ì¤˜"
  â†’ [search(INFO12) â†’ search(INFO13) â†’ aggregator(ë¹„êµë¶„ì„) â†’ synthesizer]
- "2024ë…„ê³¼ 2025ë…„ í˜„ëŒ€ì°¨ ì¸í¬í…Œì¸ë¨¼íŠ¸ ë¬¸ì œ ë¹„êµ"
  â†’ [glossary(ìš©ì–´ë²ˆì—­) â†’ search(2024) â†’ search(2025) â†’ aggregator(ë¹„êµ) â†’ synthesizer]
"""
        
        try:
            # LLM ê¸°ë°˜ ë™ì  ê³„íš ìˆ˜ë¦½
            plan_result = self.llm_client.complete_json(enhanced_planning_prompt)
            
            # ë™ì  ìƒíƒœ ì—…ë°ì´íŠ¸
            execution_plan = plan_result.get("execution_plan", [])
            user_intent = plan_result.get("user_intent", "")
            complexity = plan_result.get("complexity_level", "moderate")
            reasoning = plan_result.get("reasoning", "")
            
            # ìƒˆë¡œìš´ ë™ì  í•„ë“œë“¤ ì„¤ì •
            state.update({
                # ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€ (ìë™ ê°ì§€)
                "requires_search": any(step.get("action") == "search" for step in execution_plan),
                "requires_aggregation": any(step.get("action") == "aggregator" for step in execution_plan),
                "requires_reranking": any(step.get("action") == "reranker" for step in execution_plan),
                
                # ë™ì  ê³„íš í•„ë“œë“¤
                "execution_plan": execution_plan,
                "current_step_index": 0,
                "user_intent": user_intent,
                "complexity_level": complexity,
                "plan_reasoning": reasoning,
                "confidence_score": plan_result.get("confidence", 0.8),
                "step_count": state.get("step_count", 0) + 1
            })
            
            # ì²« ë²ˆì§¸ ë‹¨ê³„ ê²°ì •
            if execution_plan and len(execution_plan) > 0:
                first_step = execution_plan[0]
                state["next_action"] = first_step.get("action", "search")
                
                # ê³„íš ë¡œê¹…
                log.info(f"Dynamic plan created: {len(execution_plan)} steps")
                log.info(f"User intent: {user_intent}")
                log.info(f"Complexity: {complexity}")
                log.info(f"First action: {state['next_action']}")
                
                # ê³„íš ìš”ì•½ ìƒì„±
                plan_summary = f"{complexity} ë³µì¡ë„ - {len(execution_plan)}ë‹¨ê³„ ê³„íš: {' â†’ '.join([step.get('action', '') for step in execution_plan])}"
                
            else:
                # í´ë°±: ê¸°ë³¸ ê²€ìƒ‰
                state["next_action"] = "search"
                plan_summary = "ê¸°ë³¸ ê²€ìƒ‰ ê³„íšìœ¼ë¡œ í´ë°±"
                log.warning("No execution plan generated, falling back to search")
            
            # ë©”ì‹œì§€ ì¶”ê°€
            state["messages"].append(
                AIMessage(content=f"ë™ì  ê³„íš ìˆ˜ë¦½ ì™„ë£Œ: {user_intent} - {plan_summary}")
            )
            
            log.info(f"Enhanced planning complete: next_action={state['next_action']}")
            
        except Exception as e:
            log.error(f"Enhanced planning failed: {e}")
            # í´ë°±: ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ë‹¨ìˆœí™”
            state.update({
                "next_action": "search",
                "execution_plan": [{"step": 1, "action": "search", "purpose": "ê¸°ë³¸ ê²€ìƒ‰"}],
                "current_step_index": 0,
                "user_intent": "ê²€ìƒ‰ ìš”ì²­",
                "complexity_level": "simple",
                "plan_reasoning": "ê³„íš ìˆ˜ë¦½ ì‹¤íŒ¨ë¡œ ì¸í•œ ê¸°ë³¸ ê³„íš",
                "errors": state.get("errors", []) + [f"Enhanced planning error: {e}"]
            })
        
        return state
    
    def _glossary_node(self, state: AgentState) -> AgentState:

        log.info("GLOSSARY NODE: Translating Korean terms")
        
        try:
            glossary_result = self.tools['glossary'].execute(
                query=state["original_query"],
                include_synonyms=True
            )
            
            # Update query if translation applied
            if glossary_result.get("translation_applied", False):
                enhanced_query = glossary_result.get("enhanced_query", state["original_query"])
                state["current_goal"] = enhanced_query
                log.info(f"Query enhanced: '{state['original_query']}' â†’ '{enhanced_query}'")
            
            state.update({
                "step_count": state.get("step_count", 0) + 1,
                "tools_used": state.get("tools_used", []) + ["glossary"]
            })
            
            state["messages"].append(
                AIMessage(content=f"ìš©ì–´ ë²ˆì—­ ì™„ë£Œ: {glossary_result.get('enhanced_query', '')}")
            )
            
        except Exception as e:
            log.error(f"Glossary translation failed: {e}")
            state["errors"] = state.get("errors", []) + [f"Glossary error: {e}"]
        
        return state
    
    def _search_node(self, state: AgentState) -> AgentState:
        """
        Search Executor Node: Perform hybrid search
        """
        log.info("SEARCH NODE: Executing hybrid search")
        
        try:
            query = state.get("current_goal", state["original_query"])
            
            search_result = self.tools['search'].execute(
                query=query,
                limit=3000,
                step_description="Comprehensive data search",
                original_query=state["original_query"]
            )
            
            state.update({
                "search_results": search_result,
                "step_count": state.get("step_count", 0) + 1,
                "tools_used": state.get("tools_used", []) + ["search"]
            })
            
            total_hits = search_result.get("total_hits", 0)
            state["messages"].append(
                AIMessage(content=f"ê²€ìƒ‰ ì™„ë£Œ: {total_hits:,}ê°œ ë¬¸ì„œ ë°œê²¬")
            )
            
            log.info(f"Search completed: {total_hits:,} documents found")
            
        except Exception as e:
            log.error(f"Search failed: {e}")
            state["errors"] = state.get("errors", []) + [f"Search error: {e}"]
        
        return state
    
    def _aggregator_node(self, state: AgentState) -> AgentState:
        """
        Aggregator Executor Node: Perform data aggregation
        """
        log.info("AGGREGATOR NODE: Performing data aggregation")
        
        try:
            # Use search results if available
            documents = None
            if state.get("search_results"):
                documents = state["search_results"].get("documents", [])
            
            agg_result = self.tools['aggregator'].execute(
                documents=documents,
                aggregation="terms",
                field="problem",
                size=20,
                step_description="Statistical analysis",
                original_query=state["original_query"]
            )
            
            state.update({
                "aggregation_results": agg_result,
                "step_count": state.get("step_count", 0) + 1,
                "tools_used": state.get("tools_used", []) + ["aggregator"]
            })
            
            bucket_count = len(agg_result.get("buckets", []))
            state["messages"].append(
                AIMessage(content=f"ì§‘ê³„ ì™„ë£Œ: {bucket_count}ê°œ í•­ëª© ë¶„ì„")
            )
            
            log.info(f"Aggregation completed: {bucket_count} buckets")
            
        except Exception as e:
            log.error(f"Aggregation failed: {e}")
            state["errors"] = state.get("errors", []) + [f"Aggregation error: {e}"]
        
        return state
    
    def _reranker_node(self, state: AgentState) -> AgentState:
        """
        Reranker Executor Node: Rerank and filter results
        """
        log.info("RERANKER NODE: Reranking results")
        
        try:
            # Use search or aggregation results
            documents = None
            if state.get("search_results"):
                documents = state["search_results"].get("documents", [])
            elif state.get("aggregation_results"):
                documents = state["aggregation_results"].get("documents", [])
            
            if documents:
                rerank_result = self.tools['reranker'].execute(
                    documents=documents,
                    query=state.get("current_goal", state["original_query"]),
                    top_k=15
                )
                
                state.update({
                    "reranked_results": rerank_result,
                    "step_count": state.get("step_count", 0) + 1,
                    "tools_used": state.get("tools_used", []) + ["reranker"]
                })
                
                reranked_count = rerank_result.get("total_reranked", 0)
                state["messages"].append(
                    AIMessage(content=f"ì¬ìˆœìœ„í™” ì™„ë£Œ: {reranked_count}ê°œ ë¬¸ì„œ ì •ë ¬")
                )
                
                log.info(f"Reranking completed: {reranked_count} documents")
            else:
                log.warning("No documents to rerank")
            
        except Exception as e:
            log.error(f"Reranking failed: {e}")
            state["errors"] = state.get("errors", []) + [f"Reranking error: {e}"]
        
        return state
    
    def _synthesizer_node(self, state: AgentState) -> AgentState:
        """
        Synthesizer Node: Generate final answer (standard mode)
        """
        log.info("SYNTHESIZER NODE: Generating final synthesis (standard mode)")
        
        try:
            # Collect all results
            results = []
            if state.get("search_results"):
                results.append(state["search_results"])
            if state.get("aggregation_results"):
                results.append(state["aggregation_results"])
            if state.get("reranked_results"):
                results.append(state["reranked_results"])
            
            synthesis_result = self.tools['synthesizer'].execute(
                results=results,
                original_query=state["original_query"]
            )
            
            state.update({
                "final_synthesis": synthesis_result,
                "step_count": state.get("step_count", 0) + 1,
                "tools_used": state.get("tools_used", []) + ["synthesizer"]
            })
            
            state["messages"].append(
                AIMessage(content=synthesis_result.get("synthesis", "ë¶„ì„ ì™„ë£Œ"))
            )
            
            log.info("Final synthesis completed")
            
        except Exception as e:
            log.error(f"Synthesis failed: {e}")
            state["errors"] = state.get("errors", []) + [f"Synthesis error: {e}"]
        
        return state
    
    def _synthesizer_node_streaming(self, state: AgentState):
        """
        Synthesizer Node: Generate final answer (streaming mode)
        LangGraph astream() ì „ìš© - ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ yield
        """
        log.info("SYNTHESIZER NODE: Starting streaming synthesis")
        
        try:
            # Collect all results
            results = []
            if state.get("search_results"):
                results.append(state["search_results"])
            if state.get("aggregation_results"):
                results.append(state["aggregation_results"])
            if state.get("reranked_results"):
                results.append(state["reranked_results"])
            
            # SynthesizerTool ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
            accumulated_text = ""
            for chunk in self.tools['synthesizer'].execute_streaming(
                results=results,
                original_query=state["original_query"]
            ):
                if chunk:
                    accumulated_text += chunk
                    # LangGraph astreamì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ yield
                    yield {
                        "type": "synthesis_chunk",
                        "content": chunk,
                        "accumulated": accumulated_text
                    }
            
            # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ state ì—…ë°ì´íŠ¸
            synthesis_result = {
                "query": state["original_query"],
                "synthesis": accumulated_text,
                "source_count": len(results)
            }
            
            state.update({
                "final_synthesis": synthesis_result,
                "step_count": state.get("step_count", 0) + 1,
                "tools_used": state.get("tools_used", []) + ["synthesizer"]
            })
            
            # ìµœì¢… ì™„ë£Œ ì‹ í˜¸
            yield {
                "type": "synthesis_complete",
                "content": accumulated_text,
                "state": state
            }
            
            log.info("Streaming synthesis completed")
            
        except Exception as e:
            log.error(f"Streaming synthesis failed: {e}")
            yield {
                "type": "synthesis_error",
                "content": f"Synthesis error: {e}",
                "state": state
            }
    
    def _decision_node(self, state: AgentState) -> AgentState:
        """
        ğŸš€ Enhanced Dynamic Decision Node: Plan-aware intelligent decision making
        """
        log.info("ENHANCED DECISION NODE: Dynamic plan-based decision making")
        
        # ğŸ†• ë™ì  ê³„íš ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        execution_plan = state.get("execution_plan", [])
        current_step_index = state.get("current_step_index", 0)
        user_intent = state.get("user_intent", "")
        
        # ê¸°ì¡´ ìƒíƒœ ì²´í¬
        has_search = state.get("search_results") is not None
        has_aggregation = state.get("aggregation_results") is not None
        has_reranking = state.get("reranked_results") is not None
        step_count = state.get("step_count", 0)
        max_steps = state.get("max_steps", 10)
        tools_used = state.get("tools_used", [])
        
        # ğŸ†• ë™ì  ê³„íšì´ ìˆëŠ” ê²½ìš° - ê³„íš ë°©ì‹ ëŒ€ì‹  LLM ììœ¨ íŒë‹¨
        if execution_plan and len(execution_plan) > 0:
            log.info(f"Following dynamic execution plan: step {current_step_index + 1}/{len(execution_plan)}")
            
            # í˜„ì¬ ì§€ì  ë° ë‹¤ìŒ ë‹¨ê³„ ì²´í¬
            next_step_index = current_step_index + 1
            
            # LLM ê¸°ë°˜ ë™ì  ì˜ì‚¬ê²°ì • (ë” ìœ ì—°í•¨)
            decision_context = {
                "original_plan": execution_plan,
                "current_step": current_step_index,
                "next_planned_step": next_step_index,
                "user_intent": user_intent,
                "has_search_results": has_search,
                "has_aggregation_results": has_aggregation, 
                "has_rerank_results": has_reranking,
                "step_count": step_count,
                "tools_used": tools_used,
                "errors": state.get("errors", [])
            }
            
            decision_prompt = f"""
ë‹¹ì‹ ì€ IQS ë°ì´í„° ë¶„ì„ ì‹¤í–‰ ì œì–´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì›ë³¸ ìš”ì²­**: "{state.get('original_query', '')}"
**ì‚¬ìš©ì ì˜ë„**: "{user_intent}"
**ì›ë˜ ê³„íš**: {json.dumps(execution_plan, ensure_ascii=False, indent=2)}

**í˜„ì¬ ì‹¤í–‰ ìƒí™©**:
{json.dumps(decision_context, ensure_ascii=False, indent=2)}

**ì—­í• **: ë‹¤ìŒ ì‹¤í–‰ ë‹¨ê³„ë¥¼ ë™ì ìœ¼ë¡œ ê²°ì •í•˜ì„¸ìš”.

ê³ ë ¤ì‚¬í•­:
1. ì›ë˜ ê³„íšì„ ë”°ë¥¼ ê²ƒì¸ê°€, ì•„ë‹ˆë©´ ì ì‘í•  ê²ƒì¸ê°€?
2. í˜„ì¬ê¹Œì§€ì˜ ê²°ê³¼ê°€ ì¶©ë¶„í•œê°€?
3. ì—ëŸ¬ë‚˜ ì‹¤íŒ¨ê°€ ìˆì—ˆë‹¤ë©´ ì–´ë–»ê²Œ ë³µêµ¬í•  ê²ƒì¸ê°€?
4. ì‚¬ìš©ì ì˜ë„ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì¶”ê°€ ë‹¨ê³„ê°€ í•„ìš”í•œê°€?

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "decision": "continue_plan|adapt_plan|skip_step|complete|synthesis",
    "next_action": "search|aggregator|reranker|glossary|synthesis|end",
    "reasoning": "ê²°ì • ê·¼ê±°",
    "adaptation_applied": false,
    "confidence": 0.0-1.0
}}
"""
            
            try:
                # LLM ê¸°ë°˜ ë™ì  ì˜ì‚¬ê²°ì •
                decision_result = self.llm_client.complete_json(decision_prompt)
                
                decision = decision_result.get("decision", "continue_plan")
                next_action = decision_result.get("next_action", "synthesis")
                reasoning = decision_result.get("reasoning", "")
                
                # ìƒíƒœ ì—…ë°ì´íŠ¸
                state.update({
                    "next_action": next_action,
                    "step_count": step_count + 1
                })
                
                # ê³„íš ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                if decision == "continue_plan" and next_step_index < len(execution_plan):
                    state["current_step_index"] = next_step_index
                elif decision in ["complete", "synthesis"]:
                    state["current_step_index"] = len(execution_plan)  # ê³„íš ì™„ë£Œ
                
                log.info(f"Enhanced decision: {decision} â†’ {next_action}")
                log.info(f"Reasoning: {reasoning}")
                log.info(f"Plan progress: {state.get('current_step_index', 0)}/{len(execution_plan)}")
                
                return state
                
            except Exception as e:
                log.error(f"Enhanced decision making failed: {e}")
                # í´ë°±: ê¸°ì¡´ ë¡œì§ìœ¼ë¡œ
                pass
        
        # ğŸ†• í´ë°±: ê¸°ì¡´ ë¡œì§ (ê³„íšì´ ì—†ìœ¼ê±°ë‚˜ LLM ì˜ì‚¬ê²°ì • ì‹¤íŒ¨ ì‹œ)
        log.info("Falling back to legacy decision logic")
        
        # Count how many times each tool was used
        reranker_count = tools_used.count("reranker")
        search_count = tools_used.count("search")
        aggregator_count = tools_used.count("aggregator")
        
        # Enhanced decision logic with comprehensive loop prevention
        if step_count >= max_steps:
            state["next_action"] = "synthesis"
            log.info("Max steps reached, forcing synthesis")
        elif reranker_count >= 2:  # Prevent infinite reranking
            state["next_action"] = "synthesis"
            log.info(f"Reranking limit reached ({reranker_count} times), forcing synthesis")
        elif not has_search and search_count == 0:  # Need initial search
            state["next_action"] = "search"
            log.info("Starting with search")
        elif has_search and not has_aggregation and state.get("requires_aggregation", False) and aggregator_count == 0:
            state["next_action"] = "aggregation"
            log.info("Moving to aggregation")
        elif (has_search or has_aggregation) and state.get("requires_reranking", False) and reranker_count == 0:
            # Only rerank if we have actual documents
            search_docs = state.get("search_results", {}).get("documents", [])
            agg_docs = state.get("aggregation_results", {}).get("documents", [])
            if search_docs or agg_docs:
                state["next_action"] = "reranking"
                log.info("Starting reranking")
            else:
                state["next_action"] = "synthesis"
                log.info("No documents to rerank, forcing synthesis")
        else:
            state["next_action"] = "synthesis"
            log.info("All tasks complete, proceeding to synthesis")
        
        log.info(f"Legacy decision: next_action={state['next_action']} (reranker_count={reranker_count}, step_count={step_count})")
        
        return state
    
    # ========== ğŸ†• ENHANCED ROUTING FUNCTIONS ==========

    def _route_from_planner(self, state: AgentState) -> str:
        """ğŸ†• Enhanced routing from planner - supports dynamic actions"""
        next_action = state.get("next_action", "search")
        
        # ë™ì  ë¼ìš°íŒ… ë§µ
        routing_map = {
            "glossary": "glossary_translator",
            "search": "search_executor", 
            "aggregator": "aggregator_executor",
            "aggregation": "aggregator_executor",  # ë™ì˜ì–´ ì§€ì›
            "reranker": "reranker_executor",
            "reranking": "reranker_executor",      # ë™ì˜ì–´ ì§€ì›
            "synthesizer": "synthesizer",
            "synthesis": "synthesizer",            # ë™ì˜ì–´ ì§€ì›
            "end": "END"
        }
        
        route = routing_map.get(next_action, "search_executor")
        log.info(f"Enhanced planner routing: {next_action} â†’ {route}")
        return route
    
    def _route_from_search(self, state: AgentState) -> str:
        """ğŸ†• Enhanced routing from search - plan-aware"""
        # ë™ì  ê³„íšì´ ìˆìœ¼ë©´ decisionìœ¼ë¡œ ë³´ë‚´ì„œ LLMì´ ê²°ì •
        execution_plan = state.get("execution_plan", [])
        if execution_plan:
            log.info("Search completed, routing to enhanced decision maker")
            return "decision"
        
        # í´ë°±: ê¸°ì¡´ ë¡œì§
        if state.get("requires_aggregation", False):
            return "aggregation"
        elif state.get("requires_reranking", False):
            return "reranking"
        else:
            return "decision"
        
    def _route_from_aggregator(self, state: AgentState) -> str:
        """ğŸ†• Enhanced routing from aggregator - plan-aware"""
        # ë™ì  ê³„íšì´ ìˆìœ¼ë©´ decisionìœ¼ë¡œ ë³´ë‚´ì„œ LLMì´ ê²°ì •
        execution_plan = state.get("execution_plan", [])
        if execution_plan:
            log.info("Aggregation completed, routing to enhanced decision maker")
            return "decision"
            
        # í´ë°±: ê¸°ì¡´ ë¡œì§
        if state.get("requires_reranking", False):
            return "reranking" 
        else:
            return "decision"
        
    def _route_from_decision(self, state: AgentState) -> str:
        """ğŸ†• Enhanced routing from decision maker - supports all tools"""
        next_action = state.get("next_action", "synthesis")
        
        # í™•ì¥ëœ ë¼ìš°íŒ… ë§µ
        routing_map = {
            "search": "search",
            "aggregator": "aggregation",
            "aggregation": "aggregation", 
            "reranker": "reranking",
            "reranking": "reranking",
            "glossary": "glossary",
            "synthesizer": "synthesis",
            "synthesis": "synthesis",
            "end": "end"
        }
        
        route = routing_map.get(next_action, "synthesis")
        log.info(f"Enhanced decision routing: {next_action} â†’ {route}")
        return route
    
    # ========== TOOL MANAGEMENT METHODS ==========
    
    def build_search_index(self, documents: List[Dict[str, Any]]):
        """
        Build search index for the hybrid search tool
        
        Args:
            documents: List of documents to index
        """
        search_tool = self.tools.get('search')
        if search_tool and hasattr(search_tool, 'build_index'):
            search_tool.build_index(documents)
            log.info(f"Search index built with {len(documents)} documents")
        else:
            log.warning("Search tool not available or does not support indexing")
    
    def get_tool_info(self) -> Dict[str, Any]:
        """
        Get information about available tools
        
        Returns:
            Tool information dictionary
        """
        tool_info = {}
        for name, tool in self.tools.items():
            tool_info[name] = {
                'name': getattr(tool, 'name', name),
                'description': getattr(tool, 'description', 'No description'),
                'available': True
            }
        return tool_info
    
    def validate_tools(self) -> Dict[str, bool]:
        """
        Validate that all tools are properly initialized
        
        Returns:
            Dictionary with tool validation results
        """
        validation_results = {}
        for name, tool in self.tools.items():
            try:
                # Basic validation - check if tool has required methods
                has_execute = hasattr(tool, 'execute')
                has_description = hasattr(tool, 'description') or hasattr(tool, 'get_description')
                validation_results[name] = has_execute and has_description
                
                if not validation_results[name]:
                    log.warning(f"Tool {name} failed validation: execute={has_execute}, description={has_description}")
                    
            except Exception as e:
                log.error(f"Tool {name} validation error: {e}")
                validation_results[name] = False
        
        return validation_results
    
    # ========== PUBLIC METHODS ==========
    
    def process_query(self, query: str, thread_id: str = None) -> Dict[str, Any]:
        """
        Process query using LangGraph (standard mode)
        
        Args:
            query: User query
            thread_id: Optional thread ID for conversation
            
        Returns:
            Processing result
        """
        log.info(f"Processing query (standard mode): '{query}'")
        
        initial_state = AgentState(
            messages=[HumanMessage(content=query)],
            original_query=query,
            current_goal=query, 
            step_count=0,
            max_steps=15,  # ë™ì  ê³„íšì„ ìœ„í•´ ì¦ê°€
            execution_start_time=time.time(),
            step_execution_times=[],
            tools_used=[],
            errors=[],
            retry_count=0,
            confidence_score=0.7,
            # ê¸°ì¡´ í•„ë“œ (í˜¸í™˜ì„±)
            requires_search=True,
            requires_aggregation=False,
            requires_reranking=False,
            # ë™ì  ê³„íš í•„ë“œë“¤
            execution_plan=None,
            current_step_index=0,
            user_intent=None,
            complexity_level=None,
            plan_reasoning=None
        )
        
        thread_config = {
            "configurable": {"thread_id": thread_id or "default"},
            "recursion_limit": 50
        }
        
        try:
            # Execute the graph
            final_state = self.compiled_graph.invoke(initial_state, config=thread_config)
            
            execution_time = time.time() - final_state["execution_start_time"]
            
            # Prepare result
            result = {
                "success": True,
                "query": query,
                "execution_time": execution_time,
                "steps_executed": final_state["step_count"],
                "tools_used": final_state["tools_used"],
                "final_synthesis": final_state.get("final_synthesis"),
                "search_results": final_state.get("search_results"),
                "aggregation_results": final_state.get("aggregation_results"),
                "errors": final_state.get("errors", []),
                "langgraph_state": final_state
            }
            
            log.info(f"LangGraph execution completed in {execution_time:.2f}s")
            return result
            
        except Exception as e:
            log.error(f"LangGraph execution failed: {e}")
            return {
                "success": False,
                "query": query,
                "error": str(e),
                "execution_time": time.time() - initial_state["execution_start_time"]
            }
    
    def process_query_streaming(self, query: str, thread_id: str = None):
        """
        Process query using LangGraph (streaming mode)
        ì „ëµ 1 ì ìš©: LangGraph astream() + Synthesizer ìŠ¤íŠ¸ë¦¬ë°
        
        Args:
            query: User query
            thread_id: Optional thread ID for conversation
            
        Yields:
            Streaming chunks from synthesis stage
        """
        log.info(f"Processing query (streaming mode): '{query}'")
        
        initial_state = AgentState(
            messages=[HumanMessage(content=query)],
            original_query=query,
            current_goal=query, 
            step_count=0,
            max_steps=15,  # ë™ì  ê³„íšì„ ìœ„í•´ ì¦ê°€
            execution_start_time=time.time(),
            step_execution_times=[],
            tools_used=[],
            errors=[],
            retry_count=0,
            confidence_score=0.7,
            # ê¸°ì¡´ í•„ë“œ (í˜¸í™˜ì„±)
            requires_search=True,
            requires_aggregation=False,
            requires_reranking=False,
            # ë™ì  ê³„íš í•„ë“œë“¤
            execution_plan=None,
            current_step_index=0,
            user_intent=None,
            complexity_level=None,
            plan_reasoning=None
        )
        
        thread_config = {
            "configurable": {"thread_id": thread_id or "default"},
            "recursion_limit": 50
        }
        
        try:
            # ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ëŒ€ì‹  ì‚¬ìš©ì ì •ì˜ ìŠ¤íŠ¸ë¦¬ë°
            for chunk in self._custom_streaming_execution(initial_state, thread_config):
                yield chunk
                
        except Exception as e:
            log.error(f"LangGraph streaming failed: {e}")
            yield f"Streaming execution error: {e}"
    
    def _custom_streaming_execution(self, initial_state: AgentState, thread_config: dict):
        """
        ì‚¬ìš©ì ì •ì˜ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
        LangGraph astream() ëŒ€ì‹  ì§ì ‘ ì œì–´
        """
        state = initial_state
        
        # Phase 1: ë¹„ ìŠ¤íŠ¸ë¦¬ë° ë‹¨ê³„ë“¤ ìˆœì°¨ ì‹¤í–‰
        yield "LangGraph Agent ì‹œì‘ ì™„ë£Œ"
        
        # Planner node
        state = self._planner_node(state)
        yield "Planning ë‹¨ê³„ ì™„ë£Œ"
        
        # Conditional execution based on plan
        if state.get("next_action") == "glossary":
            state = self._glossary_node(state)
            yield "Glossary ë‹¨ê³„ ì™„ë£Œ"
        
        # Search node
        if state.get("requires_search", True):
            state = self._search_node(state)
            search_hits = state.get("search_results", {}).get("total_hits", 0)
            yield f"Search ë‹¨ê³„ ì™„ë£Œ ({search_hits:,}ê°œ ë¬¸ì„œ ë°œê²¬)"
        
        # Aggregation node
        if state.get("requires_aggregation", False):
            state = self._aggregator_node(state)
            bucket_count = len(state.get("aggregation_results", {}).get("buckets", []))
            yield f"Aggregation ë‹¨ê³„ ì™„ë£Œ ({bucket_count}ê°œ ì¹´í…Œê³ ë¦¬)"
        
        # Reranking node
        if state.get("requires_reranking", False):
            state = self._reranker_node(state)
            reranked_count = state.get("reranked_results", {}).get("total_reranked", 0)
            yield f"Reranking ë‹¨ê³„ ì™„ë£Œ ({reranked_count}ê°œ ë¬¸ì„œ ì •ë ¬)"
        
        # Phase 2: Synthesis ë‹¨ê³„ ìŠ¤íŠ¸ë¦¬ë°
        yield "\n\nSynthesis ë‹¨ê³„ ì‹œì‘ - ì‹¤ì‹œê°„ ë‹µë³€ ìƒì„± ì¤‘:\n\n"
        
        # ìŠ¤íŠ¸ë¦¬ë° Synthesizer ì‹¤í–‰
        for synthesis_chunk in self._synthesizer_node_streaming(state):
            if synthesis_chunk["type"] == "synthesis_chunk":
                yield synthesis_chunk["content"]
            elif synthesis_chunk["type"] == "synthesis_complete":
                # ì¢…í•© ì™„ë£Œ - state ì—…ë°ì´íŠ¸
                state = synthesis_chunk["state"]
                break
            elif synthesis_chunk["type"] == "synthesis_error":
                yield synthesis_chunk["content"]
                break
        
        log.info("Custom streaming execution completed")
    
    def visualize_graph(self, output_path: str = "langgraph_visualization.png"):
        """
        Visualize the LangGraph structure
        
        Args:
            output_path: Path to save visualization
        """
        try:
            # Create a mermaid representation
            mermaid_graph = self._generate_mermaid_graph()
            
            # Save to file
            with open(output_path.replace('.png', '.md'), 'w', encoding='utf-8') as f:
                f.write(f"# LangGraph IQS Agent Visualization\n\n```mermaid\n{mermaid_graph}\n```")
            
            log.info(f"Graph visualization saved to {output_path}")
            return mermaid_graph
            
        except Exception as e:
            log.error(f"Graph visualization failed: {e}")
            return None
    
    def _generate_mermaid_graph(self) -> str:
        """Generate Mermaid graph representation"""
        mermaid = """
graph TD
    START([User Query]) --> PLANNER{Planner<br/>Query Analysis}
    
    PLANNER -->|Korean detected| GLOSSARY[Glossary<br/>Translation]
    PLANNER -->|Direct search| SEARCH[Search<br/>Hybrid Search]
    PLANNER -->|Direct aggregation| AGGREGATOR[Aggregator<br/>Data Analysis]
    PLANNER -->|Complete| END([End])
    
    GLOSSARY --> SEARCH
    
    SEARCH -->|Need aggregation| AGGREGATOR
    SEARCH -->|Need reranking| RERANKER[Reranker<br/>Result Filtering]
    SEARCH -->|Check completion| DECISION{Decision<br/>Next Action?}
    
    AGGREGATOR -->|Need reranking| RERANKER
    AGGREGATOR -->|Check completion| DECISION
    
    RERANKER --> DECISION
    
    DECISION -->|More search needed| SEARCH
    DECISION -->|More aggregation| AGGREGATOR
    DECISION -->|More reranking| RERANKER
    DECISION -->|Ready for synthesis| SYNTHESIZER[Synthesizer<br/>Final Answer]
    DECISION -->|Complete| END
    
    SYNTHESIZER --> END
    
    style START fill:#e1f5fe
    style PLANNER fill:#f3e5f5
    style GLOSSARY fill:#fff3e0
    style SEARCH fill:#e8f5e8
    style AGGREGATOR fill:#fff8e1
    style RERANKER fill:#fce4ec
    style DECISION fill:#f1f8e9
    style SYNTHESIZER fill:#e3f2fd
    style END fill:#ffebee
"""
        return mermaid
    
    def get_graph_state_info(self) -> Dict[str, Any]:
        """Get information about the graph structure"""
        return {
            "nodes": [
                "planner", "glossary_translator", "search_executor",
                "aggregator_executor", "reranker_executor", "synthesizer", "decision_maker"
            ],
            "entry_point": "planner",
            "end_points": ["synthesizer"],
            "conditional_edges": [
                "planner -> {glossary, search, aggregation, end}",
                "search_executor -> {aggregation, reranking, synthesis, decision}",
                "aggregator_executor -> {reranking, synthesis, decision}",
                "decision_maker -> {search, aggregation, reranking, synthesis, end}"
            ],
            "tools_available": list(self.tools.keys()),
            "state_fields": list(AgentState.__annotations__.keys())
        }


### test
def test_langgraph_agent():
    """Test the LangGraph Agent"""
    print("=" * 70)
    print("Testing LangGraph IQS Agent")
    print("=" * 70)
    
    try:
        # Initialize agent
        agent = LangGraphIQSAgent(llm_type="h-chat")
        
        # Test graph visualization
        print("\n1. Generating graph visualization...")
        mermaid = agent.visualize_graph()
        if mermaid:
            print("Graph visualization generated")
            print("Check 'langgraph_visualization.md' for the graph")
        
        # Test query processing
        print("\n2. Testing query processing...")
        test_queries = [
            "ì¸í¬í…Œì¸ë¨¼íŠ¸ ë¬¸ì œ ìƒìœ„ 3ê°œ",
            "2025ë…„ ì‚°íƒ€í˜ íƒ€ì´ì–´ ë¬¸ì œ",
            "ì—”ì§„ ì†ŒìŒ ê´€ë ¨ ë¶ˆë§Œ ì°¾ì•„ì¤˜"
        ]
        
        for i, query in enumerate(test_queries, 1):
            print(f"\n  Test {i}: {query}")
            result = agent.process_query(query, thread_id=f"test_{i}")
            
            if result["success"]:
                print(f"    Success: {result['steps_executed']} steps")
                print(f"    Tools used: {result['tools_used']}")
                print(f"    Time: {result['execution_time']:.2f}s")
            else:
                print(f"    Failed: {result['error']}")
        
        # Test graph info
        print("\n3. Graph structure info:")
        graph_info = agent.get_graph_state_info()
        print(f"    Nodes: {len(graph_info['nodes'])}")
        print(f"    Tools: {len(graph_info['tools_available'])}")
        print(f"    State fields: {len(graph_info['state_fields'])}")
        
        print("\n" + "=" * 70)
        print("LangGraph Agent test complete!")
        return True
        
    except Exception as e:
        print(f"LangGraph test failed: {e}")
        return False


if __name__ == "__main__":
    test_langgraph_agent()