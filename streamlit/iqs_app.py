import streamlit as st
import sys
import os
from pathlib import Path
from langchain_core.messages import HumanMessage, AIMessage
import pandas as pd
import traceback
from datetime import datetime
import time
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

try:
    from src.agent.agent_pipeline import AgentPipeline
    from src.utils.logger import log
except ImportError as e:
    st.error(f"ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    st.error("í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”.")
    st.stop()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "search_history" not in st.session_state:
    st.session_state.search_history = []
if "last_analysis_time" not in st.session_state:
    st.session_state.last_analysis_time = None

def initialize_iqs_system():
    """IQS ì‹œìŠ¤í…œ ì´ˆê¸°í™” - ì‘ì€ ìŠ¤í”¼ë„ˆ ì‚¬ìš©"""
    try:
        if "iqs_pipeline" not in st.session_state:
            # ì‘ì€ ë¡œë”© ìŠ¤í”¼ë„ˆë¡œ ì´ˆê¸°í™” í‘œì‹œ
            init_container = st.empty()
            init_container.markdown('<div class="small-spinner"><div class="spinner"></div>IQS ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤...</div>', unsafe_allow_html=True)
            
            st.session_state.iqs_pipeline = AgentPipeline(llm_type="h-chat")
            
            # ì´ˆê¸°í™” ì™„ë£Œ í›„ ìŠ¤í”¼ë„ˆ ì œê±°
            init_container.empty()
            st.success("ì´ˆê¸°í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. IQS ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
        return st.session_state.iqs_pipeline
    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        st.error("í™˜ê²½ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš” (.env íŒŒì¼ì˜ HCHAT_API_KEY ë“±)")
        return None

def stream_agent_response(pipeline, query):
    """
    LangGraph Agent ì™„ì „ ìœ„ì„ - Hallucination ë°©ì§€:
    1. LangGraph Agentê°€ ì „ì²´ ë°ì´í„° ì²˜ë¦¬ ë° ë‹µë³€ ìƒì„± ì™„ë£Œ
    2. Streamlitì€ Agent ê²°ê³¼ë§Œ í‘œì‹œ (ì¶”ê°€ LLM í˜¸ì¶œ ê¸ˆì§€)
    """
    try:
        # ì‹œì  1: LangGraph Agent ì „ì²´ ì‹¤í–‰ ì‹œì‘ (HTML íƒœê·¸ ì œê±°)
        yield 'LangGraph Agentê°€ ë°ì´í„° ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...'
        
        # Agent ëŒ€ê¸° ì‹œê°„ ì‹œê°ì  í”¼ë“œë°±
        import asyncio
        for i in range(3):
            yield "."
            time.sleep(0.3)
        
        yield 'LangGraph Agentê°€ 68,982ê±´ì˜ IQS ë°ì´í„°ë¥¼ ì™„ì „ ììœ¨ì ìœ¼ë¡œ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...'  # HTML íƒœê·¸ ì œê±°
        
        # ì‹œì  1: Agent ì‹¤ì œ ì‹¤í–‰ (ë°ì´í„° ìˆ˜ì§‘ì—ë§Œ ì§‘ì¤‘)
        result = pipeline.process_query(query)
        
        if not result['success']:
            error_msg = result.get('error', 'Unknown error')
            yield f'ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨: {error_msg}'  # HTML íƒœê·¸ ì œê±°
            
            # êµ¬ì²´ì ì¸ í•´ê²° ë°©ì•ˆ ì œì‹œ
            if 'API' in error_msg or 'key' in error_msg.lower():
                yield '<div class="process-info">í•´ê²° ë°©ì•ˆ: .env íŒŒì¼ì˜ HCHAT_API_KEYë¥¼ í™•ì¸í•˜ì„¸ìš”.</div>'
            elif 'timeout' in error_msg.lower():
                yield '<div class="process-info">í•´ê²° ë°©ì•ˆ: ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•˜ê³  ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.</div>'
            else:
                yield '<div class="process-info">í•´ê²° ë°©ì•ˆ: ì§ˆë¬¸ì„ ë‹¤ì‹œ ì…ë ¥í•˜ê±°ë‚˜ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.</div>'
            return
        
        # Agent ì‹¤í–‰ ì™„ë£Œ ì•Œë¦¼
        execution_time = result.get('execution_time', 0)
        tools_used = result.get('agent_info', {}).get('tools_used', [])
        
        yield f'<div class="process-info">LangGraph Agent ì™„ë£Œ (ì†Œìš”ì‹œê°„: {execution_time:.1f}ì´ˆ)'
        if tools_used:
            yield f' ğŸ›  ì‚¬ìš©ëœ ë„êµ¬: {", ".join(tools_used)}'
        yield '</div>'
        
        # ì‹œì  2: LangGraph Agent ê²°ê³¼ ì¶”ì¶œ (ì¶”ê°€ LLM í˜¸ì¶œ ê¸ˆì§€)
        yield '<div class="process-info">Agentê°€ ìƒì„±í•œ ìµœì¢… ë‹µë³€ì„ ì¶”ì¶œí•˜ê³  ìˆìŠµë‹ˆë‹¤...</div>\n\n'
        
        # Agentì˜ SynthesizerTool ê²°ê³¼ë§Œ ì‚¬ìš© (ì¶”ê°€ LLM í˜¸ì¶œ ê¸ˆì§€)
        final_answer = extract_langgraph_final_answer(result)
        
        if final_answer:
            # Agentê°€ ì´ë¯¸ ìƒì„±í•œ ë‹µë³€ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            yield final_answer
        else:
            # Agent ê²°ê³¼ê°€ ì—†ì„ ê²½ìš°ì—ë§Œ ê¸°ë³¸ ì •ë³´ í‘œì‹œ
            yield "ğŸ˜• LangGraph Agentê°€ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ìœ¼ë¡œ ì‹œë„í•´ì£¼ì„¸ìš”."
                
    except Exception as e:
        yield f'<div class="process-info">ì‹œìŠ¤í…œ ì˜¤ë¥˜ ë°œìƒ: {str(e)[:200]}...</div>'
        yield f'<div class="process-info">ì§„ë‹¨ ì •ë³´:'
        yield f'<br>- Pipeline ìƒíƒœ: {"ì •ìƒ" if hasattr(pipeline, "agent") else "ë¹„ì •ìƒ"}'
        yield f'<br>- Agent ìƒíƒœ: {"ì •ìƒ" if hasattr(pipeline, "agent") and pipeline.agent else "ë¹„ì •ìƒ"}'
        yield '</div>'
        
        # ê¸´ê¸‰ í´ë°±: ê¸°ë³¸ ì •ë³´ ì œê³µ
        yield '<div class="process-info">ê¸´ê¸‰ ëª¨ë“œ: ì‹œìŠ¤í…œì„ ì¬ì‹œì‘í•˜ê±°ë‚˜ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.</div>'

def enhance_agent_data_for_streaming(agent_data, query):
    """
    Agent ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìš©ìœ¼ë¡œ ì¦ê°• ë° ì¬ê°€ê³µ
    """
    enhanced = agent_data.copy()
    
    # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ì •ë³´ ì¶”ê°€
    search_results = agent_data.get('search_results')
    if search_results:
        total_hits = search_results.get('total_hits', 0)
        documents = search_results.get('documents', [])
        
        enhanced['search_summary'] = {
            'total_found': total_hits,
            'analyzed_count': len(documents),
            'key_brands': list(set([doc.get('make_of_vehicle', 'Unknown') for doc in documents[:10]])),
            'key_models': list(set([doc.get('model_of_vehicle', 'Unknown') for doc in documents[:10]])),
            'problem_types': len(set([doc.get('problem', '') for doc in documents])),
            'sample_issues': [doc.get('problem', '')[:100] for doc in documents[:3]]
        }
    
    # ì§‘ê³„ ê²°ê³¼ ìš”ì•½ ì •ë³´ ì¶”ê°€
    agg_results = agent_data.get('aggregation_results')
    if agg_results:
        buckets = agg_results.get('buckets', [])
        enhanced['aggregation_summary'] = {
            'top_issues_count': len(buckets),
            'total_categories': sum([bucket.get('doc_count', 0) for bucket in buckets]),
            'most_common': buckets[0] if buckets else None
        }
    
    return enhanced

def create_enhanced_streaming_prompt(query, enhanced_data):
    """
    ìŠ¤íŠ¸ë¦¬ë°ìš© ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
    ë” ì²­ì²œí•˜ê³  ê°„ê²°í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° í’ˆì§ˆ í–¥ìƒ
    """
    
    search_summary = enhanced_data.get('search_summary', {})
    agg_summary = enhanced_data.get('aggregation_summary', {})
    
    # ê¸°ë³¸ ì •ë³´
    prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë¶„ì„ ê²°ê³¼ ìš”ì•½:"""
    
    # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
    if search_summary:
        prompt += f"""
ê²€ìƒ‰ ì •ë³´:
- ì´ {search_summary.get('total_found', 0):,}ê±´ì˜ ê´€ë ¨ ë°ì´í„° ë°œê²¬
- ë¶„ì„ ëŒ€ìƒ: {search_summary.get('analyzed_count', 0)}ê±´
- ëŒ€ìƒ ë¸Œëœë“œ: {', '.join(search_summary.get('key_brands', [])[:3])}"""
    
    # ì§‘ê³„ ê²°ê³¼ ìš”ì•½
    if agg_summary:
        prompt += f"""
ì§‘ê³„ ë¶„ì„:
- {agg_summary.get('top_issues_count', 0)}ê°œ ì£¼ìš” ë¬¸ì œ ìœ í˜• ì‹ë³„
- ì´ {agg_summary.get('total_categories', 0):,}ê±´ì˜ ë¶ˆë§Œ ì‚¬ë¡€ ë¶„ì„"""
    
    prompt += f"""

ì‚¬ìš©ìì—ê²Œ ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ì„¤ëª…í•˜ë©°, ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì„ í™œìš©í•´ êµ¬ì¡°ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”."""
    
    return prompt

def stream_with_empty_container_v2(pipeline, query):
    """
    st.empty()ë¥¼ ì‚¬ìš©í•œ ì „ëµ 1 ìŠ¤íŠ¸ë¦¬ë°
    LangGraph Agent ì§„í–‰ìƒí™© + ì‹¤ì œ í† í° ìŠ¤íŠ¸ë¦¬ë°
    """
    streaming_container = st.empty()
    accumulated_response = ""
    
    try:
        log.info("Starting Strategy 1 streaming with st.empty()")
        chunk_count = 0
        
        for chunk in stream_agent_response_v2(pipeline, query):
            if chunk:
                accumulated_response += chunk
                chunk_count += 1
                
                # Streamlit ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ - HTML íƒœê·¸ ì§€ì›
                streaming_container.markdown(accumulated_response, unsafe_allow_html=True)
                
                # ë¶€ë“œëŸ¬ìš´ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ìµœì†Œ ë”˜ë ˆì´
                time.sleep(0.01)
                
                # ì£¼ê¸°ì  ë¡œê¹…
                if chunk_count % 20 == 0:
                    log.debug(f"Strategy 1 streaming progress: {chunk_count} chunks, {len(accumulated_response)} chars")
        
        log.info(f"Strategy 1 streaming completed: {chunk_count} chunks, {len(accumulated_response)} characters")
        return accumulated_response
        
    except Exception as e:
        log.error(f"Strategy 1 st.empty() streaming failed: {e}")
        streaming_container.error(f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return fallback_to_basic_response(pipeline, query)

def stream_with_empty_container_OLD(pipeline, query):
    """
    st.empty()ë¥¼ ì‚¬ìš©í•œ í™•ì‹¤í•œ ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„
    í„°ë¯¸ë„ í…ŒìŠ¤íŠ¸ì—ì„œ ê²€ì¦ëœ H-Chat ìŠ¤íŠ¸ë¦¬ë°ì„ Streamlitì— ì ìš©
    """
    streaming_container = st.empty()
    accumulated_response = ""
    
    try:
        log.info("Starting Streamlit streaming with st.empty()")
        chunk_count = 0
        
        for chunk in stream_agent_response(pipeline, query):
            if chunk:
                accumulated_response += chunk
                chunk_count += 1
                
                # Streamlit ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ (í„°ë¯¸ë„ê³¼ ë™ì¼í•œ ë°©ì‹)
                streaming_container.markdown(accumulated_response, unsafe_allow_html=True)
                
                # ë¶€ë“œëŸ¬ìš´ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ìµœì†Œ ë”œë ˆì´
                time.sleep(0.01)
                
                # ì£¼ê¸°ì  ë¡œê¹… (ë””ë²„ê¹…ìš©)
                if chunk_count % 20 == 0:
                    log.debug(f"Streamlit streaming progress: {chunk_count} chunks, {len(accumulated_response)} chars")
        
        log.info(f"Streamlit streaming completed: {chunk_count} chunks, {len(accumulated_response)} characters")
        return accumulated_response
        
    except Exception as e:
        log.error(f"st.empty() streaming failed: {e}")
        streaming_container.error(f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return fallback_to_basic_response(pipeline, query)

def fallback_to_basic_response(pipeline, query):
    """
    ëª¨ë“  ìŠ¤íŠ¸ë¦¬ë°ì´ ì‹¤íŒ¨í–ˆì„ ë•Œì˜ ìµœì¢… í´ë°±
    """
    try:
        with st.spinner("ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ë‹µë³€ ìƒì„± ì¤‘..."):
            result = pipeline.process_query(query)
            if result['success']:
                agent_data = extract_agent_data(result)
                synthesis_prompt = create_synthesis_prompt(query, agent_data)
                ai_response = pipeline.agent.llm_client.complete(synthesis_prompt)
                st.markdown(ai_response)
                return ai_response
            else:
                error_msg = f"ë¶„ì„ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}"
                st.error(error_msg)
                return error_msg
    except Exception as fallback_error:
        error_msg = f"ëª¨ë“  ë°©ì‹ ì‹¤íŒ¨: {fallback_error}"
        st.error(error_msg)
        return error_msg

def stream_hchat_response(llm_client, prompt, enhanced_data):
    """
    í„°ë¯¸ë„ í…ŒìŠ¤íŠ¸ì—ì„œ ê²€ì¦ëœ H-Chat ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„
    í† í° ë‹¨ìœ„ë¡œ ì„¸ë°€í•œ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (2.4ì/ì²­í¬ ìˆ˜ì¤€)
    """
    try:
        # í„°ë¯¸ë„ í…ŒìŠ¤íŠ¸ì—ì„œ í™•ì¸ëœ ìŠ¤íŠ¸ë¦¬ë° ì§€ì› ì²´í¬
        if hasattr(llm_client, 'stream_complete'):
            log.info("Starting H-Chat real-time streaming (verified working)")
            
            accumulated_response = ""
            chunk_count = 0
            empty_chunks = 0
            
            # í„°ë¯¸ë„ê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘
            for chunk in llm_client.stream_complete(prompt):
                if chunk:  # ë¹ˆ ì²­í¬ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ ì²˜ë¦¬
                    accumulated_response += chunk
                    chunk_count += 1
                    yield chunk  # í„°ë¯¸ë„ê³¼ ë™ì¼í•˜ê²Œ ë°”ë¡œ yield
                else:
                    empty_chunks += 1
            
            # í„°ë¯¸ë„ í…ŒìŠ¤íŠ¸ì™€ ë¹„ìŠ·í•œ ë¡œê·¸ ì¶œë ¥
            log.info(f"H-Chat streaming completed: {len(accumulated_response)} chars in {chunk_count} chunks (avg: {len(accumulated_response)/max(chunk_count,1):.1f} chars/chunk)")
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ ê°ì§€ (í„°ë¯¸ë„ í…ŒìŠ¤íŠ¸ì—ì„œëŠ” 165ì ì„±ê³µ)
            if len(accumulated_response.strip()) < 10:
                log.warning(f"H-Chat streaming returned too short response ({len(accumulated_response)} chars), falling back")
                yield from _fallback_simulated_streaming(llm_client, prompt)
                
        else:
            log.warning("H-Chat stream_complete method not found, using fallback")
            yield from _fallback_simulated_streaming(llm_client, prompt)
            
    except Exception as e:
        log.error(f"H-Chat streaming error: {e}")
        yield f"\n\nâš ï¸ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜ ë°œìƒ, ì¼ë°˜ ëª¨ë“œë¡œ ì „í™˜ ì¤‘...\n\n"
        yield from _fallback_simulated_streaming(llm_client, prompt)

def _fallback_simulated_streaming(llm_client, prompt):
    """
    H-Chat ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ ì‹œ ì‹œë®¬ë ˆì´ì…˜ í´ë°±
    í„°ë¯¸ë„ í…ŒìŠ¤íŠ¸ì—ì„œ ê²€ì¦ëœ ë°©ì‹ì„ ì‚¬ìš©
    """
    try:
        log.info("Using simulated streaming fallback (like terminal test)")
        
        # í„°ë¯¸ë„ í…ŒìŠ¤íŠ¸ì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì¼ë°˜ ì™„ë£Œ í˜¸ì¶œ
        response = llm_client.complete(prompt)
        
        if response and response.strip():
            log.info(f"Simulated streaming: {len(response)} chars to stream")
            
            # í„°ë¯¸ë„ì—ì„œ ì‚¬ìš©í•œ ë°©ì‹: ë” ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ë³„ ìŠ¤íŠ¸ë¦¬ë°
            sentences = response.split('. ')
            
            for i, sentence in enumerate(sentences):
                if sentence.strip():
                    # ë§ˆì§€ë§‰ ë¬¸ì¥ì´ ì•„ë‹ˆë©´ ë§ˆì¹¨í‘œ ì¶”ê°€ (í„°ë¯¸ë„ê³¼ ë™ì¼)
                    if i < len(sentences) - 1:
                        yield sentence + '. '
                    else:
                        yield sentence
                    time.sleep(0.08)  # í„°ë¯¸ë„ í…ŒìŠ¤íŠ¸ì™€ ë™ì¼í•œ ì†ë„
            
            log.info("Simulated streaming completed successfully")
        else:
            log.warning("LLM returned empty response for simulated streaming")
            yield "ğŸ˜• ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
            
    except Exception as e:
        log.error(f"Simulated streaming fallback failed: {e}")
        yield f"ğŸ˜• ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)[:100]}..."

def _yield_raw_data(agent_data):
    """ì›ì‹œ ë°ì´í„°ë¥¼ ì‚¬ìš©ì ì¹œí™”ì ìœ¼ë¡œ í‘œì‹œí•˜ëŠ” ìƒì„±ì"""
    search_results = agent_data.get('search_results')
    aggregation_results = agent_data.get('aggregation_results')
    
    if search_results and search_results.get('total_hits', 0) > 0:
        yield f"\n\n### ê²€ìƒ‰ ê²°ê³¼\n\nì´ {search_results['total_hits']:,}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n\n"
        
        documents = search_results.get('documents', [])[:3]
        if documents:
            yield "ì£¼ìš” ì‚¬ë¡€:\n\n"
            for i, doc in enumerate(documents, 1):
                model = doc.get('model_of_vehicle', 'N/A')
                problem = doc.get('problem', 'N/A')[:80] + '...' if len(doc.get('problem', '')) > 80 else doc.get('problem', 'N/A')
                brand = doc.get('make_of_vehicle', 'N/A')
                yield f"{i}. {brand} {model}: {problem}\n\n"
    
    if aggregation_results and aggregation_results.get('buckets'):
        yield "### ì§‘ê³„ ë¶„ì„ ê²°ê³¼\n\n"
        buckets = aggregation_results['buckets'][:5]
        for bucket in buckets:
            key = bucket.get('key', 'N/A')
            count = bucket.get('doc_count', 0)
            yield f"- {key}: {count:,}ê±´\n"
        yield "\n"
    
    if not search_results and not aggregation_results:
        yield "\n\në¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”."

def stream_agent_response_v2(pipeline, query):
    """
    ì „ëµ 1 ì ìš©: LangGraph Agent ìŠ¤íŠ¸ë¦¬ë°
    1. Agent ì§„í–‰ìƒí™© í‘œì‹œ (15ì´ˆ)
    2. Synthesizer ë‹¨ê³„ì—ì„œ ì‹¤ì œ í† í° ìŠ¤íŠ¸ë¦¬ë° (15ì´ˆ~)
    3. ì¶”ê°€ LLM í˜¸ì¶œ ì™„ì „ ê¸ˆì§€
    """
    try:
        # ì´ˆê¸° ì•Œë¦¼ - HTML íƒœê·¸ ì œê±°
        yield 'LangGraph Agent ì‹œì‘ - 68,982ê±´ ë°ì´í„° ë¶„ì„ ì¤‘...\n\n'
        
        # ì „ëµ 1: Agent ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
        progress_stage = "prep"  # prep -> processing -> synthesis
        
        for chunk in pipeline.agent.process_query_streaming(query):
            if "LangGraph Agent ì‹œì‘" in chunk or "ë‹¨ê³„ ì™„ë£Œ" in chunk:
                progress_stage = "processing"
                yield chunk + '\n'  # HTML íƒœê·¸ ì œê±°
            elif "Synthesis ë‹¨ê³„ ì‹œì‘" in chunk:
                progress_stage = "synthesis"
                yield 'ìµœì¢… ë‹µë³€ ìƒì„± ì‹œì‘...\n\n'  # HTML íƒœê·¸ ì œê±°
            elif progress_stage == "synthesis":
                # ì‹¤ì œ í† í° ìŠ¤íŠ¸ë¦¬ë°
                if chunk and chunk.strip():
                    yield chunk
            else:
                # ê¸°íƒ€ ì§„í–‰ìƒí™© - HTML íƒœê·¸ ì œê±°
                yield chunk + '\n'
                
    except Exception as e:
        log.error(f"Agent streaming failed: {e}")
        yield f'<div class="process-info">ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {str(e)}</div>\n\n'
        
        # í´ë°±: ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì‹¤í–‰
        try:
            result = pipeline.process_query(query)
            if result['success']:
                final_answer = extract_langgraph_final_answer(result)
                if final_answer:
                    # ì‹œë®¬ë ˆì´ì…˜ ìŠ¤íŠ¸ë¦¬ë°
                    sentences = final_answer.split('. ')
                    for i, sentence in enumerate(sentences):
                        if sentence.strip():
                            if i < len(sentences) - 1:
                                yield sentence + '. '
                            else:
                                yield sentence
                            time.sleep(0.05)
                else:
                    yield "Agent ê²°ê³¼ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            else:
                yield f"ì˜¤ë¥˜: {result.get('error', 'Unknown error')}"
                
        except Exception as fallback_error:
            yield f"í´ë°± ì‹¤í–‰ ì˜¤ë¥˜: {str(fallback_error)}"

def extract_langgraph_final_answer(result):
    """
    LangGraph Agentì˜ SynthesizerTool ìµœì¢… ê²°ê³¼ë§Œ ì¶”ì¶œ
    ì¶”ê°€ LLM í˜¸ì¶œ ì—†ì´ Agentê°€ ìƒì„±í•œ ë‹µë³€ë§Œ ì‚¬ìš©
    """
    try:
        # LangGraph Agent ê²°ê³¼ êµ¬ì¡° ë¶„ì„
        result_data = result.get('result', {})
        
        # Pipelineì—ì„œ enhancedëœ ê²°ê³¼ ì²˜ë¦¬
        if isinstance(result_data, dict):
            # 1ìˆœìœ„: Agentì˜ final_synthesis ì¶”ì¶œ
            if 'agent_result' in result_data:
                agent_result = result_data['agent_result']
                if isinstance(agent_result, dict) and 'final_synthesis' in agent_result:
                    synthesis_result = agent_result['final_synthesis']
                    if isinstance(synthesis_result, dict) and 'synthesis' in synthesis_result:
                        return synthesis_result['synthesis']  # SynthesizerToolì´ ìƒì„±í•œ ë‹µë³€
            
            # 2ìˆœìœ„: final_answer ì²˜ë¦¬ (ì´ë¯¸ Agentê°€ ì²˜ë¦¬í•œ ê²°ê³¼)
            elif 'final_answer' in result_data:
                final_answer = result_data['final_answer']
                if isinstance(final_answer, dict) and 'synthesis' in final_answer:
                    return final_answer['synthesis']
                elif isinstance(final_answer, str):
                    return final_answer
        
        log.warning("No final synthesis found in LangGraph Agent result")
        return None
        
    except Exception as e:
        log.error(f"Failed to extract LangGraph final answer: {e}")
        return None

def extract_agent_data(result):
    """ì—ì´ì „íŠ¸ ê²°ê³¼ì—ì„œ ì‹¤ì œ ë°ì´í„° ì¶”ì¶œ"""
    result_data = result.get('result', {})
    
    extracted = {
        "query": result.get('query', ''),
        "execution_time": result.get('execution_time', 0),
        "tools_used": result.get('agent_info', {}).get('tools_used', [])
    }
    
    # ê²€ìƒ‰ ê²°ê³¼ ì¶”ì¶œ
    if 'final_answer' in result_data:
        final_answer = result_data['final_answer']
        if isinstance(final_answer, dict) and 'agent_result' in final_answer:
            agent_result = final_answer['agent_result']
            extracted['search_results'] = agent_result.get('search_results')
            extracted['aggregation_results'] = agent_result.get('aggregation_results')
    
    return extracted

def create_synthesis_prompt(query, agent_data):
    """ì—ì´ì „íŠ¸ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ LLMìš© ì¢…í•© í”„ë¡¬í”„íŠ¸ ìƒì„± - ë§ˆí¬ë‹¤ìš´ í’ˆì§ˆ í–¥ìƒ"""
    
    search_results = agent_data.get('search_results')
    aggregation_results = agent_data.get('aggregation_results')
    
    prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸: {query}

ì‹œìŠ¤í…œì´ ë¶„ì„í•œ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.

ë¶„ì„ ì •ë³´:
- ì‹¤í–‰ ì‹œê°„: {agent_data.get('execution_time', 0):.2f}ì´ˆ
- ì‚¬ìš©ëœ ë„êµ¬: {', '.join(agent_data.get('tools_used', []))}

"""
    
    # ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
    if search_results and isinstance(search_results, dict):
        total_hits = search_results.get('total_hits', 0)
        documents = search_results.get('documents', [])
        
        prompt += f"ê²€ìƒ‰ ê²°ê³¼: {total_hits:,}ê°œ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬\n"
        
        if documents:
            prompt += "ì£¼ìš” ì‚¬ë¡€:\n"
            for i, doc in enumerate(documents[:3], 1):
                model = doc.get('model_of_vehicle', 'N/A')
                problem = doc.get('problem', 'N/A')
                prompt += f"{i}. {model}: {problem}\n"
    
    # ì§‘ê³„ ê²°ê³¼ ì¶”ê°€
    if aggregation_results and isinstance(aggregation_results, dict):
        buckets = aggregation_results.get('buckets', [])
        if buckets:
            prompt += "\nì§‘ê³„ ë¶„ì„ ê²°ê³¼:\n"
            for bucket in buckets[:5]:
                key = bucket.get('key', 'N/A')
                count = bucket.get('doc_count', 0)
                prompt += f"- {key}: {count:,}ê±´\n"
    
    prompt += f"""

ìœ„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.

ë¶„ì„ ìš”êµ¬ì‚¬í•­:
- ë°ì´í„°ì— ê¸°ë°˜í•œ ì •í™•í•œ ì •ë³´ ì œê³µ
- í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…
- í•„ìš”ì‹œ ê°œì„  ë°©ì•ˆì´ë‚˜ ì¶”ê°€ ë¶„ì„ ì œì•ˆ

ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì¤€ìˆ˜ì‚¬í•­ (í•„ìˆ˜):
â€¢ í—¤ë”(###) ì•ì—ëŠ” ë°˜ë“œì‹œ ë‘ ì¤„ ê°œí–‰ì„ ë„£ìœ¼ì„¸ìš”
  ì˜ˆì‹œ: "ë¶„ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.\n\n### ì£¼ìš” ë°œê²¬ì‚¬í•­"
â€¢ ë¦¬ìŠ¤íŠ¸(1., 2., 3.) ì•ì—ëŠ” ë°˜ë“œì‹œ ë‘ ì¤„ ê°œí–‰ì„ ë„£ìœ¼ì„¸ìš”
  ì˜ˆì‹œ: "ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\n1. ì²« ë²ˆì§¸ í•­ëª©"
â€¢ ì†Œì œëª©(####) ì•ì—ë„ ë‘ ì¤„ ê°œí–‰ì„ ë„£ìœ¼ì„¸ìš”
  ì˜ˆì‹œ: "ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n\n#### ìƒì„¸ ë‚´ìš©"
â€¢ ê° ì„¹ì…˜ ì‚¬ì´ì—ëŠ” ì ì ˆí•œ ê°„ê²©ì„ ìœ ì§€í•˜ì„¸ìš”
â€¢ í‘œë‚˜ ì½”ë“œ ë¸”ë¡ ì•ì—ë„ ë‘ ì¤„ ê°œí–‰ì„ ë„£ìœ¼ì„¸ìš”

ì˜¬ë°”ë¥¸ í˜•ì‹ ì˜ˆì‹œ:
```
ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.

### ì£¼ìš” ë°œê²¬ì‚¬í•­

ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì œë“¤ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.

1. INFO12 ì½”ë“œ ê´€ë ¨ ë¬¸ì œ
2. INFO13 ì½”ë“œ ê´€ë ¨ ë¬¸ì œ

#### ìƒì„¸ ë¶„ì„

ê° í•­ëª©ì˜ ì„¸ë¶€ ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
```

ìœ„ ê²°ê³¼ì™€ í˜•ì‹ ì§€ì¹¨ì„ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
"""
    
    return prompt

def extract_iqs_related_data(query, pipeline_result=None):
    """ê´€ë ¨ ë°ì´í„° ì¶”ì¶œ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
    try:
        # ì‹¤ì œ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê´€ë ¨ ë°ì´í„° ì¶”ì¶œ
        if pipeline_result and 'result' in pipeline_result:
            result_data = pipeline_result['result']
            
            # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê´€ë ¨ ì°¨ì¢…/ë¬¸ì œ ì •ë³´ ì¶”ì¶œ
            related_data = []
            
            if 'final_answer' in result_data:
                final_answer = result_data['final_answer']
                
                # agent_resultì—ì„œ ì‹¤ì œ ë°ì´í„° ì¶”ì¶œ ì‹œë„
                if isinstance(final_answer, dict) and 'agent_result' in final_answer:
                    agent_result = final_answer['agent_result']
                    
                    # ê²€ìƒ‰ ê²°ê³¼ ì¶”ì¶œ
                    if 'search_results' in agent_result:
                        search_results = agent_result['search_results']
                        if 'documents' in search_results:
                            documents = search_results['documents'][:5]  # ìƒìœ„ 5ê°œë§Œ
                            for doc in documents:
                                related_data.append({
                                    "ì°¨ì¢…": doc.get('model_of_vehicle', 'N/A'),
                                    "ëª¨ë¸ì—°ë„": doc.get('model_year', 'N/A'),
                                    "ë¬¸ì œì½”ë“œ": doc.get('problem', 'N/A')[:50] + '...' if len(str(doc.get('problem', ''))) > 50 else doc.get('problem', 'N/A'),
                                    "ë¸Œëœë“œ": doc.get('make_of_vehicle', 'N/A')
                                })
                    
                    # ì§‘ê³„ ê²°ê³¼ ì¶”ì¶œ
                    elif 'aggregation_results' in agent_result:
                        agg_results = agent_result['aggregation_results']
                        if 'buckets' in agg_results:
                            buckets = agg_results['buckets'][:5]  # ìƒìœ„ 5ê°œë§Œ
                            for bucket in buckets:
                                related_data.append({
                                    "ë¬¸ì œìœ í˜•": bucket.get('key', 'N/A'),
                                    "ë¶ˆë§Œê±´ìˆ˜": bucket.get('doc_count', 'N/A'),
                                    "ë¹„ì¤‘": f"{bucket.get('doc_count', 0) / agg_results.get('total_docs', 1) * 100:.1f}%" if 'total_docs' in agg_results else 'N/A'
                                })
            
            return related_data
    except Exception as e:
        st.error(f"ê´€ë ¨ ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return []
    
    # ê¸°ë³¸ ì˜ˆì‹œ ë°ì´í„°
    return [
        {
            "ì°¨ì¢…": "Hyundai Santa Fe 4dr SUV",
            "ëª¨ë¸ì—°ë„": 2025,
            "ë¬¸ì œì½”ë“œ": "INFO12: Built-in navigation - Broken/works inconsistently",
            "ë¸Œëœë“œ": "Hyundai"
        }
    ]

def generate_iqs_recommendations(query, response_text):
    """IQS íŠ¹í™” ì¶”ì²œ ì§ˆë¬¸ ìƒì„±"""
    try:
        # ì¿¼ë¦¬ ë¶„ì„ ê¸°ë°˜ ì¶”ì²œ ì§ˆë¬¸ ìƒì„±
        recommendations = []
        
        query_lower = query.lower()
        
        # ë¸Œëœë“œ ê´€ë ¨ ì§ˆë¬¸ì´ë©´ ë¸Œëœë“œ ë¹„êµ ì¶”ì²œ
        if any(brand in query_lower for brand in ['í˜„ëŒ€', 'hyundai', 'ê¸°ì•„', 'kia']):
            recommendations.append("ì´ ë¬¸ì œì—ì„œ í˜„ëŒ€ì™€ ê¸°ì•„ ë¸Œëœë“œ ê°„ ì°¨ì´ì ì„ ë¹„êµí•´ì¤˜")
        
        # ì°¨ì¢… ê´€ë ¨ ì§ˆë¬¸ì´ë©´ ë‹¤ë¥¸ ì°¨ì¢… ì¶”ì²œ
        if any(model in query_lower for model in ['ì‹¼íƒ€í˜', 'santa fe', 'íŒ°ë¦¬ì„¸ì´ë“œ', 'palisade']):
            recommendations.append("ë™ì¼ ë¬¸ì œì—ì„œ ë‹¤ë¥¸ ì°¨ì¢…ë“¤ì˜ í˜„í™©ì€ ì–´ë–¤ì§€ ì•Œë ¤ì¤˜")
        
        # INFO ì½”ë“œ ê´€ë ¨ì´ë©´ ê´€ë ¨ ì½”ë“œ ì¶”ì²œ
        if 'info' in query_lower:
            recommendations.append("INFO ê³„ì—´ì˜ ë‹¤ë¥¸ ë¬¸ì œ ì½”ë“œë“¤ê³¼ì˜ ì—°ê´€ì„±ì„ ë¶„ì„í•´ì¤˜")
        
        # ì—°ë„ ê´€ë ¨ì´ë©´ íŠ¸ë Œë“œ ì¶”ì²œ
        if any(year in query for year in ['2023', '2024', '2025', '23MY', '24MY', '25MY']):
            recommendations.append("ì´ ë¬¸ì œì˜ ìµœê·¼ 3ë…„ê°„ ë³€í™” ì¶”ì´ë¥¼ ë¶„ì„í•´ì¤˜")
        
        # ê¸°ë³¸ ì¶”ì²œ ì§ˆë¬¸ë“¤
        if len(recommendations) < 3:
            basic_recommendations = [
                "ì´ ë¶„ì„ ê²°ê³¼ì—ì„œ ê°€ì¥ ì‹¬ê°í•œ ë¬¸ì œì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "ê°œì„ ì´ í•„ìš”í•œ ìš°ì„ ìˆœìœ„ ì˜ì—­ì„ ì•Œë ¤ì£¼ì„¸ìš”",
                "ê²½ìŸì‚¬ ëŒ€ë¹„ ìš°ë¦¬ ë¸Œëœë“œì˜ ê°•ì•½ì ì„ ë¶„ì„í•´ì£¼ì„¸ìš”",
                "ê³ ê° ë¶ˆë§Œì„ ì¤„ì´ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ ê°œì„  ë°©ì•ˆì€?",
                "ì´ì™€ ìœ ì‚¬í•œ ë‹¤ë¥¸ í’ˆì§ˆ ë¬¸ì œë“¤ë„ ìˆë‚˜ìš”?"
            ]
            
            for rec in basic_recommendations:
                if len(recommendations) >= 3:
                    break
                if rec not in recommendations:
                    recommendations.append(rec)
        
        return recommendations[:3]  # ìµœëŒ€ 3ê°œ
        
    except Exception as e:
        # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ì¶”ì²œ ì§ˆë¬¸ ë°˜í™˜
        return [
            "ì´ ë¶„ì„ ê²°ê³¼ì—ì„œ ê°€ì¥ ì‹¬ê°í•œ ë¬¸ì œì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ê°œì„ ì´ í•„ìš”í•œ ìš°ì„ ìˆœìœ„ ì˜ì—­ì„ ì•Œë ¤ì£¼ì„¸ìš”",
            "ê²½ìŸì‚¬ ëŒ€ë¹„ ìš°ë¦¬ ë¸Œëœë“œì˜ ê°•ì•½ì ì„ ë¶„ì„í•´ì£¼ì„¸ìš”"
        ]

def save_feedback_log(query, response, rating, feedback):
    """í”¼ë“œë°± ì €ì¥"""
    try:
        log_data = {
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "user_query": query,
            "ai_response": response,
            "rating": str(rating) if rating else "",
            "feedback": str(feedback) if feedback else ""
        }
        
        log_df = pd.DataFrame([log_data])
        log_path = Path("logs/iqs_feedback.csv")
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        log_path.parent.mkdir(parents=True, exist_ok=True)
        
        if log_path.exists():
            existing_df = pd.read_csv(log_path)
            updated_df = pd.concat([existing_df, log_df], ignore_index=True)
        else:
            updated_df = log_df
        
        updated_df.to_csv(log_path, index=False, encoding='utf-8-sig')
        return True
    except Exception as e:
        st.error(f"í”¼ë“œë°± ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

def show_chat_history(chat_history):
    """ì±„íŒ… ê¸°ë¡ í‘œì‹œ"""
    for message in chat_history:
        if isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        elif isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)

def add_search_history(query, timestamp=None):
    """ê²€ìƒ‰ ê¸°ë¡ ì¶”ê°€"""
    if timestamp is None:
        timestamp = datetime.now()
    
    # ì¤‘ë³µ ë°©ì§€
    if query in [h.get('query') for h in st.session_state.search_history]:
        return
    
    # ìµœëŒ€ 10ê°œë§Œ ìœ ì§€
    if len(st.session_state.search_history) >= 10:
        st.session_state.search_history.pop(0)
    
    st.session_state.search_history.append({
        'query': query,
        'timestamp': timestamp.strftime("%Y-%m-%d %H:%M"),
        'short_query': query[:30] + '...' if len(query) > 30 else query
    })

def show_sidebar():
    """ì‚¬ì´ë“œë°” í‘œì‹œ - ëŒ€í™” ì´ˆê¸°í™”, ë°ì´í„° ì •ë³´, ê²€ìƒ‰ ê¸°ë¡"""
    with st.sidebar:
        # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼ (ìƒë‹¨)
        if st.button("ğŸ” IQS-Search", key="sidebar_reset_button", help="ìƒˆë¡œìš´ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤", use_container_width=True):
            # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
            st.session_state.chat_history = []
            if "analysis_result" in st.session_state:
                del st.session_state["analysis_result"]
            if "related_data" in st.session_state:
                del st.session_state["related_data"]
            if "recommendations" in st.session_state:
                del st.session_state["recommendations"]
            if "current_query" in st.session_state:
                del st.session_state["current_query"]
            if "pipeline_result" in st.session_state:
                del st.session_state["pipeline_result"]
            st.rerun()
        
        st.divider()
        
        # ë°ì´í„° ì •ë³´
        st.markdown("### ë°ì´í„° ì •ë³´")
        
        # ê¸°ë³¸ ë°ì´í„° í†µê³„
        col1, col2 = st.columns(2)
        with col1:
            st.metric("ì´ ë°ì´í„°", "68,982ê±´")
            st.metric("ëŒ€ìƒ ì—°ë„", "23-25MY")
        with col2:
            st.metric("ë¸Œëœë“œ", "3ê°œ")
            st.metric("ì°¨ì¢…", "49ê°œ")
        
        # ìƒì„¸ ì •ë³´
        with st.expander("ìƒì„¸ ì •ë³´", expanded=False):
            st.markdown("""
            **ë¸Œëœë“œ**: í˜„ëŒ€, ê¸°ì•„, ì œë„¤ì‹œìŠ¤  
            **ì°¨ì¢…**: SUV, ì„¸ë‹¨, í•´ì¹˜ë°± ë“± 49ê°œ  
            **ë¬¸ì œ ìœ í˜•**: INFO, FCD, PWR ë“± 223ê°œ  
            **ë°ì´í„° ê¸°ê°„**: 2023-2025 ëª¨ë¸ì´ì–´  
            **ì¶œì²˜**: J.D.Power Initial Quality Study
            """)
        
        st.divider()
        
        # ê²€ìƒ‰ ê¸°ë¡
        st.markdown("### ê²€ìƒ‰ ê¸°ë¡")
        
        if st.session_state.search_history:
            # ê¸°ë¡ ì‚­ì œ ë²„íŠ¼
            col1, col2 = st.columns([3, 1])
            with col2:
                if st.button("ì‚­ì œ", key="clear_history"):
                    st.session_state.search_history = []
                    st.rerun()
            
            # ê¸°ë¡ í‘œì‹œ (ìµœê·¼ 10ê°œ)
            for i, history_item in enumerate(reversed(st.session_state.search_history)):
                with st.container():
                    # ê°„ëµí•œ í‘œì‹œ
                    if st.button(
                        f"{history_item['short_query']}", 
                        key=f"history_{i}",
                        help=f"{history_item['timestamp']} - {history_item['query']}",
                        use_container_width=True
                    ):
                        st.session_state["pending_user_query"] = history_item['query']
                        st.rerun()
                    
                    # ì‹œê°„ í‘œì‹œ
                    st.caption(history_item['timestamp'])
        else:
            st.info("ê²€ìƒ‰ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")


def restart_streamlit():
    """Streamlit ì•± ì¬ì‹œì‘"""
    st.session_state.clear()
    st.markdown(
        """
        <meta http-equiv="refresh" content="0;url=/" />
        """,
        unsafe_allow_html=True
    )

# CSS ìŠ¤íƒ€ì¼ë§ ì¶”ê°€
def apply_custom_css():
    st.markdown("""
    <style>
    /* ë©”ì¸ íƒ€ì´í‹€ ìŠ¤íƒ€ì¼ë§ */
    .main-title {
        font-size: 2.5rem;
        font-weight: 600;
        margin-bottom: 0.5rem;
        color: #1f2937;
    }
    
    .subtitle {
        font-size: 0.95rem;
        color: #6b7280;
        margin-bottom: 2rem;
        line-height: 1.4;
    }
    
    /* ì‚¬ì´ë“œë°” IQS-Search ë²„íŠ¼ ìŠ¤íƒ€ì¼ë§ */
    button[key="sidebar_reset_button"] {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%) !important;
        border: none !important;
        border-radius: 6px !important;
        color: white !important;
        font-weight: 500 !important;
        font-size: 13px !important;
        padding: 0.4rem 0.8rem !important;
        transition: all 0.2s ease !important;
        margin-bottom: 0.5rem !important;
    }
    
    button[key="sidebar_reset_button"]:hover {
        background: linear-gradient(135deg, #5a6fd8 0%, #6a4c93 100%) !important;
        transform: translateY(-1px) !important;
    }
    
    /* í”„ë¡œì„¸ìŠ¤ ì •ë³´ ìŠ¤íƒ€ì¼ë§ (ë” ì‘ê³  ì—°í•˜ê²Œ + ê°„ê²© ì¤„ì´ê¸°) */
    .process-info {
        font-size: 0.75rem !important;
        color: #9ca3af !important;
        line-height: 1.2 !important;
        margin: 0.1rem 0 !important;
        padding: 0.05rem 0 !important;
        font-weight: 400 !important;
    }
    
    .process-info p {
        margin: 0.1rem 0 !important;
        padding: 0 !important;
    }
    
    /* ì‘ì€ ë¡œë”© ìŠ¤í”¼ë„ˆ (ì¸ë¼ì¸ ìŠ¤íƒ€ì¼) */
    .small-spinner {
        display: inline-flex;
        align-items: center;
        font-size: 0.75rem;
        color: #6b7280;
        margin-right: 8px;
    }
    
    .small-spinner .spinner {
        width: 14px;
        height: 14px;
        border: 1.5px solid #f3f4f6;
        border-top: 1.5px solid #3b82f6;
        border-radius: 50%;
        animation: spin 1s linear infinite;
        margin-right: 6px;
    }
    
    @keyframes spin {
        0% { transform: rotate(0deg); }
        100% { transform: rotate(360deg); }
    }
    
    /* ë¡œë”© ì¤‘ ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ë§ */
    .loading-message {
        font-size: 0.8rem;
        color: #6b7280;
        font-style: italic;
        display: flex;
        align-items: center;
        margin: 0.3rem 0;
    }
    
    .loading-message .loading-dots {
        margin-left: 8px;
    }
    
    /* ì±„íŒ… ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ë§ */
    .stChatMessage {
        padding: 1rem;
        margin-bottom: 1rem;
        border-radius: 8px;
    }
    
    /* ì‚¬ì´ë“œë°” ìŠ¤íƒ€ì¼ë§ */
    .sidebar .element-container {
        margin-bottom: 1rem;
    }
    
    /* ë°ì´í„° í…Œì´ë¸” ìŠ¤íƒ€ì¼ë§ */
    .dataframe {
        font-size: 0.9rem;
    }
    
    /* ë²„íŠ¼ ìŠ¤íƒ€ì¼ë§ */
    .stButton > button {
        width: 100%;
        border-radius: 6px;
        border: 1px solid #e5e7eb;
        transition: all 0.2s;
    }
    
    .stButton > button:hover {
        border-color: #3b82f6;
        color: #3b82f6;
    }
    </style>
    """, unsafe_allow_html=True)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    st.set_page_config(
        page_title="IQS Quality Data Analytics", 
        page_icon="ğŸš—",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # CSS ìŠ¤íƒ€ì¼ë§ ì ìš©
    apply_custom_css()
    
    # ë©”ì¸ íƒ€ì´í‹€ ë° ì„¤ëª…
    st.markdown("<h1 class='main-title'>IQS(Initial Quality Study) í’ˆì§ˆ AIì–´ì‹œìŠ¤í„´íŠ¸</h1>", unsafe_allow_html=True)
    st.markdown("<div class='subtitle'>J.D.Power 23~25ë…„ ì‹ ì°¨í’ˆì§ˆì¡°ì‚¬ ê²°ê³¼ë°ì´í„°(IQS)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.</div>", unsafe_allow_html=True)
    
    # ì‚¬ì´ë“œë°” í‘œì‹œ
    show_sidebar()
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    pipeline = initialize_iqs_system()
    if pipeline is None:
        st.stop()
    
    # ëŒ€í™” ì´ˆê¸°í™”ëŠ” ìš°ìƒë‹¨ AI ì•„ì´ì½˜ìœ¼ë¡œ ëŒ€ì²´ë¨
    
    # ëŒ€í™” ì´ˆê¸°í™”ëŠ” ìš°ìƒë‹¨ AI ì•„ì´ì½˜ìœ¼ë¡œ ëŒ€ì²´ë¨
    
    # ì±„íŒ… ì…ë ¥
    user_query = st.chat_input("IQS í’ˆì§ˆ ë°ì´í„° ë¶„ì„ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
    
    # ì§ˆë¬¸ ì²˜ë¦¬
    if user_query:
        st.session_state["pending_user_query"] = user_query
    
    # ì‚¬ìš©ì ì§ˆë¬¸ì´ ì…ë ¥ë˜ë©´ ë°”ë¡œ ì±„íŒ…ì°½ì— í‘œì‹œ
    if "pending_user2user_display" not in st.session_state and "pending_user_query" in st.session_state:
        query = st.session_state["pending_user_query"]
        with st.chat_message("user"):
            st.markdown(query)
        st.session_state["pending_user2user_display"] = True
    
    # LangGraph Agent ì‹¤í–‰
    if "pending_user_query" in st.session_state:
        query = st.session_state["pending_user_query"]
        
        # ê°œì„ ëœ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±
        with st.chat_message("assistant"):
            try:
                # Streamlit ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ 1: st.write_stream ì‹œë„
                if hasattr(st, 'write_stream'):
                    ai_response = ""
                    try:
                        def response_generator():
                            nonlocal ai_response
                            for chunk in stream_agent_response_v2(pipeline, query):
                                ai_response += chunk
                                yield chunk
                        
                        st.write_stream(response_generator())
                        
                    except Exception as write_stream_error:
                        log.warning(f"st.write_stream failed: {write_stream_error}, falling back to st.empty()")
                    # st.empty() ë°©ì‹ìœ¼ë¡œ í´ë°±
                    ai_response = stream_with_empty_container_v2(pipeline, query)
                else:
                    # st.empty() ë°©ì‹ ì‚¬ìš©
                    ai_response = stream_with_empty_container_v2(pipeline, query)
                
                # ë‹µë³€ ìœ íš¨ì„± ê²€ì‚¬
                if not ai_response or len(str(ai_response).strip()) < 10:
                    st.warning("ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                    ai_response = "ë‹µë³€ ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                    
            except Exception as stream_error:
                log.error(f"Streaming error: {stream_error}")
                st.error(f"ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {stream_error}")
                
                # ìµœì¢… í´ë°±: ê¸°ë³¸ ë°©ì‹
                ai_response = fallback_to_basic_response(pipeline, query)
        
        # ê´€ë ¨ ë°ì´í„° ë° ì¶”ì²œ ì§ˆë¬¸ ìƒì„± (ì‘ì€ ìŠ¤í”¼ë„ˆ ì‚¬ìš©)
        loading_container = st.empty()
        loading_container.markdown('<div class="small-spinner"><div class="spinner"></div>ê´€ë ¨ ì •ë³´ë¥¼ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤...</div>', unsafe_allow_html=True)
        
        pipeline_result = getattr(st.session_state.get('iqs_pipeline', {}), '_last_result', None)
        related_data = extract_iqs_related_data(query, pipeline_result)
        recommendations = generate_iqs_recommendations(query, ai_response)
        
        # ë¡œë”© ìŠ¤í”¼ë„ˆ ì œê±°
        loading_container.empty()
        
        # ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€
        st.session_state.chat_history.append(HumanMessage(query))
        st.session_state.chat_history.append(AIMessage(ai_response))
        
        # ê²€ìƒ‰ ê¸°ë¡ì— ì¶”ê°€
        add_search_history(query)
        
        # ì„¸ì…˜ì— ì €ì¥
        st.session_state["analysis_result"] = ai_response
        st.session_state["related_data"] = related_data
        st.session_state["recommendations"] = recommendations
        st.session_state["current_query"] = query
        
        # pending_user_query ì‚­ì œ
        del st.session_state["pending_user_query"]
        if "pending_user2user_display" in st.session_state:
            del st.session_state["pending_user2user_display"]
        
        st.rerun()
    
    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    show_chat_history(st.session_state.chat_history)
    
    # ê´€ë ¨ ë°ì´í„° í‘œì‹œ (ê°„ì†Œí™”)
    if "related_data" in st.session_state and st.session_state["related_data"]:
        st.markdown("### ê´€ë ¨ ë°ì´í„°")
        df = pd.DataFrame(st.session_state["related_data"])
        st.dataframe(df, use_container_width=True, hide_index=True)
    
    # ì¶”ì²œ ì§ˆë¬¸ í‘œì‹œ (ê°„ì†Œí™”)
    if "recommendations" in st.session_state and st.session_state["recommendations"]:
        st.markdown("### ì¶”ì²œ ì§ˆë¬¸")
        questions = st.session_state["recommendations"]
        for i, question in enumerate(questions):
            if st.button(question, key=f"rec_q_{i}"):
                st.session_state["pending_user_query"] = question
                st.rerun()
    
    # í”¼ë“œë°± ì²˜ë¦¬ (ì„ íƒì  í‘œì‹œ)
    if "current_query" in st.session_state and "analysis_result" in st.session_state:
        with st.expander("ë‹µë³€ í‰ê°€", expanded=False):
            rating = st.radio("ë§Œì¡±ë„", [1, 2, 3, 4, 5], index=None, 
                            key=f"rating_{st.session_state['current_query']}")
            feedback = st.text_area("ì¶”ê°€ ì˜ê²¬", 
                                  key=f"feedback_{st.session_state['current_query']}")
            
            if st.button("í‰ê°€ ì œì¶œ", key=f"submit_{st.session_state['current_query']}"):
                if save_feedback_log(st.session_state['current_query'], 
                                   st.session_state['analysis_result'], 
                                   rating, feedback):
                    st.success("í‰ê°€ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        st.error("ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë‹¤ì‹œ ì‹œì‘í•©ë‹ˆë‹¤.")
        st.text(traceback.format_exc())
        restart_streamlit()